{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "PATH_to_log_dir = '/data/cv_final/CT-Predict/3D-Seg/result' # 如果输出路径不存在会被自动创建\n",
    "writer = SummaryWriter(PATH_to_log_dir)\n",
    "\n",
    "# for n_iter in range(100):\n",
    "#     writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "#     writer.add_scalar('Accuracy/train', np.random.random(), n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import random \n",
    "from torchvision.datasets import ImageFolder\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from skimage.io import imread, imsave\n",
    "import skimage\n",
    "from PIL import ImageFile\n",
    "from PIL import Image\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "val_transformer = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "batchsize=4\n",
    "def read_txt(txt_path):\n",
    "    with open(txt_path) as f:\n",
    "        lines = f.readlines()\n",
    "    txt_data = [line.strip() for line in lines]\n",
    "    return txt_data\n",
    "\n",
    "class VolumeDataset(Dataset):\n",
    "    def __init__(self, root_dir, txt_COVID, txt_NonCOVID, volume_size= 4, transform=None):\n",
    "        super(VolumeDataset, self).__init__()\n",
    "        \n",
    "        self._cube_dir = Path(root_dir)\n",
    "        self._paths = []\n",
    "        self._labels = []\n",
    "        self.volume_depth = volume_size\n",
    "        \n",
    "        self.txt_path = [txt_COVID,txt_NonCOVID]\n",
    "        self.classes = ['COVID19', 'Normal']\n",
    "        self.num_cls = len(self.classes)\n",
    "        one_class_path = []\n",
    "        one_class_laebl = []\n",
    "        for c in range(self.num_cls):\n",
    "            one_class_path = read_txt(self.txt_path[c])\n",
    "            one_class_label = [c] * len(one_class_path) \n",
    "            self._paths += one_class_path\n",
    "            self._labels += one_class_label\n",
    "        \n",
    "        \n",
    "    def _load_cube(self, filepath):\n",
    "        vol = np.load(filepath)\n",
    "        # file shape: (volume_size, H, W) \n",
    "        vol = torch.tensor(vol, dtype=torch.float)\n",
    "        vol = vol.permute(3,0,1,2) # to (C,D,H,W)\n",
    "        return vol\n",
    "\n",
    "\n",
    "    def _get_cube(self, filepath):\n",
    "        vol = self._load_cube(filepath)\n",
    "#         print(vol)\n",
    "        return vol\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "#         print('len')\n",
    "        return len(self._paths)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(idx)\n",
    "        path = self._paths[idx]\n",
    "        label = self._labels[idx]\n",
    "        x = self._get_cube(path)\n",
    "#         print(x)\n",
    "        y = label\n",
    "        return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349\n",
      "99\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    trainset = VolumeDataset(root_dir='/data/Data/3D',\n",
    "                              txt_COVID='/data/Data/3D/Data_split/COVID19/COVID19_train.txt',\n",
    "                              txt_NonCOVID='/data/Data/3D/Data_split/Normal/Normal_train.txt')\n",
    "    valset = VolumeDataset(root_dir='/data/Data3D/',\n",
    "                              txt_COVID='/data/Data/3D/Data_split/COVID19/COVID19_val.txt',\n",
    "                              txt_NonCOVID='/data/Data/3D/Data_split/Normal/Normal_val.txt')\n",
    "    testset = VolumeDataset(root_dir='/data/Data3D/',\n",
    "                              txt_COVID='/data/Data/3D/Data_split/COVID19/COVID19_test.txt',\n",
    "                              txt_NonCOVID='/data/Data/3D/Data_split/Normal/Normal_test.txt')\n",
    "    print(trainset.__len__())\n",
    "    print(valset.__len__())\n",
    "    print(testset.__len__())\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True)\n",
    "    val_loader = DataLoader(valset, batch_size=batchsize, drop_last=False, shuffle=False)\n",
    "    test_loader = DataLoader(testset, batch_size=batchsize, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 512, 512]) 0\n"
     ]
    }
   ],
   "source": [
    "for b,l in trainset:\n",
    "    print(b.shape, l)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load Self-Trans model\"\"\"\n",
    "\"\"\"Change names and locations to the Self-Trans.pt\"\"\"\n",
    "\n",
    "# from resnet3d import resnet3d18 as renet\n",
    "# import resnet3d \n",
    "from resnet3d  import resnet3d18 as resnet18\n",
    "model = resnet18()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = model.to(device)\n",
    "\n",
    "modelname = '3DResNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "his = {}\n",
    "his['train_loss'] = []\n",
    "his['train_acc'] = []\n",
    "his['val_loss'] = []\n",
    "his['val_acc'] = []\n",
    "his['test_loss'] = []\n",
    "his['test_acc'] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training process is defined here \n",
    "\n",
    "alpha = None\n",
    "## alpha is None if mixup is not used\n",
    "alpha_name = f'{alpha}'\n",
    "device = 'cuda'\n",
    "\n",
    "def train(optimizer, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    for batch_index, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        criteria = nn.CrossEntropyLoss()\n",
    "        loss = criteria(output, target.long())\n",
    "        \n",
    "        train_loss += criteria(output, target.long())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        train_correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "    \n",
    "        # Display progress and write to tensorboard\n",
    "        if batch_index % bs == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}'.format(\n",
    "                epoch, batch_index, len(train_loader),\n",
    "                100.0 * batch_index / len(train_loader), loss.item()/ bs))\n",
    "    \n",
    "    # 记录一个epoch的总loss和batch的平均acc\n",
    "    avg_loss = train_loss/len(train_loader.dataset)\n",
    "    avg_acc = train_correct / len(train_loader.dataset)\n",
    "    \n",
    "#     writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "#     writer.add_scalar('Accuracy/train', avg_acc, epoch)\n",
    "\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, train_correct, len(train_loader.dataset),\n",
    "        avg_acc))\n",
    "    his['train_loss'].append(train_loss.data.cpu().numpy()/ len(train_loader.dataset))\n",
    "    his['train_acc'].append(train_correct / len(train_loader.dataset))\n",
    "    f = open('/data/cv_final/CT-Predict/3D-Seg/result/{}.txt'.format(modelname), 'a+')\n",
    "    f.write('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, train_correct, len(train_loader.dataset),\n",
    "        avg_acc))\n",
    "    f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val process is defined here\n",
    "\n",
    "def val(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    \n",
    "    \n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    # Don't update model\n",
    "    with torch.no_grad():\n",
    "        tpr_list = []\n",
    "        fpr_list = []\n",
    "        \n",
    "        predlist=[]\n",
    "        scorelist=[]\n",
    "        targetlist=[]\n",
    "        # Predict\n",
    "        for batch_index, (data, target) in enumerate(val_loader):\n",
    "            print('batch_index:\\t', batch_index)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            val_loss += criteria(output, target.long())\n",
    "            score = F.softmax(output, dim=1)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "            \n",
    "            targetcpu=target.long().cpu().numpy()\n",
    "            predlist=np.append(predlist, pred.cpu().numpy())\n",
    "            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])\n",
    "            targetlist=np.append(targetlist,targetcpu)\n",
    "    \n",
    "    # 记录一个epoch的loss, acc\n",
    "    his['val_loss'].append(val_loss.data.cpu().numpy()/len(val_loader.dataset))\n",
    "    his['val_acc'].append(correct/len(val_loader.dataset))\n",
    "\n",
    "    return targetlist, scorelist, predlist\n",
    "    \n",
    "    # Write to tensorboard\n",
    "#     writer.add_scalar('Test Accuracy', 100.0 * correct / len(test_loader.dataset), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/88 (0%)]\tTrain Loss: 0.178571\n",
      "Train Epoch: 1 [4/88 (5%)]\tTrain Loss: 0.012117\n",
      "Train Epoch: 1 [8/88 (9%)]\tTrain Loss: 0.184390\n",
      "Train Epoch: 1 [12/88 (14%)]\tTrain Loss: 0.199516\n",
      "Train Epoch: 1 [16/88 (18%)]\tTrain Loss: 0.065555\n",
      "Train Epoch: 1 [20/88 (23%)]\tTrain Loss: 0.152092\n",
      "Train Epoch: 1 [24/88 (27%)]\tTrain Loss: 0.133089\n",
      "Train Epoch: 1 [28/88 (32%)]\tTrain Loss: 0.031986\n",
      "Train Epoch: 1 [32/88 (36%)]\tTrain Loss: 0.107577\n",
      "Train Epoch: 1 [36/88 (41%)]\tTrain Loss: 0.033044\n",
      "Train Epoch: 1 [40/88 (45%)]\tTrain Loss: 0.084500\n",
      "Train Epoch: 1 [44/88 (50%)]\tTrain Loss: 0.086891\n",
      "Train Epoch: 1 [48/88 (55%)]\tTrain Loss: 0.038149\n",
      "Train Epoch: 1 [52/88 (59%)]\tTrain Loss: 0.013664\n",
      "Train Epoch: 1 [56/88 (64%)]\tTrain Loss: 0.174366\n",
      "Train Epoch: 1 [60/88 (68%)]\tTrain Loss: 0.017874\n",
      "Train Epoch: 1 [64/88 (73%)]\tTrain Loss: 0.204565\n",
      "Train Epoch: 1 [68/88 (77%)]\tTrain Loss: 0.007581\n",
      "Train Epoch: 1 [72/88 (82%)]\tTrain Loss: 0.030086\n",
      "Train Epoch: 1 [76/88 (86%)]\tTrain Loss: 0.010154\n",
      "Train Epoch: 1 [80/88 (91%)]\tTrain Loss: 0.010516\n",
      "Train Epoch: 1 [84/88 (95%)]\tTrain Loss: 0.084217\n",
      "\n",
      "Train set: Average loss: 0.0981, Accuracy: 287/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.03094224 0.03530184 0.0330157  0.07074682 0.02975767 0.2402001\n",
      " 0.04911179 0.02538656 0.02548056 0.04797149 0.04504849 0.03576805\n",
      " 0.03850106 0.02628537 0.06521458 0.04207814 0.09291447 0.11122056\n",
      " 0.07777511 0.05101095 0.05153457 0.03185934 0.23406871 0.11019306\n",
      " 0.05839704 0.04308956 0.11678886 0.12326977 0.0499209  0.04315478\n",
      " 0.02299594 0.02766188 0.98687565 0.97579145 0.96326202 0.94460475\n",
      " 0.98101646 0.98014921 0.98267996 0.97003102 0.98105377 0.97297847\n",
      " 0.9819665  0.95660633 0.96796721 0.98636985 0.95608866 0.97986448\n",
      " 0.97406876 0.9806717  0.97202718 0.96928763 0.98261482 0.98619103\n",
      " 0.97993958 0.98047835 0.97410768 0.98594993 0.98532397 0.98751509\n",
      " 0.98182124 0.97967911 0.98553836 0.98673022 0.97754055 0.98670304\n",
      " 0.9807809  0.98594564 0.98142278 0.98389429 0.98067874 0.97339123\n",
      " 0.98587924 0.98135602 0.98274934 0.98546439 0.98725408 0.98762542\n",
      " 0.97309756 0.98552519 0.97672617 0.97940695 0.98633087 0.98372424\n",
      " 0.97795016 0.96662772 0.98541695 0.97517574 0.98216957 0.98069555\n",
      " 0.9881115  0.98432434 0.98434401 0.98008782 0.98393852 0.97895467\n",
      " 0.98682809 0.98395479 0.98486125]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 2 [0/88 (0%)]\tTrain Loss: 0.056959\n",
      "Train Epoch: 2 [4/88 (5%)]\tTrain Loss: 0.032449\n",
      "Train Epoch: 2 [8/88 (9%)]\tTrain Loss: 0.216491\n",
      "Train Epoch: 2 [12/88 (14%)]\tTrain Loss: 0.084518\n",
      "Train Epoch: 2 [16/88 (18%)]\tTrain Loss: 0.094013\n",
      "Train Epoch: 2 [20/88 (23%)]\tTrain Loss: 0.072235\n",
      "Train Epoch: 2 [24/88 (27%)]\tTrain Loss: 0.026631\n",
      "Train Epoch: 2 [28/88 (32%)]\tTrain Loss: 0.038917\n",
      "Train Epoch: 2 [32/88 (36%)]\tTrain Loss: 0.032931\n",
      "Train Epoch: 2 [36/88 (41%)]\tTrain Loss: 0.025077\n",
      "Train Epoch: 2 [40/88 (45%)]\tTrain Loss: 0.064391\n",
      "Train Epoch: 2 [44/88 (50%)]\tTrain Loss: 0.008505\n",
      "Train Epoch: 2 [48/88 (55%)]\tTrain Loss: 0.057411\n",
      "Train Epoch: 2 [52/88 (59%)]\tTrain Loss: 0.138673\n",
      "Train Epoch: 2 [56/88 (64%)]\tTrain Loss: 0.163022\n",
      "Train Epoch: 2 [60/88 (68%)]\tTrain Loss: 0.029792\n",
      "Train Epoch: 2 [64/88 (73%)]\tTrain Loss: 0.041860\n",
      "Train Epoch: 2 [68/88 (77%)]\tTrain Loss: 0.093772\n",
      "Train Epoch: 2 [72/88 (82%)]\tTrain Loss: 0.026806\n",
      "Train Epoch: 2 [76/88 (86%)]\tTrain Loss: 0.130783\n",
      "Train Epoch: 2 [80/88 (91%)]\tTrain Loss: 0.037505\n",
      "Train Epoch: 2 [84/88 (95%)]\tTrain Loss: 0.019046\n",
      "\n",
      "Train set: Average loss: 0.0614, Accuracy: 316/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.07123148 0.09689615 0.06054881 0.16136564 0.06557652 0.6638962\n",
      " 0.17822526 0.04113054 0.07401703 0.13757327 0.10090605 0.06207772\n",
      " 0.04595491 0.04184156 0.16193764 0.1185734  0.43607792 0.42024654\n",
      " 0.28372422 0.12838282 0.1274589  0.05607745 0.74025571 0.44602302\n",
      " 0.1588176  0.10880136 0.50235766 0.50748646 0.14918685 0.09833541\n",
      " 0.0275022  0.02781641 0.99352515 0.9912464  0.98798311 0.98335266\n",
      " 0.99257129 0.99221075 0.99299037 0.98943275 0.9925493  0.99038774\n",
      " 0.99294114 0.98520941 0.98861921 0.99345666 0.98511571 0.99229807\n",
      " 0.99101347 0.99280143 0.99029106 0.98922122 0.99243027 0.99338573\n",
      " 0.99244493 0.99237871 0.99094182 0.99337667 0.9932462  0.99334466\n",
      " 0.99272305 0.99236578 0.99313873 0.99328268 0.99180788 0.99318248\n",
      " 0.99204344 0.99326271 0.99263072 0.99326152 0.9921611  0.98968107\n",
      " 0.99348664 0.99268442 0.99284995 0.99324036 0.99336857 0.9935236\n",
      " 0.99085742 0.99322742 0.991368   0.99189889 0.99343127 0.99306858\n",
      " 0.99179029 0.98775709 0.99331999 0.99066859 0.99265021 0.99241745\n",
      " 0.99342865 0.99321222 0.99303752 0.99232191 0.9930864  0.99177462\n",
      " 0.9933247  0.99304366 0.99329656]\n",
      "predict [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 3 [0/88 (0%)]\tTrain Loss: 0.030552\n",
      "Train Epoch: 3 [4/88 (5%)]\tTrain Loss: 0.114710\n",
      "Train Epoch: 3 [8/88 (9%)]\tTrain Loss: 0.031849\n",
      "Train Epoch: 3 [12/88 (14%)]\tTrain Loss: 0.064111\n",
      "Train Epoch: 3 [16/88 (18%)]\tTrain Loss: 0.348073\n",
      "Train Epoch: 3 [20/88 (23%)]\tTrain Loss: 0.001914\n",
      "Train Epoch: 3 [24/88 (27%)]\tTrain Loss: 0.014753\n",
      "Train Epoch: 3 [28/88 (32%)]\tTrain Loss: 0.010945\n",
      "Train Epoch: 3 [32/88 (36%)]\tTrain Loss: 0.050906\n",
      "Train Epoch: 3 [36/88 (41%)]\tTrain Loss: 0.006415\n",
      "Train Epoch: 3 [40/88 (45%)]\tTrain Loss: 0.004829\n",
      "Train Epoch: 3 [44/88 (50%)]\tTrain Loss: 0.061185\n",
      "Train Epoch: 3 [48/88 (55%)]\tTrain Loss: 0.031212\n",
      "Train Epoch: 3 [52/88 (59%)]\tTrain Loss: 0.004507\n",
      "Train Epoch: 3 [56/88 (64%)]\tTrain Loss: 0.011481\n",
      "Train Epoch: 3 [60/88 (68%)]\tTrain Loss: 0.003734\n",
      "Train Epoch: 3 [64/88 (73%)]\tTrain Loss: 0.006920\n",
      "Train Epoch: 3 [68/88 (77%)]\tTrain Loss: 0.149218\n",
      "Train Epoch: 3 [72/88 (82%)]\tTrain Loss: 0.034442\n",
      "Train Epoch: 3 [76/88 (86%)]\tTrain Loss: 0.028289\n",
      "Train Epoch: 3 [80/88 (91%)]\tTrain Loss: 0.048200\n",
      "Train Epoch: 3 [84/88 (95%)]\tTrain Loss: 0.007163\n",
      "\n",
      "Train set: Average loss: 0.0447, Accuracy: 322/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.07219076 0.07975184 0.05840508 0.11212111 0.06593519 0.39837849\n",
      " 0.14964379 0.04544909 0.10402335 0.09941691 0.09334005 0.05913226\n",
      " 0.05540726 0.04983529 0.12043887 0.09776679 0.31080306 0.22771761\n",
      " 0.19467895 0.1051237  0.10192899 0.04697447 0.53884649 0.29541233\n",
      " 0.1236513  0.07592722 0.32791865 0.30428439 0.11440694 0.08676751\n",
      " 0.03607659 0.06984597 0.99354929 0.98827773 0.9784106  0.96552002\n",
      " 0.9908036  0.99046969 0.9916802  0.98453897 0.99090213 0.98638368\n",
      " 0.99114418 0.97379667 0.98320317 0.99349266 0.97483408 0.99041909\n",
      " 0.98739529 0.99068147 0.98536474 0.9841314  0.99182713 0.99335217\n",
      " 0.99074358 0.99078137 0.98704547 0.99322164 0.99313825 0.9937495\n",
      " 0.99134564 0.99038202 0.99283338 0.99356306 0.98876357 0.99369252\n",
      " 0.99098057 0.9934355  0.99105549 0.99249989 0.99096185 0.98674935\n",
      " 0.99311936 0.99119639 0.99189746 0.99306422 0.99366224 0.99386227\n",
      " 0.98645395 0.99308491 0.98880869 0.99014878 0.99323249 0.99211311\n",
      " 0.98953593 0.98184937 0.99287838 0.98767382 0.99131459 0.99069619\n",
      " 0.99387014 0.99271846 0.99250072 0.99067163 0.99218094 0.98993659\n",
      " 0.99363047 0.99245268 0.99279177]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/88 (0%)]\tTrain Loss: 0.024498\n",
      "Train Epoch: 4 [4/88 (5%)]\tTrain Loss: 0.005280\n",
      "Train Epoch: 4 [8/88 (9%)]\tTrain Loss: 0.128571\n",
      "Train Epoch: 4 [12/88 (14%)]\tTrain Loss: 0.005471\n",
      "Train Epoch: 4 [16/88 (18%)]\tTrain Loss: 0.107847\n",
      "Train Epoch: 4 [20/88 (23%)]\tTrain Loss: 0.071840\n",
      "Train Epoch: 4 [24/88 (27%)]\tTrain Loss: 0.023133\n",
      "Train Epoch: 4 [28/88 (32%)]\tTrain Loss: 0.016769\n",
      "Train Epoch: 4 [32/88 (36%)]\tTrain Loss: 0.038415\n",
      "Train Epoch: 4 [36/88 (41%)]\tTrain Loss: 0.004119\n",
      "Train Epoch: 4 [40/88 (45%)]\tTrain Loss: 0.028589\n",
      "Train Epoch: 4 [44/88 (50%)]\tTrain Loss: 0.005102\n",
      "Train Epoch: 4 [48/88 (55%)]\tTrain Loss: 0.022864\n",
      "Train Epoch: 4 [52/88 (59%)]\tTrain Loss: 0.027420\n",
      "Train Epoch: 4 [56/88 (64%)]\tTrain Loss: 0.005387\n",
      "Train Epoch: 4 [60/88 (68%)]\tTrain Loss: 0.057855\n",
      "Train Epoch: 4 [64/88 (73%)]\tTrain Loss: 0.001062\n",
      "Train Epoch: 4 [68/88 (77%)]\tTrain Loss: 0.003651\n",
      "Train Epoch: 4 [72/88 (82%)]\tTrain Loss: 0.014544\n",
      "Train Epoch: 4 [76/88 (86%)]\tTrain Loss: 0.009863\n",
      "Train Epoch: 4 [80/88 (91%)]\tTrain Loss: 0.005956\n",
      "Train Epoch: 4 [84/88 (95%)]\tTrain Loss: 0.008735\n",
      "\n",
      "Train set: Average loss: 0.0380, Accuracy: 321/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.0350359  0.0421048  0.02453263 0.05711629 0.02357346 0.375563\n",
      " 0.08506031 0.02565573 0.06203589 0.03544027 0.04119277 0.02638395\n",
      " 0.05490227 0.02919937 0.05362283 0.04563912 0.22622074 0.13023688\n",
      " 0.0918559  0.03888229 0.04104949 0.02090196 0.49309292 0.2021229\n",
      " 0.04746279 0.03240359 0.30359471 0.24887487 0.05649648 0.03093165\n",
      " 0.03131979 0.09928644 0.99673849 0.99602413 0.99397886 0.99145377\n",
      " 0.99637991 0.9962967  0.99646109 0.99524349 0.99635172 0.99571913\n",
      " 0.99641222 0.9929772  0.99511874 0.99675876 0.99330282 0.99639833\n",
      " 0.99586606 0.99625576 0.99541533 0.99492103 0.99657416 0.99664742\n",
      " 0.99635673 0.996454   0.99581146 0.99665654 0.99671894 0.99657273\n",
      " 0.99650699 0.99624288 0.99659377 0.99660563 0.9959895  0.99661785\n",
      " 0.99644864 0.99679822 0.99640793 0.99661285 0.99650997 0.99563652\n",
      " 0.9966954  0.99633092 0.99655747 0.9967038  0.99665242 0.99671555\n",
      " 0.99556726 0.99665427 0.99606675 0.99646866 0.99664426 0.99656206\n",
      " 0.99626511 0.994919   0.99660373 0.99592978 0.99651563 0.99637127\n",
      " 0.99654168 0.99658102 0.9965018  0.9962219  0.99663603 0.99633694\n",
      " 0.99666923 0.99664694 0.99668223]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 5 [0/88 (0%)]\tTrain Loss: 0.086294\n",
      "Train Epoch: 5 [4/88 (5%)]\tTrain Loss: 0.002938\n",
      "Train Epoch: 5 [8/88 (9%)]\tTrain Loss: 0.017841\n",
      "Train Epoch: 5 [12/88 (14%)]\tTrain Loss: 0.003289\n",
      "Train Epoch: 5 [16/88 (18%)]\tTrain Loss: 0.053177\n",
      "Train Epoch: 5 [20/88 (23%)]\tTrain Loss: 0.005287\n",
      "Train Epoch: 5 [24/88 (27%)]\tTrain Loss: 0.001235\n",
      "Train Epoch: 5 [28/88 (32%)]\tTrain Loss: 0.003279\n",
      "Train Epoch: 5 [32/88 (36%)]\tTrain Loss: 0.004814\n",
      "Train Epoch: 5 [36/88 (41%)]\tTrain Loss: 0.032505\n",
      "Train Epoch: 5 [40/88 (45%)]\tTrain Loss: 0.123704\n",
      "Train Epoch: 5 [44/88 (50%)]\tTrain Loss: 0.001346\n",
      "Train Epoch: 5 [48/88 (55%)]\tTrain Loss: 0.004141\n",
      "Train Epoch: 5 [52/88 (59%)]\tTrain Loss: 0.001099\n",
      "Train Epoch: 5 [56/88 (64%)]\tTrain Loss: 0.001904\n",
      "Train Epoch: 5 [60/88 (68%)]\tTrain Loss: 0.043503\n",
      "Train Epoch: 5 [64/88 (73%)]\tTrain Loss: 0.052178\n",
      "Train Epoch: 5 [68/88 (77%)]\tTrain Loss: 0.016283\n",
      "Train Epoch: 5 [72/88 (82%)]\tTrain Loss: 0.002305\n",
      "Train Epoch: 5 [76/88 (86%)]\tTrain Loss: 0.004080\n",
      "Train Epoch: 5 [80/88 (91%)]\tTrain Loss: 0.007743\n",
      "Train Epoch: 5 [84/88 (95%)]\tTrain Loss: 0.015715\n",
      "\n",
      "Train set: Average loss: 0.0159, Accuracy: 343/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00606001 0.00626969 0.00609196 0.00880218 0.00558869 0.02413883\n",
      " 0.00734044 0.00621724 0.00578629 0.00677734 0.00701845 0.00577372\n",
      " 0.0084084  0.00665976 0.00865708 0.00588108 0.01205963 0.01007905\n",
      " 0.00830068 0.00705038 0.00685909 0.00519124 0.02785738 0.01298458\n",
      " 0.00765772 0.00668226 0.01015047 0.01124794 0.00645937 0.00655269\n",
      " 0.00920694 0.01199615 0.99734855 0.99686062 0.99438596 0.9898572\n",
      " 0.9971596  0.99700862 0.99721849 0.99595994 0.99710637 0.99630618\n",
      " 0.99732023 0.99340349 0.99565351 0.99735475 0.99337775 0.99723768\n",
      " 0.99663687 0.99721599 0.9961825  0.9956457  0.99726045 0.99723572\n",
      " 0.99728584 0.99716967 0.9966054  0.99727482 0.99738008 0.99700505\n",
      " 0.99722666 0.99703962 0.99721932 0.99712402 0.99687153 0.99713671\n",
      " 0.99710768 0.9973647  0.99712914 0.99734491 0.99723649 0.99610656\n",
      " 0.99732685 0.99718177 0.99726641 0.99727732 0.99716395 0.99718577\n",
      " 0.99626321 0.99729025 0.99681336 0.9970203  0.99724329 0.99727899\n",
      " 0.9970696  0.9954288  0.99725574 0.9965778  0.99718201 0.99712652\n",
      " 0.99694341 0.99732721 0.99723512 0.99711645 0.99733979 0.99707139\n",
      " 0.99718684 0.99735141 0.99734479]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 6 [0/88 (0%)]\tTrain Loss: 0.006807\n",
      "Train Epoch: 6 [4/88 (5%)]\tTrain Loss: 0.002184\n",
      "Train Epoch: 6 [8/88 (9%)]\tTrain Loss: 0.001213\n",
      "Train Epoch: 6 [12/88 (14%)]\tTrain Loss: 0.003853\n",
      "Train Epoch: 6 [16/88 (18%)]\tTrain Loss: 0.219092\n",
      "Train Epoch: 6 [20/88 (23%)]\tTrain Loss: 0.014980\n",
      "Train Epoch: 6 [24/88 (27%)]\tTrain Loss: 0.007551\n",
      "Train Epoch: 6 [28/88 (32%)]\tTrain Loss: 0.027481\n",
      "Train Epoch: 6 [32/88 (36%)]\tTrain Loss: 0.003603\n",
      "Train Epoch: 6 [36/88 (41%)]\tTrain Loss: 0.026987\n",
      "Train Epoch: 6 [40/88 (45%)]\tTrain Loss: 0.022393\n",
      "Train Epoch: 6 [44/88 (50%)]\tTrain Loss: 0.027856\n",
      "Train Epoch: 6 [48/88 (55%)]\tTrain Loss: 0.055742\n",
      "Train Epoch: 6 [52/88 (59%)]\tTrain Loss: 0.015629\n",
      "Train Epoch: 6 [56/88 (64%)]\tTrain Loss: 0.053541\n",
      "Train Epoch: 6 [60/88 (68%)]\tTrain Loss: 0.060044\n",
      "Train Epoch: 6 [64/88 (73%)]\tTrain Loss: 0.007708\n",
      "Train Epoch: 6 [68/88 (77%)]\tTrain Loss: 0.009175\n",
      "Train Epoch: 6 [72/88 (82%)]\tTrain Loss: 0.013498\n",
      "Train Epoch: 6 [76/88 (86%)]\tTrain Loss: 0.001133\n",
      "Train Epoch: 6 [80/88 (91%)]\tTrain Loss: 0.007744\n",
      "Train Epoch: 6 [84/88 (95%)]\tTrain Loss: 0.015646\n",
      "\n",
      "Train set: Average loss: 0.0315, Accuracy: 331/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00927012 0.01046924 0.00731104 0.01301713 0.00789687 0.07545179\n",
      " 0.01631163 0.00766387 0.02161534 0.0106857  0.00997453 0.00697766\n",
      " 0.00717195 0.00765645 0.013456   0.01081202 0.04777304 0.02742493\n",
      " 0.02087113 0.01092603 0.01128435 0.00635251 0.15602691 0.04204024\n",
      " 0.01388223 0.01099134 0.05185506 0.04567237 0.01220098 0.00975382\n",
      " 0.00677153 0.01148736 0.99786747 0.99758959 0.99639314 0.99471503\n",
      " 0.99779654 0.99770641 0.99781775 0.99709523 0.99771959 0.9973985\n",
      " 0.99777079 0.99607795 0.99706858 0.997895   0.99615413 0.9977653\n",
      " 0.9974969  0.99772745 0.99721068 0.9970578  0.99784398 0.99782592\n",
      " 0.99781007 0.99776888 0.9974342  0.9978314  0.99789727 0.99768078\n",
      " 0.99785209 0.99769419 0.99780124 0.99775296 0.99759549 0.99775285\n",
      " 0.99779654 0.99791521 0.99773765 0.99788612 0.99783486 0.99730229\n",
      " 0.99786323 0.99777871 0.99783176 0.99786538 0.99777299 0.99779379\n",
      " 0.99730265 0.99784207 0.99760568 0.99772221 0.99782372 0.9978441\n",
      " 0.99770641 0.99692035 0.9978441  0.9975242  0.99781537 0.99776638\n",
      " 0.99767321 0.99786121 0.99778312 0.99774772 0.99787962 0.99774951\n",
      " 0.99781537 0.9978891  0.99790347]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [0/88 (0%)]\tTrain Loss: 0.001543\n",
      "Train Epoch: 7 [4/88 (5%)]\tTrain Loss: 0.000532\n",
      "Train Epoch: 7 [8/88 (9%)]\tTrain Loss: 0.003380\n",
      "Train Epoch: 7 [12/88 (14%)]\tTrain Loss: 0.006972\n",
      "Train Epoch: 7 [16/88 (18%)]\tTrain Loss: 0.003728\n",
      "Train Epoch: 7 [20/88 (23%)]\tTrain Loss: 0.004215\n",
      "Train Epoch: 7 [24/88 (27%)]\tTrain Loss: 0.088407\n",
      "Train Epoch: 7 [28/88 (32%)]\tTrain Loss: 0.003681\n",
      "Train Epoch: 7 [32/88 (36%)]\tTrain Loss: 0.002712\n",
      "Train Epoch: 7 [36/88 (41%)]\tTrain Loss: 0.026348\n",
      "Train Epoch: 7 [40/88 (45%)]\tTrain Loss: 0.007192\n",
      "Train Epoch: 7 [44/88 (50%)]\tTrain Loss: 0.001243\n",
      "Train Epoch: 7 [48/88 (55%)]\tTrain Loss: 0.013738\n",
      "Train Epoch: 7 [52/88 (59%)]\tTrain Loss: 0.004972\n",
      "Train Epoch: 7 [56/88 (64%)]\tTrain Loss: 0.001732\n",
      "Train Epoch: 7 [60/88 (68%)]\tTrain Loss: 0.017479\n",
      "Train Epoch: 7 [64/88 (73%)]\tTrain Loss: 0.014334\n",
      "Train Epoch: 7 [68/88 (77%)]\tTrain Loss: 0.018025\n",
      "Train Epoch: 7 [72/88 (82%)]\tTrain Loss: 0.008071\n",
      "Train Epoch: 7 [76/88 (86%)]\tTrain Loss: 0.006457\n",
      "Train Epoch: 7 [80/88 (91%)]\tTrain Loss: 0.009370\n",
      "Train Epoch: 7 [84/88 (95%)]\tTrain Loss: 0.006348\n",
      "\n",
      "Train set: Average loss: 0.0216, Accuracy: 340/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00808552 0.00903474 0.00733487 0.01643992 0.00734811 0.06290065\n",
      " 0.01243645 0.00617671 0.00724132 0.00998005 0.00989262 0.00772765\n",
      " 0.00691976 0.00735971 0.01404378 0.0090601  0.0271323  0.02307735\n",
      " 0.01877554 0.0105892  0.01140338 0.00669401 0.08630309 0.03049523\n",
      " 0.01452794 0.00958908 0.02664939 0.03155998 0.01035191 0.00982314\n",
      " 0.00585048 0.00636179 0.99717474 0.9967553  0.99503797 0.99183381\n",
      " 0.99712938 0.99695861 0.99717295 0.99602664 0.99687684 0.9963935\n",
      " 0.99715507 0.99385685 0.99578178 0.99721599 0.99406695 0.99719161\n",
      " 0.99662423 0.99714929 0.99612576 0.9958204  0.99710697 0.99706155\n",
      " 0.99712092 0.99703974 0.99661332 0.99712795 0.99727517 0.99681574\n",
      " 0.99711132 0.99689806 0.99710232 0.99697113 0.99688452 0.99698812\n",
      " 0.99697304 0.9971956  0.99686497 0.99723333 0.99709356 0.99615592\n",
      " 0.99717736 0.99709761 0.99715495 0.99705964 0.99684614 0.99703813\n",
      " 0.99622369 0.99711967 0.99681669 0.99685585 0.9970386  0.99708122\n",
      " 0.99693215 0.99517125 0.99713314 0.99650949 0.99708623 0.99697292\n",
      " 0.99677604 0.9972524  0.9971149  0.9970693  0.99726933 0.99698251\n",
      " 0.99702841 0.9972294  0.99719894]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 8 [0/88 (0%)]\tTrain Loss: 0.009902\n",
      "Train Epoch: 8 [4/88 (5%)]\tTrain Loss: 0.013164\n",
      "Train Epoch: 8 [8/88 (9%)]\tTrain Loss: 0.001223\n",
      "Train Epoch: 8 [12/88 (14%)]\tTrain Loss: 0.002961\n",
      "Train Epoch: 8 [16/88 (18%)]\tTrain Loss: 0.154376\n",
      "Train Epoch: 8 [20/88 (23%)]\tTrain Loss: 0.002539\n",
      "Train Epoch: 8 [24/88 (27%)]\tTrain Loss: 0.003644\n",
      "Train Epoch: 8 [28/88 (32%)]\tTrain Loss: 0.001688\n",
      "Train Epoch: 8 [32/88 (36%)]\tTrain Loss: 0.001454\n",
      "Train Epoch: 8 [36/88 (41%)]\tTrain Loss: 0.001943\n",
      "Train Epoch: 8 [40/88 (45%)]\tTrain Loss: 0.002213\n",
      "Train Epoch: 8 [44/88 (50%)]\tTrain Loss: 0.023871\n",
      "Train Epoch: 8 [48/88 (55%)]\tTrain Loss: 0.001992\n",
      "Train Epoch: 8 [52/88 (59%)]\tTrain Loss: 0.002403\n",
      "Train Epoch: 8 [56/88 (64%)]\tTrain Loss: 0.002325\n",
      "Train Epoch: 8 [60/88 (68%)]\tTrain Loss: 0.008675\n",
      "Train Epoch: 8 [64/88 (73%)]\tTrain Loss: 0.003904\n",
      "Train Epoch: 8 [68/88 (77%)]\tTrain Loss: 0.001509\n",
      "Train Epoch: 8 [72/88 (82%)]\tTrain Loss: 0.001334\n",
      "Train Epoch: 8 [76/88 (86%)]\tTrain Loss: 0.001073\n",
      "Train Epoch: 8 [80/88 (91%)]\tTrain Loss: 0.002857\n",
      "Train Epoch: 8 [84/88 (95%)]\tTrain Loss: 0.004798\n",
      "\n",
      "Train set: Average loss: 0.0121, Accuracy: 345/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00866201 0.01006617 0.00568147 0.01725146 0.00535342 0.1025348\n",
      " 0.01709377 0.00611581 0.02258858 0.00787589 0.0109193  0.00648028\n",
      " 0.01371891 0.00946172 0.01357219 0.00903509 0.03659466 0.02124109\n",
      " 0.01905916 0.00828649 0.00902425 0.00551924 0.10632838 0.03624764\n",
      " 0.01202583 0.00758641 0.04109741 0.03537599 0.010524   0.00774738\n",
      " 0.00602207 0.00847705 0.99875307 0.99871135 0.99826652 0.99754542\n",
      " 0.99880457 0.99874216 0.99879754 0.99852908 0.99876094 0.99865192\n",
      " 0.99876726 0.99797016 0.99845934 0.99876922 0.99805284 0.99875367\n",
      " 0.99869019 0.99876559 0.99857199 0.99846905 0.9987815  0.99874192\n",
      " 0.99881005 0.99878639 0.9986726  0.99876046 0.99876994 0.99859399\n",
      " 0.99882191 0.99875677 0.99872249 0.99866343 0.99871647 0.99863547\n",
      " 0.99877757 0.99878877 0.99874985 0.99881244 0.99878556 0.99854302\n",
      " 0.99876308 0.99876678 0.99878973 0.9987722  0.99868923 0.99868733\n",
      " 0.99862397 0.99874794 0.9987036  0.99877995 0.99874961 0.99878997\n",
      " 0.99876958 0.99836797 0.99877065 0.99868506 0.9987914  0.99877852\n",
      " 0.99859768 0.99878424 0.99873561 0.99874938 0.99881017 0.99876851\n",
      " 0.99869412 0.99881124 0.99880314]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 9 [0/88 (0%)]\tTrain Loss: 0.000679\n",
      "Train Epoch: 9 [4/88 (5%)]\tTrain Loss: 0.257431\n",
      "Train Epoch: 9 [8/88 (9%)]\tTrain Loss: 0.002553\n",
      "Train Epoch: 9 [12/88 (14%)]\tTrain Loss: 0.045955\n",
      "Train Epoch: 9 [16/88 (18%)]\tTrain Loss: 0.011914\n",
      "Train Epoch: 9 [20/88 (23%)]\tTrain Loss: 0.001671\n",
      "Train Epoch: 9 [24/88 (27%)]\tTrain Loss: 0.003423\n",
      "Train Epoch: 9 [28/88 (32%)]\tTrain Loss: 0.013998\n",
      "Train Epoch: 9 [32/88 (36%)]\tTrain Loss: 0.001333\n",
      "Train Epoch: 9 [36/88 (41%)]\tTrain Loss: 0.055442\n",
      "Train Epoch: 9 [40/88 (45%)]\tTrain Loss: 0.005264\n",
      "Train Epoch: 9 [44/88 (50%)]\tTrain Loss: 0.012342\n",
      "Train Epoch: 9 [48/88 (55%)]\tTrain Loss: 0.005080\n",
      "Train Epoch: 9 [52/88 (59%)]\tTrain Loss: 0.003738\n",
      "Train Epoch: 9 [56/88 (64%)]\tTrain Loss: 0.001277\n",
      "Train Epoch: 9 [60/88 (68%)]\tTrain Loss: 0.000778\n",
      "Train Epoch: 9 [64/88 (73%)]\tTrain Loss: 0.007202\n",
      "Train Epoch: 9 [68/88 (77%)]\tTrain Loss: 0.006273\n",
      "Train Epoch: 9 [72/88 (82%)]\tTrain Loss: 0.005114\n",
      "Train Epoch: 9 [76/88 (86%)]\tTrain Loss: 0.001961\n",
      "Train Epoch: 9 [80/88 (91%)]\tTrain Loss: 0.000563\n",
      "Train Epoch: 9 [84/88 (95%)]\tTrain Loss: 0.051373\n",
      "\n",
      "Train set: Average loss: 0.0139, Accuracy: 342/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00268658 0.00292921 0.0026111  0.00500617 0.00255144 0.01710093\n",
      " 0.00375372 0.00280296 0.00261622 0.00326957 0.00336237 0.0027761\n",
      " 0.00336344 0.00285734 0.00461634 0.00299656 0.00735853 0.00653144\n",
      " 0.00519409 0.00337844 0.00349496 0.00256172 0.02211582 0.00874048\n",
      " 0.00429831 0.00332438 0.00688381 0.00811607 0.00345887 0.0032078\n",
      " 0.00287628 0.00329736 0.99878567 0.99862587 0.99792212 0.99664474\n",
      " 0.99876875 0.99870098 0.99878913 0.99832934 0.99873966 0.99853241\n",
      " 0.99874699 0.9975937  0.99826765 0.99878436 0.99762648 0.99875247\n",
      " 0.99859411 0.99873716 0.99839801 0.9982729  0.99876153 0.99876225\n",
      " 0.99874616 0.99876535 0.99855322 0.99878103 0.99878877 0.99866331\n",
      " 0.99881232 0.99869138 0.99874508 0.99868935 0.99866986 0.99867392\n",
      " 0.99873072 0.99880624 0.99873358 0.99879956 0.99878138 0.99836439\n",
      " 0.99880767 0.99873847 0.99878079 0.99879241 0.99872297 0.99874449\n",
      " 0.99848974 0.99876237 0.99862671 0.99874663 0.99877983 0.99878091\n",
      " 0.99875546 0.99810672 0.99878734 0.99856621 0.99877614 0.9987489\n",
      " 0.99863869 0.99877864 0.99874532 0.9986999  0.99882048 0.99869305\n",
      " 0.99872607 0.99880064 0.99881041]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [0/88 (0%)]\tTrain Loss: 0.000826\n",
      "Train Epoch: 10 [4/88 (5%)]\tTrain Loss: 0.000736\n",
      "Train Epoch: 10 [8/88 (9%)]\tTrain Loss: 0.003476\n",
      "Train Epoch: 10 [12/88 (14%)]\tTrain Loss: 0.084082\n",
      "Train Epoch: 10 [16/88 (18%)]\tTrain Loss: 0.013517\n",
      "Train Epoch: 10 [20/88 (23%)]\tTrain Loss: 0.042206\n",
      "Train Epoch: 10 [24/88 (27%)]\tTrain Loss: 0.004164\n",
      "Train Epoch: 10 [28/88 (32%)]\tTrain Loss: 0.002868\n",
      "Train Epoch: 10 [32/88 (36%)]\tTrain Loss: 0.000270\n",
      "Train Epoch: 10 [36/88 (41%)]\tTrain Loss: 0.044071\n",
      "Train Epoch: 10 [40/88 (45%)]\tTrain Loss: 0.001257\n",
      "Train Epoch: 10 [44/88 (50%)]\tTrain Loss: 0.060100\n",
      "Train Epoch: 10 [48/88 (55%)]\tTrain Loss: 0.001026\n",
      "Train Epoch: 10 [52/88 (59%)]\tTrain Loss: 0.000461\n",
      "Train Epoch: 10 [56/88 (64%)]\tTrain Loss: 0.003732\n",
      "Train Epoch: 10 [60/88 (68%)]\tTrain Loss: 0.225610\n",
      "Train Epoch: 10 [64/88 (73%)]\tTrain Loss: 0.013544\n",
      "Train Epoch: 10 [68/88 (77%)]\tTrain Loss: 0.049394\n",
      "Train Epoch: 10 [72/88 (82%)]\tTrain Loss: 0.013200\n",
      "Train Epoch: 10 [76/88 (86%)]\tTrain Loss: 0.007380\n",
      "Train Epoch: 10 [80/88 (91%)]\tTrain Loss: 0.003841\n",
      "Train Epoch: 10 [84/88 (95%)]\tTrain Loss: 0.001052\n",
      "\n",
      "Train set: Average loss: 0.0184, Accuracy: 340/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.01244903 0.01420925 0.00781578 0.01844557 0.00829128 0.05587919\n",
      " 0.01799511 0.01434969 0.01907375 0.00899896 0.01484844 0.0115764\n",
      " 0.02902441 0.014944   0.01802279 0.01452778 0.02870867 0.01819244\n",
      " 0.02505453 0.01084868 0.01282623 0.00857045 0.05214506 0.02791998\n",
      " 0.01438476 0.00973611 0.03397426 0.02825409 0.01364921 0.00921937\n",
      " 0.00920867 0.01278654 0.99742019 0.99554032 0.99216312 0.98608822\n",
      " 0.99668175 0.99631041 0.99696761 0.99433422 0.99657452 0.99489707\n",
      " 0.99671412 0.98985368 0.99363822 0.99737167 0.99108136 0.9962225\n",
      " 0.99554294 0.99655044 0.99451494 0.99404317 0.99671352 0.9973917\n",
      " 0.99663973 0.99652314 0.99526751 0.99732143 0.99718505 0.99729067\n",
      " 0.99684143 0.99627799 0.99708694 0.99731255 0.99598628 0.99716741\n",
      " 0.99639255 0.9972778  0.99649966 0.99710673 0.99640131 0.9945122\n",
      " 0.99717534 0.99678826 0.99699295 0.99722028 0.99739182 0.99739313\n",
      " 0.99499184 0.9972536  0.9956429  0.99615759 0.99726737 0.99699992\n",
      " 0.99618798 0.99326611 0.99729949 0.99527788 0.99663442 0.99650288\n",
      " 0.9974274  0.99724627 0.99702853 0.99647105 0.99707866 0.99611086\n",
      " 0.99735838 0.99710876 0.99721932]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "TP= 67 TN= 32 FN= 0 FP= 0\n",
      "TP+FP 67\n",
      "precision 1.0\n",
      "recall 1.0\n",
      "F1 1.0\n",
      "acc 1.0\n",
      "AUCp 1.0\n",
      "AUC 1.0\n",
      "\n",
      " The epoch is 10, average recall: 1.0000, average precision: 1.0000,    average F1: 1.0000, average accuracy: 1.0000, average AUC: 1.0000\n",
      "Train Epoch: 11 [0/88 (0%)]\tTrain Loss: 0.003490\n",
      "Train Epoch: 11 [4/88 (5%)]\tTrain Loss: 0.003845\n",
      "Train Epoch: 11 [8/88 (9%)]\tTrain Loss: 0.003185\n",
      "Train Epoch: 11 [12/88 (14%)]\tTrain Loss: 0.003043\n",
      "Train Epoch: 11 [16/88 (18%)]\tTrain Loss: 0.002209\n",
      "Train Epoch: 11 [20/88 (23%)]\tTrain Loss: 0.014410\n",
      "Train Epoch: 11 [24/88 (27%)]\tTrain Loss: 0.057574\n",
      "Train Epoch: 11 [28/88 (32%)]\tTrain Loss: 0.020983\n",
      "Train Epoch: 11 [32/88 (36%)]\tTrain Loss: 0.003770\n",
      "Train Epoch: 11 [36/88 (41%)]\tTrain Loss: 0.000823\n",
      "Train Epoch: 11 [40/88 (45%)]\tTrain Loss: 0.003919\n",
      "Train Epoch: 11 [44/88 (50%)]\tTrain Loss: 0.002208\n",
      "Train Epoch: 11 [48/88 (55%)]\tTrain Loss: 0.001131\n",
      "Train Epoch: 11 [52/88 (59%)]\tTrain Loss: 0.001440\n",
      "Train Epoch: 11 [56/88 (64%)]\tTrain Loss: 0.000797\n",
      "Train Epoch: 11 [60/88 (68%)]\tTrain Loss: 0.001037\n",
      "Train Epoch: 11 [64/88 (73%)]\tTrain Loss: 0.000375\n",
      "Train Epoch: 11 [68/88 (77%)]\tTrain Loss: 0.001454\n",
      "Train Epoch: 11 [72/88 (82%)]\tTrain Loss: 0.000752\n",
      "Train Epoch: 11 [76/88 (86%)]\tTrain Loss: 0.011014\n",
      "Train Epoch: 11 [80/88 (91%)]\tTrain Loss: 0.000887\n",
      "Train Epoch: 11 [84/88 (95%)]\tTrain Loss: 0.091823\n",
      "\n",
      "Train set: Average loss: 0.0108, Accuracy: 345/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.0041941  0.00426663 0.00357917 0.00635878 0.00358385 0.01928082\n",
      " 0.00581087 0.00403283 0.00766236 0.00428888 0.00462312 0.00393384\n",
      " 0.00436627 0.00446392 0.00549308 0.00413744 0.01090612 0.00822579\n",
      " 0.00751495 0.00401697 0.00468019 0.0034489  0.02363474 0.01011507\n",
      " 0.00609036 0.004728   0.00994267 0.0102758  0.00471521 0.00446619\n",
      " 0.00385824 0.00483982 0.9990747  0.9989053  0.99812633 0.99697399\n",
      " 0.99903178 0.99896657 0.99902582 0.9986186  0.99900526 0.99879092\n",
      " 0.99900419 0.99779046 0.99859279 0.99907887 0.99793124 0.99902713\n",
      " 0.99882525 0.99898022 0.99867588 0.99854398 0.99904925 0.99905616\n",
      " 0.99900573 0.99900872 0.99880075 0.99906939 0.99905592 0.99899024\n",
      " 0.99906045 0.99891829 0.99906343 0.99900419 0.99892187 0.99900728\n",
      " 0.99900645 0.99907899 0.9989832  0.99907184 0.99903631 0.99869484\n",
      " 0.99906725 0.9989987  0.99905628 0.99905199 0.99903452 0.99905068\n",
      " 0.99873871 0.99905509 0.99889946 0.99900061 0.99904722 0.99904233\n",
      " 0.99902511 0.99841547 0.99905986 0.99886972 0.99902868 0.99899489\n",
      " 0.9989773  0.99905282 0.99901998 0.99896705 0.99907386 0.99894863\n",
      " 0.99902701 0.99907196 0.99907291]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 12 [0/88 (0%)]\tTrain Loss: 0.000716\n",
      "Train Epoch: 12 [4/88 (5%)]\tTrain Loss: 0.000624\n",
      "Train Epoch: 12 [8/88 (9%)]\tTrain Loss: 0.013022\n",
      "Train Epoch: 12 [12/88 (14%)]\tTrain Loss: 0.001601\n",
      "Train Epoch: 12 [16/88 (18%)]\tTrain Loss: 0.004931\n",
      "Train Epoch: 12 [20/88 (23%)]\tTrain Loss: 0.000951\n",
      "Train Epoch: 12 [24/88 (27%)]\tTrain Loss: 0.014628\n",
      "Train Epoch: 12 [28/88 (32%)]\tTrain Loss: 0.000425\n",
      "Train Epoch: 12 [32/88 (36%)]\tTrain Loss: 0.001170\n",
      "Train Epoch: 12 [36/88 (41%)]\tTrain Loss: 0.000668\n",
      "Train Epoch: 12 [40/88 (45%)]\tTrain Loss: 0.000990\n",
      "Train Epoch: 12 [44/88 (50%)]\tTrain Loss: 0.006960\n",
      "Train Epoch: 12 [48/88 (55%)]\tTrain Loss: 0.000266\n",
      "Train Epoch: 12 [52/88 (59%)]\tTrain Loss: 0.041713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [56/88 (64%)]\tTrain Loss: 0.003290\n",
      "Train Epoch: 12 [60/88 (68%)]\tTrain Loss: 0.009828\n",
      "Train Epoch: 12 [64/88 (73%)]\tTrain Loss: 0.003884\n",
      "Train Epoch: 12 [68/88 (77%)]\tTrain Loss: 0.001348\n",
      "Train Epoch: 12 [72/88 (82%)]\tTrain Loss: 0.178064\n",
      "Train Epoch: 12 [76/88 (86%)]\tTrain Loss: 0.000746\n",
      "Train Epoch: 12 [80/88 (91%)]\tTrain Loss: 0.168676\n",
      "Train Epoch: 12 [84/88 (95%)]\tTrain Loss: 0.000844\n",
      "\n",
      "Train set: Average loss: 0.0111, Accuracy: 344/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00466784 0.00570041 0.0041505  0.00914955 0.00426164 0.03934861\n",
      " 0.00886579 0.00387629 0.00708035 0.00632091 0.00578196 0.00469937\n",
      " 0.00419094 0.00436778 0.00831108 0.00592815 0.02160214 0.01637752\n",
      " 0.01285221 0.00589013 0.00664396 0.00391624 0.06653202 0.02114846\n",
      " 0.00992288 0.00750138 0.023337   0.02390695 0.00716983 0.00619986\n",
      " 0.00307144 0.00410269 0.99866104 0.99651402 0.98962003 0.97876143\n",
      " 0.99789858 0.99770039 0.99814725 0.99530208 0.9979443  0.9952839\n",
      " 0.99778533 0.98429352 0.99373311 0.99867094 0.99071598 0.9978708\n",
      " 0.99644345 0.99765295 0.99558073 0.99411052 0.99820352 0.99865544\n",
      " 0.9978016  0.99779058 0.99558419 0.99865568 0.99856228 0.99877626\n",
      " 0.99813902 0.99758112 0.99852175 0.99871063 0.99706984 0.99871802\n",
      " 0.9978466  0.99870002 0.99767739 0.99842048 0.99778962 0.99602145\n",
      " 0.99864441 0.99813026 0.99834573 0.99861574 0.99863893 0.99877006\n",
      " 0.99614668 0.99859685 0.99674821 0.99750727 0.99853003 0.99816173\n",
      " 0.99783856 0.99133027 0.99855632 0.99648857 0.9977324  0.99776328\n",
      " 0.99876046 0.99849486 0.99838877 0.99779224 0.99836117 0.99720156\n",
      " 0.9987483  0.99845326 0.99852824]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 13 [0/88 (0%)]\tTrain Loss: 0.001749\n",
      "Train Epoch: 13 [4/88 (5%)]\tTrain Loss: 0.001738\n",
      "Train Epoch: 13 [8/88 (9%)]\tTrain Loss: 0.001212\n",
      "Train Epoch: 13 [12/88 (14%)]\tTrain Loss: 0.000767\n",
      "Train Epoch: 13 [16/88 (18%)]\tTrain Loss: 0.057648\n",
      "Train Epoch: 13 [20/88 (23%)]\tTrain Loss: 0.005261\n",
      "Train Epoch: 13 [24/88 (27%)]\tTrain Loss: 0.004426\n",
      "Train Epoch: 13 [28/88 (32%)]\tTrain Loss: 0.013149\n",
      "Train Epoch: 13 [32/88 (36%)]\tTrain Loss: 0.152890\n",
      "Train Epoch: 13 [36/88 (41%)]\tTrain Loss: 0.014426\n",
      "Train Epoch: 13 [40/88 (45%)]\tTrain Loss: 0.005120\n",
      "Train Epoch: 13 [44/88 (50%)]\tTrain Loss: 0.002289\n",
      "Train Epoch: 13 [48/88 (55%)]\tTrain Loss: 0.001527\n",
      "Train Epoch: 13 [52/88 (59%)]\tTrain Loss: 0.000849\n",
      "Train Epoch: 13 [56/88 (64%)]\tTrain Loss: 0.001738\n",
      "Train Epoch: 13 [60/88 (68%)]\tTrain Loss: 0.003121\n",
      "Train Epoch: 13 [64/88 (73%)]\tTrain Loss: 0.003463\n",
      "Train Epoch: 13 [68/88 (77%)]\tTrain Loss: 0.001621\n",
      "Train Epoch: 13 [72/88 (82%)]\tTrain Loss: 0.215285\n",
      "Train Epoch: 13 [76/88 (86%)]\tTrain Loss: 0.001114\n",
      "Train Epoch: 13 [80/88 (91%)]\tTrain Loss: 0.000680\n",
      "Train Epoch: 13 [84/88 (95%)]\tTrain Loss: 0.002253\n",
      "\n",
      "Train set: Average loss: 0.0164, Accuracy: 339/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00715056 0.00809463 0.00634407 0.01245297 0.00695951 0.07502689\n",
      " 0.01517049 0.00522364 0.01588068 0.01355433 0.01048486 0.00685253\n",
      " 0.00552093 0.00629701 0.015238   0.0097589  0.05349447 0.03199976\n",
      " 0.0295124  0.01032772 0.0116023  0.00478366 0.15705524 0.04418206\n",
      " 0.0159291  0.00940076 0.06432744 0.04835562 0.01301139 0.01119463\n",
      " 0.00385506 0.00682036 0.99832934 0.99723947 0.99383563 0.98953468\n",
      " 0.99775118 0.99759799 0.99783891 0.99663925 0.99775946 0.99653637\n",
      " 0.99764818 0.9926312  0.99573851 0.99836379 0.99426901 0.99784899\n",
      " 0.99682474 0.99742538 0.99652123 0.99615759 0.99804676 0.9982937\n",
      " 0.99777609 0.99778134 0.99651957 0.9982717  0.99828619 0.99837577\n",
      " 0.99787068 0.99759895 0.99826872 0.99838793 0.99725205 0.99842119\n",
      " 0.99787843 0.99840397 0.99773228 0.9981364  0.99802631 0.99657333\n",
      " 0.9982658  0.99783993 0.99797255 0.99822062 0.99836975 0.99843305\n",
      " 0.99652559 0.99821222 0.99722773 0.99766964 0.99822599 0.99795341\n",
      " 0.9978295  0.99514991 0.99816328 0.99720722 0.99783462 0.99766362\n",
      " 0.99839348 0.99813783 0.99807429 0.99777788 0.99802774 0.9974795\n",
      " 0.9984231  0.99817562 0.99814367]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 14 [0/88 (0%)]\tTrain Loss: 0.001152\n",
      "Train Epoch: 14 [4/88 (5%)]\tTrain Loss: 0.011816\n",
      "Train Epoch: 14 [8/88 (9%)]\tTrain Loss: 0.021187\n",
      "Train Epoch: 14 [12/88 (14%)]\tTrain Loss: 0.003591\n",
      "Train Epoch: 14 [16/88 (18%)]\tTrain Loss: 0.001165\n",
      "Train Epoch: 14 [20/88 (23%)]\tTrain Loss: 0.001295\n",
      "Train Epoch: 14 [24/88 (27%)]\tTrain Loss: 0.005199\n",
      "Train Epoch: 14 [28/88 (32%)]\tTrain Loss: 0.001259\n",
      "Train Epoch: 14 [32/88 (36%)]\tTrain Loss: 0.000386\n",
      "Train Epoch: 14 [36/88 (41%)]\tTrain Loss: 0.003735\n",
      "Train Epoch: 14 [40/88 (45%)]\tTrain Loss: 0.001253\n",
      "Train Epoch: 14 [44/88 (50%)]\tTrain Loss: 0.001163\n",
      "Train Epoch: 14 [48/88 (55%)]\tTrain Loss: 0.001208\n",
      "Train Epoch: 14 [52/88 (59%)]\tTrain Loss: 0.012088\n",
      "Train Epoch: 14 [56/88 (64%)]\tTrain Loss: 0.002696\n",
      "Train Epoch: 14 [60/88 (68%)]\tTrain Loss: 0.001431\n",
      "Train Epoch: 14 [64/88 (73%)]\tTrain Loss: 0.009454\n",
      "Train Epoch: 14 [68/88 (77%)]\tTrain Loss: 0.001018\n",
      "Train Epoch: 14 [72/88 (82%)]\tTrain Loss: 0.000587\n",
      "Train Epoch: 14 [76/88 (86%)]\tTrain Loss: 0.000405\n",
      "Train Epoch: 14 [80/88 (91%)]\tTrain Loss: 0.003377\n",
      "Train Epoch: 14 [84/88 (95%)]\tTrain Loss: 0.000668\n",
      "\n",
      "Train set: Average loss: 0.0066, Accuracy: 346/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00439246 0.00699579 0.00409082 0.017514   0.00387214 0.38042641\n",
      " 0.01512789 0.00327352 0.00493877 0.00897602 0.00666601 0.00463864\n",
      " 0.00453249 0.00391336 0.01445584 0.00703621 0.09252209 0.06245585\n",
      " 0.0347987  0.00710229 0.00999011 0.00344612 0.51769274 0.09900085\n",
      " 0.01761006 0.00860829 0.12510063 0.15659547 0.01135303 0.00720225\n",
      " 0.00253153 0.0030405  0.9991467  0.99926263 0.9990958  0.9988721\n",
      " 0.99923766 0.99922335 0.99920815 0.9991709  0.99923134 0.99923396\n",
      " 0.99921525 0.99902046 0.9991855  0.9991653  0.99906224 0.99925488\n",
      " 0.99922633 0.99921978 0.99918419 0.99917847 0.99921906 0.99913186\n",
      " 0.99925488 0.99924374 0.99921644 0.9991467  0.99918443 0.9990114\n",
      " 0.99924409 0.99923074 0.99914932 0.99906236 0.99923098 0.99906594\n",
      " 0.99924767 0.99918026 0.99921942 0.99921834 0.99927932 0.99918705\n",
      " 0.99916971 0.99921811 0.99920434 0.99917138 0.99907839 0.99909222\n",
      " 0.99921095 0.99915183 0.99923241 0.99926227 0.99914634 0.99921346\n",
      " 0.99929106 0.99915612 0.99916756 0.99925107 0.99924517 0.99922156\n",
      " 0.99898905 0.99918014 0.99916148 0.99922848 0.99921811 0.99925798\n",
      " 0.99909902 0.99921894 0.9992041 ]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [0/88 (0%)]\tTrain Loss: 0.000258\n",
      "Train Epoch: 15 [4/88 (5%)]\tTrain Loss: 0.000675\n",
      "Train Epoch: 15 [8/88 (9%)]\tTrain Loss: 0.001315\n",
      "Train Epoch: 15 [12/88 (14%)]\tTrain Loss: 0.000670\n",
      "Train Epoch: 15 [16/88 (18%)]\tTrain Loss: 0.013478\n",
      "Train Epoch: 15 [20/88 (23%)]\tTrain Loss: 0.013086\n",
      "Train Epoch: 15 [24/88 (27%)]\tTrain Loss: 0.001257\n",
      "Train Epoch: 15 [28/88 (32%)]\tTrain Loss: 0.000524\n",
      "Train Epoch: 15 [32/88 (36%)]\tTrain Loss: 0.000345\n",
      "Train Epoch: 15 [36/88 (41%)]\tTrain Loss: 0.002390\n",
      "Train Epoch: 15 [40/88 (45%)]\tTrain Loss: 0.004004\n",
      "Train Epoch: 15 [44/88 (50%)]\tTrain Loss: 0.000463\n",
      "Train Epoch: 15 [48/88 (55%)]\tTrain Loss: 0.001111\n",
      "Train Epoch: 15 [52/88 (59%)]\tTrain Loss: 0.000889\n",
      "Train Epoch: 15 [56/88 (64%)]\tTrain Loss: 0.000397\n",
      "Train Epoch: 15 [60/88 (68%)]\tTrain Loss: 0.001409\n",
      "Train Epoch: 15 [64/88 (73%)]\tTrain Loss: 0.000570\n",
      "Train Epoch: 15 [68/88 (77%)]\tTrain Loss: 0.000486\n",
      "Train Epoch: 15 [72/88 (82%)]\tTrain Loss: 0.000598\n",
      "Train Epoch: 15 [76/88 (86%)]\tTrain Loss: 0.000426\n",
      "Train Epoch: 15 [80/88 (91%)]\tTrain Loss: 0.001058\n",
      "Train Epoch: 15 [84/88 (95%)]\tTrain Loss: 0.002283\n",
      "\n",
      "Train set: Average loss: 0.0128, Accuracy: 344/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00199333 0.00235753 0.00201934 0.00361064 0.00188305 0.01855772\n",
      " 0.00305323 0.00206496 0.00400784 0.00252352 0.00248438 0.00204114\n",
      " 0.00236121 0.00225625 0.00308735 0.00218485 0.00604322 0.00568218\n",
      " 0.00362478 0.00251058 0.00246267 0.00193217 0.0175577  0.00688548\n",
      " 0.0032411  0.00271402 0.00635071 0.00629852 0.00277334 0.00250945\n",
      " 0.0022086  0.00288962 0.99830592 0.99626452 0.98780745 0.96894342\n",
      " 0.99733305 0.99730766 0.99760109 0.99530572 0.99754286 0.99490732\n",
      " 0.99718273 0.98497099 0.99349654 0.99830014 0.99080664 0.99740005\n",
      " 0.9953413  0.99694902 0.99460703 0.99473614 0.99778688 0.99824536\n",
      " 0.99728692 0.99733174 0.99464709 0.99816126 0.99821752 0.99826211\n",
      " 0.99756169 0.99719656 0.99811953 0.99828905 0.99657923 0.99829942\n",
      " 0.99761295 0.9983052  0.99745315 0.9979614  0.99775559 0.99583387\n",
      " 0.99816388 0.99754423 0.9977119  0.99818915 0.99831831 0.99839371\n",
      " 0.99538076 0.99814916 0.99643493 0.99712676 0.99820852 0.99785596\n",
      " 0.99751377 0.99193829 0.99809438 0.99630094 0.99757141 0.99727947\n",
      " 0.99828821 0.99794573 0.99796849 0.99748033 0.99786925 0.99702066\n",
      " 0.998353   0.99808109 0.998106  ]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 16 [0/88 (0%)]\tTrain Loss: 0.004682\n",
      "Train Epoch: 16 [4/88 (5%)]\tTrain Loss: 0.063345\n",
      "Train Epoch: 16 [8/88 (9%)]\tTrain Loss: 0.000959\n",
      "Train Epoch: 16 [12/88 (14%)]\tTrain Loss: 0.012154\n",
      "Train Epoch: 16 [16/88 (18%)]\tTrain Loss: 0.034570\n",
      "Train Epoch: 16 [20/88 (23%)]\tTrain Loss: 0.002869\n",
      "Train Epoch: 16 [24/88 (27%)]\tTrain Loss: 0.002662\n",
      "Train Epoch: 16 [28/88 (32%)]\tTrain Loss: 0.001920\n",
      "Train Epoch: 16 [32/88 (36%)]\tTrain Loss: 0.000542\n",
      "Train Epoch: 16 [36/88 (41%)]\tTrain Loss: 0.035187\n",
      "Train Epoch: 16 [40/88 (45%)]\tTrain Loss: 0.000713\n",
      "Train Epoch: 16 [44/88 (50%)]\tTrain Loss: 0.000342\n",
      "Train Epoch: 16 [48/88 (55%)]\tTrain Loss: 0.013656\n",
      "Train Epoch: 16 [52/88 (59%)]\tTrain Loss: 0.002468\n",
      "Train Epoch: 16 [56/88 (64%)]\tTrain Loss: 0.001527\n",
      "Train Epoch: 16 [60/88 (68%)]\tTrain Loss: 0.001594\n",
      "Train Epoch: 16 [64/88 (73%)]\tTrain Loss: 0.001167\n",
      "Train Epoch: 16 [68/88 (77%)]\tTrain Loss: 0.001070\n",
      "Train Epoch: 16 [72/88 (82%)]\tTrain Loss: 0.001899\n",
      "Train Epoch: 16 [76/88 (86%)]\tTrain Loss: 0.001354\n",
      "Train Epoch: 16 [80/88 (91%)]\tTrain Loss: 0.001802\n",
      "Train Epoch: 16 [84/88 (95%)]\tTrain Loss: 0.000875\n",
      "\n",
      "Train set: Average loss: 0.0139, Accuracy: 341/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00393949 0.00448929 0.00400042 0.01094179 0.00383379 0.0562725\n",
      " 0.00789102 0.00342739 0.00340764 0.00570996 0.00523496 0.00466481\n",
      " 0.0038848  0.00409165 0.00983549 0.00507709 0.01671985 0.01665853\n",
      " 0.01245623 0.00707279 0.00650195 0.00337215 0.0947661  0.02619402\n",
      " 0.00892194 0.00526383 0.01827358 0.02721303 0.00613952 0.00565407\n",
      " 0.00397042 0.00616758 0.99919087 0.99923456 0.99849772 0.99773657\n",
      " 0.99927622 0.99914241 0.99921572 0.99896872 0.99917209 0.99909985\n",
      " 0.99916661 0.99859363 0.99899203 0.9992336  0.99867475 0.99924779\n",
      " 0.99904412 0.99917042 0.99897134 0.9989416  0.99922049 0.99912554\n",
      " 0.99919802 0.99925429 0.99910849 0.99921083 0.99921083 0.9990952\n",
      " 0.99925119 0.99922454 0.99916887 0.9991973  0.99910092 0.9991641\n",
      " 0.99924988 0.99924672 0.99922538 0.99926168 0.999295   0.99893504\n",
      " 0.99925572 0.99914634 0.99919587 0.99921119 0.99916995 0.99920255\n",
      " 0.99902248 0.99915648 0.99920255 0.99921334 0.99919003 0.99921703\n",
      " 0.9993006  0.99895489 0.99914849 0.99916744 0.99923778 0.99917006\n",
      " 0.99908936 0.99917454 0.99914682 0.99921751 0.99923551 0.99925345\n",
      " 0.99916935 0.99926203 0.99920422]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 17 [0/88 (0%)]\tTrain Loss: 0.096015\n",
      "Train Epoch: 17 [4/88 (5%)]\tTrain Loss: 0.000302\n",
      "Train Epoch: 17 [8/88 (9%)]\tTrain Loss: 0.013723\n",
      "Train Epoch: 17 [12/88 (14%)]\tTrain Loss: 0.019857\n",
      "Train Epoch: 17 [16/88 (18%)]\tTrain Loss: 0.001057\n",
      "Train Epoch: 17 [20/88 (23%)]\tTrain Loss: 0.002685\n",
      "Train Epoch: 17 [24/88 (27%)]\tTrain Loss: 0.001829\n",
      "Train Epoch: 17 [28/88 (32%)]\tTrain Loss: 0.000542\n",
      "Train Epoch: 17 [32/88 (36%)]\tTrain Loss: 0.000349\n",
      "Train Epoch: 17 [36/88 (41%)]\tTrain Loss: 0.159447\n",
      "Train Epoch: 17 [40/88 (45%)]\tTrain Loss: 0.002653\n",
      "Train Epoch: 17 [44/88 (50%)]\tTrain Loss: 0.005558\n",
      "Train Epoch: 17 [48/88 (55%)]\tTrain Loss: 0.002073\n",
      "Train Epoch: 17 [52/88 (59%)]\tTrain Loss: 0.004349\n",
      "Train Epoch: 17 [56/88 (64%)]\tTrain Loss: 0.000217\n",
      "Train Epoch: 17 [60/88 (68%)]\tTrain Loss: 0.015563\n",
      "Train Epoch: 17 [64/88 (73%)]\tTrain Loss: 0.030226\n",
      "Train Epoch: 17 [68/88 (77%)]\tTrain Loss: 0.000428\n",
      "Train Epoch: 17 [72/88 (82%)]\tTrain Loss: 0.001872\n",
      "Train Epoch: 17 [76/88 (86%)]\tTrain Loss: 0.001336\n",
      "Train Epoch: 17 [80/88 (91%)]\tTrain Loss: 0.000732\n",
      "Train Epoch: 17 [84/88 (95%)]\tTrain Loss: 0.001313\n",
      "\n",
      "Train set: Average loss: 0.0096, Accuracy: 345/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00161535 0.00164644 0.00151702 0.0030484  0.00143819 0.01632521\n",
      " 0.00267959 0.00157653 0.00202824 0.00213164 0.00196806 0.00170397\n",
      " 0.00175536 0.00168901 0.00296639 0.00174909 0.00817461 0.00554578\n",
      " 0.00444487 0.00189772 0.002157   0.00136239 0.02719323 0.00724134\n",
      " 0.00316481 0.00206128 0.00784538 0.00779276 0.00222657 0.00211427\n",
      " 0.00163788 0.00255372 0.99936026 0.99919134 0.99849665 0.99742597\n",
      " 0.99932349 0.99923253 0.99930906 0.99902022 0.99929285 0.99896455\n",
      " 0.99928755 0.99792296 0.99877232 0.99936634 0.9983564  0.99933308\n",
      " 0.99907219 0.9992606  0.99898225 0.99884295 0.99930334 0.99933571\n",
      " 0.99932384 0.99926561 0.99897695 0.99936253 0.99935752 0.99927396\n",
      " 0.99934036 0.99926132 0.99934691 0.99934095 0.99918991 0.99930584\n",
      " 0.99927503 0.99937481 0.99928027 0.99938262 0.99933881 0.99884951\n",
      " 0.99938178 0.99930346 0.99931395 0.99933261 0.99933356 0.99934477\n",
      " 0.99901533 0.99932802 0.99914885 0.99925309 0.99933028 0.99932754\n",
      " 0.99931753 0.99861324 0.99933988 0.9991284  0.99929893 0.99925965\n",
      " 0.99929333 0.99934119 0.99930274 0.99929476 0.99934536 0.99920684\n",
      " 0.99933463 0.99935132 0.99933749]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 18 [0/88 (0%)]\tTrain Loss: 0.000374\n",
      "Train Epoch: 18 [4/88 (5%)]\tTrain Loss: 0.001475\n",
      "Train Epoch: 18 [8/88 (9%)]\tTrain Loss: 0.000367\n",
      "Train Epoch: 18 [12/88 (14%)]\tTrain Loss: 0.000164\n",
      "Train Epoch: 18 [16/88 (18%)]\tTrain Loss: 0.000712\n",
      "Train Epoch: 18 [20/88 (23%)]\tTrain Loss: 0.000476\n",
      "Train Epoch: 18 [24/88 (27%)]\tTrain Loss: 0.115173\n",
      "Train Epoch: 18 [28/88 (32%)]\tTrain Loss: 0.000571\n",
      "Train Epoch: 18 [32/88 (36%)]\tTrain Loss: 0.000899\n",
      "Train Epoch: 18 [36/88 (41%)]\tTrain Loss: 0.001738\n",
      "Train Epoch: 18 [40/88 (45%)]\tTrain Loss: 0.000475\n",
      "Train Epoch: 18 [44/88 (50%)]\tTrain Loss: 0.004000\n",
      "Train Epoch: 18 [48/88 (55%)]\tTrain Loss: 0.000644\n",
      "Train Epoch: 18 [52/88 (59%)]\tTrain Loss: 0.000701\n",
      "Train Epoch: 18 [56/88 (64%)]\tTrain Loss: 0.070219\n",
      "Train Epoch: 18 [60/88 (68%)]\tTrain Loss: 0.000173\n",
      "Train Epoch: 18 [64/88 (73%)]\tTrain Loss: 0.000596\n",
      "Train Epoch: 18 [68/88 (77%)]\tTrain Loss: 0.001563\n",
      "Train Epoch: 18 [72/88 (82%)]\tTrain Loss: 0.001419\n",
      "Train Epoch: 18 [76/88 (86%)]\tTrain Loss: 0.001888\n",
      "Train Epoch: 18 [80/88 (91%)]\tTrain Loss: 0.002506\n",
      "Train Epoch: 18 [84/88 (95%)]\tTrain Loss: 0.144087\n",
      "\n",
      "Train set: Average loss: 0.0107, Accuracy: 341/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00222757 0.0023282  0.00213996 0.00366024 0.00189241 0.01868124\n",
      " 0.0038893  0.00223734 0.00255602 0.00257888 0.00246434 0.00239569\n",
      " 0.00259205 0.00244037 0.00313814 0.00255432 0.00809863 0.00606611\n",
      " 0.00452486 0.00270524 0.00278959 0.00197201 0.0246236  0.00829804\n",
      " 0.00367733 0.00283291 0.00788596 0.00985757 0.00311274 0.00264012\n",
      " 0.00323013 0.00685354 0.99935502 0.99913341 0.99791843 0.99468106\n",
      " 0.99929798 0.99923658 0.99930429 0.99872524 0.99926418 0.99897587\n",
      " 0.99921906 0.99765849 0.99873513 0.99935609 0.99790561 0.99925071\n",
      " 0.99902785 0.99920887 0.99883503 0.9987821  0.99930382 0.99933857\n",
      " 0.99926263 0.99928087 0.998963   0.99934953 0.99935108 0.99928385\n",
      " 0.99931884 0.99925607 0.9993099  0.99932766 0.99914455 0.99930394\n",
      " 0.99929249 0.99938357 0.99926573 0.99935371 0.99932218 0.99890411\n",
      " 0.99936289 0.99927312 0.99931145 0.99935466 0.99933392 0.99935204\n",
      " 0.9989447  0.9993186  0.99914801 0.99923921 0.99935108 0.99931669\n",
      " 0.99925476 0.99846882 0.99932778 0.99907601 0.99928826 0.9992581\n",
      " 0.99929118 0.99932289 0.99929595 0.99927551 0.99933916 0.99921501\n",
      " 0.99934202 0.99935192 0.99934667]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 19 [0/88 (0%)]\tTrain Loss: 0.002689\n",
      "Train Epoch: 19 [4/88 (5%)]\tTrain Loss: 0.004888\n",
      "Train Epoch: 19 [8/88 (9%)]\tTrain Loss: 0.019317\n",
      "Train Epoch: 19 [12/88 (14%)]\tTrain Loss: 0.004191\n",
      "Train Epoch: 19 [16/88 (18%)]\tTrain Loss: 0.000969\n",
      "Train Epoch: 19 [20/88 (23%)]\tTrain Loss: 0.017275\n",
      "Train Epoch: 19 [24/88 (27%)]\tTrain Loss: 0.016646\n",
      "Train Epoch: 19 [28/88 (32%)]\tTrain Loss: 0.020792\n",
      "Train Epoch: 19 [32/88 (36%)]\tTrain Loss: 0.001647\n",
      "Train Epoch: 19 [36/88 (41%)]\tTrain Loss: 0.001196\n",
      "Train Epoch: 19 [40/88 (45%)]\tTrain Loss: 0.005412\n",
      "Train Epoch: 19 [44/88 (50%)]\tTrain Loss: 0.018507\n",
      "Train Epoch: 19 [48/88 (55%)]\tTrain Loss: 0.002058\n",
      "Train Epoch: 19 [52/88 (59%)]\tTrain Loss: 0.001493\n",
      "Train Epoch: 19 [56/88 (64%)]\tTrain Loss: 0.001341\n",
      "Train Epoch: 19 [60/88 (68%)]\tTrain Loss: 0.009647\n",
      "Train Epoch: 19 [64/88 (73%)]\tTrain Loss: 0.000246\n",
      "Train Epoch: 19 [68/88 (77%)]\tTrain Loss: 0.001159\n",
      "Train Epoch: 19 [72/88 (82%)]\tTrain Loss: 0.000790\n",
      "Train Epoch: 19 [76/88 (86%)]\tTrain Loss: 0.000657\n",
      "Train Epoch: 19 [80/88 (91%)]\tTrain Loss: 0.002831\n",
      "Train Epoch: 19 [84/88 (95%)]\tTrain Loss: 0.001082\n",
      "\n",
      "Train set: Average loss: 0.0142, Accuracy: 343/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00383581 0.00420522 0.00376691 0.0087582  0.00314327 0.0513319\n",
      " 0.00728841 0.00343308 0.00448383 0.0048715  0.00467171 0.00408468\n",
      " 0.00541629 0.0044258  0.00755506 0.00428989 0.01854455 0.0131392\n",
      " 0.00989917 0.00475811 0.00552151 0.0030461  0.07191249 0.01901346\n",
      " 0.00785138 0.00518892 0.01565035 0.0213537  0.00544409 0.00487634\n",
      " 0.00384754 0.00705514 0.99950004 0.99948043 0.99916661 0.998698\n",
      " 0.99953985 0.99949753 0.99952805 0.99932337 0.99951446 0.99941993\n",
      " 0.99950898 0.99907112 0.99931562 0.99951303 0.99913031 0.99952424\n",
      " 0.99945205 0.99951851 0.99936277 0.99934465 0.99951124 0.99949336\n",
      " 0.99952495 0.99951053 0.99941766 0.99950945 0.9995141  0.9994241\n",
      " 0.99954307 0.99950469 0.99948138 0.99945909 0.99948716 0.99944121\n",
      " 0.99950528 0.99952149 0.99949253 0.99954432 0.99953449 0.99933404\n",
      " 0.99951839 0.9995203  0.99951398 0.9995147  0.99945909 0.99947637\n",
      " 0.99943167 0.99948537 0.99947816 0.99950147 0.999506   0.99952161\n",
      " 0.99953401 0.99924123 0.99951315 0.99944228 0.99950731 0.99950957\n",
      " 0.99941421 0.99952137 0.99949408 0.9995141  0.999529   0.99948955\n",
      " 0.99947459 0.99953461 0.99952555]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 [0/88 (0%)]\tTrain Loss: 0.000381\n",
      "Train Epoch: 20 [4/88 (5%)]\tTrain Loss: 0.001049\n",
      "Train Epoch: 20 [8/88 (9%)]\tTrain Loss: 0.055225\n",
      "Train Epoch: 20 [12/88 (14%)]\tTrain Loss: 0.002664\n",
      "Train Epoch: 20 [16/88 (18%)]\tTrain Loss: 0.002474\n",
      "Train Epoch: 20 [20/88 (23%)]\tTrain Loss: 0.000354\n",
      "Train Epoch: 20 [24/88 (27%)]\tTrain Loss: 0.000958\n",
      "Train Epoch: 20 [28/88 (32%)]\tTrain Loss: 0.045996\n",
      "Train Epoch: 20 [32/88 (36%)]\tTrain Loss: 0.000285\n",
      "Train Epoch: 20 [36/88 (41%)]\tTrain Loss: 0.002038\n",
      "Train Epoch: 20 [40/88 (45%)]\tTrain Loss: 0.000645\n",
      "Train Epoch: 20 [44/88 (50%)]\tTrain Loss: 0.000289\n",
      "Train Epoch: 20 [48/88 (55%)]\tTrain Loss: 0.002313\n",
      "Train Epoch: 20 [52/88 (59%)]\tTrain Loss: 0.002433\n",
      "Train Epoch: 20 [56/88 (64%)]\tTrain Loss: 0.000508\n",
      "Train Epoch: 20 [60/88 (68%)]\tTrain Loss: 0.000476\n",
      "Train Epoch: 20 [64/88 (73%)]\tTrain Loss: 0.071362\n",
      "Train Epoch: 20 [68/88 (77%)]\tTrain Loss: 0.001667\n",
      "Train Epoch: 20 [72/88 (82%)]\tTrain Loss: 0.017062\n",
      "Train Epoch: 20 [76/88 (86%)]\tTrain Loss: 0.000477\n",
      "Train Epoch: 20 [80/88 (91%)]\tTrain Loss: 0.002539\n",
      "Train Epoch: 20 [84/88 (95%)]\tTrain Loss: 0.006097\n",
      "\n",
      "Train set: Average loss: 0.0104, Accuracy: 341/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.0018494  0.00240879 0.00188476 0.00575191 0.00163861 0.06576769\n",
      " 0.00441422 0.0017693  0.00157392 0.00362356 0.00274069 0.00213758\n",
      " 0.00237361 0.00191615 0.00458198 0.00259675 0.01761808 0.01983083\n",
      " 0.00861802 0.00308379 0.00359536 0.00191948 0.05714416 0.02335262\n",
      " 0.00483035 0.00311585 0.01803374 0.02257558 0.00464186 0.00286326\n",
      " 0.00168089 0.00241713 0.99862802 0.99723428 0.99372029 0.98820168\n",
      " 0.99838471 0.99795437 0.99827969 0.99679935 0.99795938 0.99646097\n",
      " 0.99783981 0.99148482 0.99589288 0.99869931 0.99393314 0.9982577\n",
      " 0.99647874 0.9979412  0.99603832 0.99605966 0.99846977 0.99855131\n",
      " 0.99787545 0.99787676 0.99612916 0.9986701  0.99861372 0.99859434\n",
      " 0.99815601 0.99802411 0.99831176 0.99877447 0.99736637 0.9985252\n",
      " 0.99813038 0.99873155 0.99787557 0.99848408 0.99840146 0.99704319\n",
      " 0.99868876 0.99809402 0.99818581 0.99859744 0.9986248  0.99874532\n",
      " 0.99660218 0.99841511 0.99748373 0.99767095 0.99860793 0.9983303\n",
      " 0.99806005 0.99459708 0.99849272 0.99720228 0.99818003 0.99783331\n",
      " 0.99873883 0.99853396 0.99827147 0.99811977 0.99845374 0.99781871\n",
      " 0.99866271 0.99855024 0.99836498]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "TP= 67 TN= 32 FN= 0 FP= 0\n",
      "TP+FP 67\n",
      "precision 1.0\n",
      "recall 1.0\n",
      "F1 1.0\n",
      "acc 1.0\n",
      "AUCp 1.0\n",
      "AUC 1.0\n",
      "Train Epoch: 21 [0/88 (0%)]\tTrain Loss: 0.001649\n",
      "Train Epoch: 21 [4/88 (5%)]\tTrain Loss: 0.007538\n",
      "Train Epoch: 21 [8/88 (9%)]\tTrain Loss: 0.004966\n",
      "Train Epoch: 21 [12/88 (14%)]\tTrain Loss: 0.008582\n",
      "Train Epoch: 21 [16/88 (18%)]\tTrain Loss: 0.000714\n",
      "Train Epoch: 21 [20/88 (23%)]\tTrain Loss: 0.004879\n",
      "Train Epoch: 21 [24/88 (27%)]\tTrain Loss: 0.094968\n",
      "Train Epoch: 21 [28/88 (32%)]\tTrain Loss: 0.008887\n",
      "Train Epoch: 21 [32/88 (36%)]\tTrain Loss: 0.023859\n",
      "Train Epoch: 21 [36/88 (41%)]\tTrain Loss: 0.003015\n",
      "Train Epoch: 21 [40/88 (45%)]\tTrain Loss: 0.004029\n",
      "Train Epoch: 21 [44/88 (50%)]\tTrain Loss: 0.002146\n",
      "Train Epoch: 21 [48/88 (55%)]\tTrain Loss: 0.000346\n",
      "Train Epoch: 21 [52/88 (59%)]\tTrain Loss: 0.002761\n",
      "Train Epoch: 21 [56/88 (64%)]\tTrain Loss: 0.002360\n",
      "Train Epoch: 21 [60/88 (68%)]\tTrain Loss: 0.138255\n",
      "Train Epoch: 21 [64/88 (73%)]\tTrain Loss: 0.004921\n",
      "Train Epoch: 21 [68/88 (77%)]\tTrain Loss: 0.001615\n",
      "Train Epoch: 21 [72/88 (82%)]\tTrain Loss: 0.000680\n",
      "Train Epoch: 21 [76/88 (86%)]\tTrain Loss: 0.001195\n",
      "Train Epoch: 21 [80/88 (91%)]\tTrain Loss: 0.008710\n",
      "Train Epoch: 21 [84/88 (95%)]\tTrain Loss: 0.001136\n",
      "\n",
      "Train set: Average loss: 0.0179, Accuracy: 341/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00393451 0.00417071 0.00338289 0.00854506 0.00338418 0.05149216\n",
      " 0.00780765 0.00342769 0.01082957 0.00468469 0.0048726  0.00372629\n",
      " 0.00448745 0.00376032 0.00722309 0.00441957 0.01935372 0.01286262\n",
      " 0.01174978 0.00424561 0.00528637 0.00306982 0.06574368 0.01820937\n",
      " 0.00804405 0.00525497 0.01583013 0.0200249  0.00538597 0.00478064\n",
      " 0.00304397 0.00323939 0.99939024 0.99929488 0.99889886 0.99816185\n",
      " 0.99942589 0.99936837 0.99941993 0.99917549 0.9993943  0.99923313\n",
      " 0.99936372 0.99866939 0.9990921  0.99938869 0.99878174 0.99936396\n",
      " 0.99928325 0.99938488 0.99920315 0.99916422 0.99937469 0.99939573\n",
      " 0.99938738 0.99938893 0.99923241 0.99940479 0.99939549 0.99929786\n",
      " 0.99942017 0.99938023 0.99935812 0.99933594 0.99933523 0.99928838\n",
      " 0.99938071 0.99939895 0.99935883 0.99942845 0.99937731 0.99921274\n",
      " 0.99940264 0.99939775 0.9994055  0.99940848 0.99936801 0.99935955\n",
      " 0.99925286 0.9993825  0.99931133 0.99935597 0.99941003 0.99941874\n",
      " 0.99935716 0.99891222 0.99940193 0.99927944 0.99938273 0.99939418\n",
      " 0.99930429 0.99942255 0.99939442 0.99937868 0.99941182 0.99937338\n",
      " 0.99933785 0.99941242 0.99941516]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 22 [0/88 (0%)]\tTrain Loss: 0.000519\n",
      "Train Epoch: 22 [4/88 (5%)]\tTrain Loss: 0.001108\n",
      "Train Epoch: 22 [8/88 (9%)]\tTrain Loss: 0.000229\n",
      "Train Epoch: 22 [12/88 (14%)]\tTrain Loss: 0.002023\n",
      "Train Epoch: 22 [16/88 (18%)]\tTrain Loss: 0.000997\n",
      "Train Epoch: 22 [20/88 (23%)]\tTrain Loss: 0.000388\n",
      "Train Epoch: 22 [24/88 (27%)]\tTrain Loss: 0.018434\n",
      "Train Epoch: 22 [28/88 (32%)]\tTrain Loss: 0.001302\n",
      "Train Epoch: 22 [32/88 (36%)]\tTrain Loss: 0.000513\n",
      "Train Epoch: 22 [36/88 (41%)]\tTrain Loss: 0.000674\n",
      "Train Epoch: 22 [40/88 (45%)]\tTrain Loss: 0.000300\n",
      "Train Epoch: 22 [44/88 (50%)]\tTrain Loss: 0.001239\n",
      "Train Epoch: 22 [48/88 (55%)]\tTrain Loss: 0.000372\n",
      "Train Epoch: 22 [52/88 (59%)]\tTrain Loss: 0.003452\n",
      "Train Epoch: 22 [56/88 (64%)]\tTrain Loss: 0.001193\n",
      "Train Epoch: 22 [60/88 (68%)]\tTrain Loss: 0.000574\n",
      "Train Epoch: 22 [64/88 (73%)]\tTrain Loss: 0.000592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 22 [68/88 (77%)]\tTrain Loss: 0.000333\n",
      "Train Epoch: 22 [72/88 (82%)]\tTrain Loss: 0.000363\n",
      "Train Epoch: 22 [76/88 (86%)]\tTrain Loss: 0.002219\n",
      "Train Epoch: 22 [80/88 (91%)]\tTrain Loss: 0.000185\n",
      "Train Epoch: 22 [84/88 (95%)]\tTrain Loss: 0.000512\n",
      "\n",
      "Train set: Average loss: 0.0068, Accuracy: 347/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00264476 0.00314391 0.00253152 0.01262075 0.0027908  0.14046241\n",
      " 0.00656532 0.00216476 0.00319732 0.00413953 0.00366959 0.00296411\n",
      " 0.00263644 0.0023699  0.00918716 0.00312238 0.02283591 0.01891839\n",
      " 0.01486566 0.00370134 0.00484918 0.00207835 0.21902601 0.03138921\n",
      " 0.01028255 0.00414569 0.02281236 0.04058771 0.00429685 0.0040701\n",
      " 0.001847   0.00209888 0.99952996 0.99955553 0.99939132 0.99914372\n",
      " 0.9995932  0.99955732 0.99957627 0.99948078 0.99956769 0.99950802\n",
      " 0.99955648 0.9992879  0.99944907 0.99953413 0.99933332 0.99957973\n",
      " 0.99953747 0.99956888 0.99949038 0.99947196 0.99954247 0.99952602\n",
      " 0.99956995 0.99956709 0.99950778 0.99953723 0.99954623 0.99944049\n",
      " 0.99958485 0.99956697 0.99951208 0.99948019 0.99956018 0.99944693\n",
      " 0.99955672 0.99953878 0.99955255 0.99957472 0.99957365 0.99946398\n",
      " 0.99954933 0.99956042 0.99955684 0.99954408 0.99950099 0.99949729\n",
      " 0.99951887 0.99952233 0.99953973 0.99955541 0.99954128 0.99957329\n",
      " 0.9995721  0.99939859 0.99954212 0.99953568 0.99956745 0.99956781\n",
      " 0.99944216 0.99955291 0.99954045 0.99956387 0.99957269 0.99956483\n",
      " 0.99949253 0.99956089 0.99956173]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 23 [0/88 (0%)]\tTrain Loss: 0.005179\n",
      "Train Epoch: 23 [4/88 (5%)]\tTrain Loss: 0.001447\n",
      "Train Epoch: 23 [8/88 (9%)]\tTrain Loss: 0.000409\n",
      "Train Epoch: 23 [12/88 (14%)]\tTrain Loss: 0.011593\n",
      "Train Epoch: 23 [16/88 (18%)]\tTrain Loss: 0.000358\n",
      "Train Epoch: 23 [20/88 (23%)]\tTrain Loss: 0.001770\n",
      "Train Epoch: 23 [24/88 (27%)]\tTrain Loss: 0.002754\n",
      "Train Epoch: 23 [28/88 (32%)]\tTrain Loss: 0.000665\n",
      "Train Epoch: 23 [32/88 (36%)]\tTrain Loss: 0.000452\n",
      "Train Epoch: 23 [36/88 (41%)]\tTrain Loss: 0.000700\n",
      "Train Epoch: 23 [40/88 (45%)]\tTrain Loss: 0.002266\n",
      "Train Epoch: 23 [44/88 (50%)]\tTrain Loss: 0.000504\n",
      "Train Epoch: 23 [48/88 (55%)]\tTrain Loss: 0.000736\n",
      "Train Epoch: 23 [52/88 (59%)]\tTrain Loss: 0.002157\n",
      "Train Epoch: 23 [56/88 (64%)]\tTrain Loss: 0.000826\n",
      "Train Epoch: 23 [60/88 (68%)]\tTrain Loss: 0.001865\n",
      "Train Epoch: 23 [64/88 (73%)]\tTrain Loss: 0.003506\n",
      "Train Epoch: 23 [68/88 (77%)]\tTrain Loss: 0.000373\n",
      "Train Epoch: 23 [72/88 (82%)]\tTrain Loss: 0.000636\n",
      "Train Epoch: 23 [76/88 (86%)]\tTrain Loss: 0.011800\n",
      "Train Epoch: 23 [80/88 (91%)]\tTrain Loss: 0.006141\n",
      "Train Epoch: 23 [84/88 (95%)]\tTrain Loss: 0.001899\n",
      "\n",
      "Train set: Average loss: 0.0076, Accuracy: 343/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00211685 0.00197193 0.00188627 0.00446501 0.0016923  0.01690422\n",
      " 0.00329929 0.00210677 0.00230484 0.00226483 0.00259262 0.00213097\n",
      " 0.00449004 0.00250732 0.00369598 0.0022403  0.00530423 0.00443143\n",
      " 0.00474126 0.00249389 0.0025751  0.00170315 0.01524403 0.00649262\n",
      " 0.00331731 0.00244866 0.00462449 0.00585679 0.00247036 0.00223984\n",
      " 0.00225825 0.0035372  0.99954551 0.99930346 0.99837983 0.99685317\n",
      " 0.99950385 0.99940932 0.99948621 0.99907887 0.99945253 0.99915636\n",
      " 0.99943465 0.99812156 0.99899441 0.99954885 0.99836069 0.99941564\n",
      " 0.99922502 0.99944979 0.99910295 0.99905926 0.99948931 0.99953401\n",
      " 0.99947518 0.99943334 0.99909174 0.999542   0.99953353 0.99948728\n",
      " 0.99950624 0.9994092  0.99947947 0.99952686 0.99932468 0.99948514\n",
      " 0.99944884 0.99955422 0.9994272  0.99954504 0.99946684 0.99917883\n",
      " 0.99952149 0.99947661 0.99951148 0.99955028 0.99954045 0.99954212\n",
      " 0.9991411  0.99951327 0.999318   0.99937505 0.99953938 0.99951458\n",
      " 0.99942774 0.9986431  0.99953663 0.99929357 0.99942845 0.99942374\n",
      " 0.99951196 0.99954849 0.99948967 0.9994579  0.99952054 0.9994235\n",
      " 0.99952662 0.99953294 0.99951959]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 24 [0/88 (0%)]\tTrain Loss: 0.005856\n",
      "Train Epoch: 24 [4/88 (5%)]\tTrain Loss: 0.000566\n",
      "Train Epoch: 24 [8/88 (9%)]\tTrain Loss: 0.003017\n",
      "Train Epoch: 24 [12/88 (14%)]\tTrain Loss: 0.001063\n",
      "Train Epoch: 24 [16/88 (18%)]\tTrain Loss: 0.000941\n",
      "Train Epoch: 24 [20/88 (23%)]\tTrain Loss: 0.000575\n",
      "Train Epoch: 24 [24/88 (27%)]\tTrain Loss: 0.000588\n",
      "Train Epoch: 24 [28/88 (32%)]\tTrain Loss: 0.000345\n",
      "Train Epoch: 24 [32/88 (36%)]\tTrain Loss: 0.094740\n",
      "Train Epoch: 24 [36/88 (41%)]\tTrain Loss: 0.000566\n",
      "Train Epoch: 24 [40/88 (45%)]\tTrain Loss: 0.002343\n",
      "Train Epoch: 24 [44/88 (50%)]\tTrain Loss: 0.001816\n",
      "Train Epoch: 24 [48/88 (55%)]\tTrain Loss: 0.003953\n",
      "Train Epoch: 24 [52/88 (59%)]\tTrain Loss: 0.025458\n",
      "Train Epoch: 24 [56/88 (64%)]\tTrain Loss: 0.021321\n",
      "Train Epoch: 24 [60/88 (68%)]\tTrain Loss: 0.000803\n",
      "Train Epoch: 24 [64/88 (73%)]\tTrain Loss: 0.000981\n",
      "Train Epoch: 24 [68/88 (77%)]\tTrain Loss: 0.002337\n",
      "Train Epoch: 24 [72/88 (82%)]\tTrain Loss: 0.001012\n",
      "Train Epoch: 24 [76/88 (86%)]\tTrain Loss: 0.285832\n",
      "Train Epoch: 24 [80/88 (91%)]\tTrain Loss: 0.009396\n",
      "Train Epoch: 24 [84/88 (95%)]\tTrain Loss: 0.001199\n",
      "\n",
      "Train set: Average loss: 0.0091, Accuracy: 345/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.01028501 0.0181735  0.01133652 0.04157392 0.01177502 0.73334712\n",
      " 0.04708474 0.00712009 0.0210133  0.04556033 0.01727931 0.01177222\n",
      " 0.00685199 0.00711735 0.03695845 0.02314606 0.36967188 0.21566328\n",
      " 0.13810541 0.02049104 0.02842365 0.00764335 0.73513943 0.23638603\n",
      " 0.07069459 0.02720073 0.29097414 0.36297309 0.04694629 0.02500087\n",
      " 0.0034478  0.00498376 0.99961758 0.99964201 0.99950516 0.99922419\n",
      " 0.99966717 0.99963105 0.99964476 0.99958485 0.99964666 0.99961251\n",
      " 0.9996295  0.99947947 0.99956626 0.99962556 0.99947959 0.99964309\n",
      " 0.9996146  0.99964046 0.99960071 0.99959844 0.99963629 0.99961072\n",
      " 0.99965501 0.99965405 0.99960726 0.99962664 0.99963522 0.99954176\n",
      " 0.99965668 0.9996587  0.99960274 0.99959332 0.99963927 0.99955422\n",
      " 0.99964786 0.99963117 0.99965739 0.99966276 0.99966407 0.9995963\n",
      " 0.99963212 0.99964404 0.99963927 0.99963284 0.99960238 0.99959582\n",
      " 0.99960548 0.99960798 0.999632   0.9996376  0.99962711 0.99965429\n",
      " 0.99965    0.99954969 0.99962306 0.99961603 0.99965155 0.99964452\n",
      " 0.99956137 0.99963331 0.99961913 0.99965775 0.99964607 0.99965668\n",
      " 0.99959928 0.99964702 0.99963403]\n",
      "predict [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 25 [0/88 (0%)]\tTrain Loss: 0.006607\n",
      "Train Epoch: 25 [4/88 (5%)]\tTrain Loss: 0.057581\n",
      "Train Epoch: 25 [8/88 (9%)]\tTrain Loss: 0.005086\n",
      "Train Epoch: 25 [12/88 (14%)]\tTrain Loss: 0.003529\n",
      "Train Epoch: 25 [16/88 (18%)]\tTrain Loss: 0.001551\n",
      "Train Epoch: 25 [20/88 (23%)]\tTrain Loss: 0.000889\n",
      "Train Epoch: 25 [24/88 (27%)]\tTrain Loss: 0.000550\n",
      "Train Epoch: 25 [28/88 (32%)]\tTrain Loss: 0.000462\n",
      "Train Epoch: 25 [32/88 (36%)]\tTrain Loss: 0.000254\n",
      "Train Epoch: 25 [36/88 (41%)]\tTrain Loss: 0.133126\n",
      "Train Epoch: 25 [40/88 (45%)]\tTrain Loss: 0.000822\n",
      "Train Epoch: 25 [44/88 (50%)]\tTrain Loss: 0.003822\n",
      "Train Epoch: 25 [48/88 (55%)]\tTrain Loss: 0.000918\n",
      "Train Epoch: 25 [52/88 (59%)]\tTrain Loss: 0.047097\n",
      "Train Epoch: 25 [56/88 (64%)]\tTrain Loss: 0.000766\n",
      "Train Epoch: 25 [60/88 (68%)]\tTrain Loss: 0.001481\n",
      "Train Epoch: 25 [64/88 (73%)]\tTrain Loss: 0.009564\n",
      "Train Epoch: 25 [68/88 (77%)]\tTrain Loss: 0.000747\n",
      "Train Epoch: 25 [72/88 (82%)]\tTrain Loss: 0.007065\n",
      "Train Epoch: 25 [76/88 (86%)]\tTrain Loss: 0.001660\n",
      "Train Epoch: 25 [80/88 (91%)]\tTrain Loss: 0.001591\n",
      "Train Epoch: 25 [84/88 (95%)]\tTrain Loss: 0.012278\n",
      "\n",
      "Train set: Average loss: 0.0141, Accuracy: 339/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00285678 0.00311914 0.0031122  0.00620883 0.00269004 0.02935369\n",
      " 0.00465352 0.00258497 0.00435265 0.00416023 0.00360263 0.00330051\n",
      " 0.00337169 0.00317072 0.00578815 0.00330795 0.01204088 0.00952979\n",
      " 0.00778551 0.00410646 0.00425402 0.00257382 0.035197   0.01152073\n",
      " 0.00672922 0.00414255 0.00959342 0.01248421 0.00421786 0.00443339\n",
      " 0.00239556 0.00306487 0.99926394 0.99846315 0.99638051 0.99148983\n",
      " 0.99899226 0.99874276 0.99899119 0.99801362 0.99898857 0.99796259\n",
      " 0.9989391  0.99523658 0.99724257 0.99932158 0.99637848 0.99886203\n",
      " 0.99824846 0.99886191 0.99801457 0.99776828 0.99906427 0.99929869\n",
      " 0.9990688  0.99893397 0.99811196 0.99931192 0.99924254 0.99934131\n",
      " 0.9989832  0.99882001 0.99921    0.99936002 0.99870932 0.99931097\n",
      " 0.99889755 0.99931777 0.9988808  0.99924821 0.99898416 0.99806672\n",
      " 0.99925214 0.99908161 0.99910033 0.99922192 0.99931788 0.99937785\n",
      " 0.99815291 0.99922907 0.99844474 0.99876475 0.99926704 0.99905139\n",
      " 0.99883252 0.99667692 0.99922311 0.99835569 0.99892431 0.99880624\n",
      " 0.99936169 0.99924588 0.99917966 0.99905056 0.99910849 0.99866712\n",
      " 0.99936765 0.99919719 0.99918455]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 26 [0/88 (0%)]\tTrain Loss: 0.000599\n",
      "Train Epoch: 26 [4/88 (5%)]\tTrain Loss: 0.000690\n",
      "Train Epoch: 26 [8/88 (9%)]\tTrain Loss: 0.002527\n",
      "Train Epoch: 26 [12/88 (14%)]\tTrain Loss: 0.000723\n",
      "Train Epoch: 26 [16/88 (18%)]\tTrain Loss: 0.008655\n",
      "Train Epoch: 26 [20/88 (23%)]\tTrain Loss: 0.000790\n",
      "Train Epoch: 26 [24/88 (27%)]\tTrain Loss: 0.001522\n",
      "Train Epoch: 26 [28/88 (32%)]\tTrain Loss: 0.000987\n",
      "Train Epoch: 26 [32/88 (36%)]\tTrain Loss: 0.001381\n",
      "Train Epoch: 26 [36/88 (41%)]\tTrain Loss: 0.001729\n",
      "Train Epoch: 26 [40/88 (45%)]\tTrain Loss: 0.000355\n",
      "Train Epoch: 26 [44/88 (50%)]\tTrain Loss: 0.156538\n",
      "Train Epoch: 26 [48/88 (55%)]\tTrain Loss: 0.000808\n",
      "Train Epoch: 26 [52/88 (59%)]\tTrain Loss: 0.002004\n",
      "Train Epoch: 26 [56/88 (64%)]\tTrain Loss: 0.006175\n",
      "Train Epoch: 26 [60/88 (68%)]\tTrain Loss: 0.000167\n",
      "Train Epoch: 26 [64/88 (73%)]\tTrain Loss: 0.048995\n",
      "Train Epoch: 26 [68/88 (77%)]\tTrain Loss: 0.016952\n",
      "Train Epoch: 26 [72/88 (82%)]\tTrain Loss: 0.004697\n",
      "Train Epoch: 26 [76/88 (86%)]\tTrain Loss: 0.000958\n",
      "Train Epoch: 26 [80/88 (91%)]\tTrain Loss: 0.000454\n",
      "Train Epoch: 26 [84/88 (95%)]\tTrain Loss: 0.000409\n",
      "\n",
      "Train set: Average loss: 0.0064, Accuracy: 346/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00655651 0.00763855 0.0067386  0.01701362 0.00525499 0.21566147\n",
      " 0.0147411  0.00573438 0.00888788 0.00943992 0.00768402 0.00736835\n",
      " 0.00766604 0.00645003 0.01348788 0.00909543 0.05690501 0.03853339\n",
      " 0.02735129 0.00888295 0.01061193 0.00495286 0.26427907 0.0580963\n",
      " 0.01868657 0.00986042 0.06780136 0.08895664 0.01199375 0.00928348\n",
      " 0.00504215 0.00772566 0.99968064 0.99963868 0.99944264 0.9990409\n",
      " 0.99970067 0.99966013 0.999686   0.99955565 0.99967825 0.99960428\n",
      " 0.9996376  0.9993788  0.99954635 0.99969506 0.99941957 0.99963057\n",
      " 0.99961972 0.99966335 0.99956876 0.99959916 0.999668   0.99967992\n",
      " 0.99966609 0.99966669 0.99956912 0.99968684 0.99968648 0.9996202\n",
      " 0.99969184 0.99969065 0.99964011 0.9996649  0.99964178 0.99963152\n",
      " 0.99967933 0.99969637 0.99966824 0.99970132 0.99968469 0.99957997\n",
      " 0.99966824 0.9996748  0.99967134 0.9997009  0.99967539 0.99966455\n",
      " 0.99960929 0.99966085 0.99963641 0.99965036 0.99968779 0.99969637\n",
      " 0.99966729 0.99947816 0.99967301 0.99961865 0.99967301 0.99966633\n",
      " 0.99964869 0.99969065 0.9996655  0.99968171 0.99968171 0.9996742\n",
      " 0.99966288 0.99970102 0.9996866 ]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 27 [0/88 (0%)]\tTrain Loss: 0.000177\n",
      "Train Epoch: 27 [4/88 (5%)]\tTrain Loss: 0.004519\n",
      "Train Epoch: 27 [8/88 (9%)]\tTrain Loss: 0.003635\n",
      "Train Epoch: 27 [12/88 (14%)]\tTrain Loss: 0.000145\n",
      "Train Epoch: 27 [16/88 (18%)]\tTrain Loss: 0.000365\n",
      "Train Epoch: 27 [20/88 (23%)]\tTrain Loss: 0.033572\n",
      "Train Epoch: 27 [24/88 (27%)]\tTrain Loss: 0.000203\n",
      "Train Epoch: 27 [28/88 (32%)]\tTrain Loss: 0.003136\n",
      "Train Epoch: 27 [32/88 (36%)]\tTrain Loss: 0.000739\n",
      "Train Epoch: 27 [36/88 (41%)]\tTrain Loss: 0.000145\n",
      "Train Epoch: 27 [40/88 (45%)]\tTrain Loss: 0.002555\n",
      "Train Epoch: 27 [44/88 (50%)]\tTrain Loss: 0.000278\n",
      "Train Epoch: 27 [48/88 (55%)]\tTrain Loss: 0.000462\n",
      "Train Epoch: 27 [52/88 (59%)]\tTrain Loss: 0.001510\n",
      "Train Epoch: 27 [56/88 (64%)]\tTrain Loss: 0.000471\n",
      "Train Epoch: 27 [60/88 (68%)]\tTrain Loss: 0.000592\n",
      "Train Epoch: 27 [64/88 (73%)]\tTrain Loss: 0.001796\n",
      "Train Epoch: 27 [68/88 (77%)]\tTrain Loss: 0.001801\n",
      "Train Epoch: 27 [72/88 (82%)]\tTrain Loss: 0.012194\n",
      "Train Epoch: 27 [76/88 (86%)]\tTrain Loss: 0.001141\n",
      "Train Epoch: 27 [80/88 (91%)]\tTrain Loss: 0.003033\n",
      "Train Epoch: 27 [84/88 (95%)]\tTrain Loss: 0.007753\n",
      "\n",
      "Train set: Average loss: 0.0072, Accuracy: 344/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00156163 0.00171499 0.00179496 0.00440537 0.00150462 0.02024583\n",
      " 0.0024004  0.00158142 0.00198229 0.00225635 0.00196222 0.00185361\n",
      " 0.00182907 0.00168844 0.00374305 0.00185318 0.00501635 0.0051915\n",
      " 0.00402884 0.00238495 0.00249894 0.00138978 0.02858124 0.00659714\n",
      " 0.00439298 0.00251895 0.00455746 0.00573532 0.00213867 0.00239664\n",
      " 0.00146565 0.00169332 0.99965727 0.99960667 0.99944073 0.99909461\n",
      " 0.99967647 0.99962997 0.99967015 0.9995504  0.99967086 0.99952197\n",
      " 0.99966359 0.99926335 0.99944359 0.9996624  0.99936467 0.9996562\n",
      " 0.99958605 0.9996717  0.99955004 0.99951136 0.99963081 0.99967515\n",
      " 0.99966347 0.99963546 0.99954247 0.99968147 0.99965703 0.99960691\n",
      " 0.99968088 0.99962902 0.99963474 0.99962389 0.99964142 0.99957532\n",
      " 0.99962556 0.99966156 0.99962103 0.99969816 0.99964535 0.99948847\n",
      " 0.99966836 0.99966991 0.99966156 0.99966168 0.99963915 0.99964714\n",
      " 0.99958664 0.99965465 0.99957746 0.99962938 0.99966931 0.9996686\n",
      " 0.99964952 0.99936992 0.99967015 0.99956912 0.99963617 0.99963903\n",
      " 0.99961436 0.99968207 0.99965966 0.9996624  0.99966967 0.99960941\n",
      " 0.99962568 0.99967659 0.99967432]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 28 [0/88 (0%)]\tTrain Loss: 0.001217\n",
      "Train Epoch: 28 [4/88 (5%)]\tTrain Loss: 0.000181\n",
      "Train Epoch: 28 [8/88 (9%)]\tTrain Loss: 0.000075\n",
      "Train Epoch: 28 [12/88 (14%)]\tTrain Loss: 0.000514\n",
      "Train Epoch: 28 [16/88 (18%)]\tTrain Loss: 0.000171\n",
      "Train Epoch: 28 [20/88 (23%)]\tTrain Loss: 0.001331\n",
      "Train Epoch: 28 [24/88 (27%)]\tTrain Loss: 0.000532\n",
      "Train Epoch: 28 [28/88 (32%)]\tTrain Loss: 0.069580\n",
      "Train Epoch: 28 [32/88 (36%)]\tTrain Loss: 0.000976\n",
      "Train Epoch: 28 [36/88 (41%)]\tTrain Loss: 0.000959\n",
      "Train Epoch: 28 [40/88 (45%)]\tTrain Loss: 0.000176\n",
      "Train Epoch: 28 [44/88 (50%)]\tTrain Loss: 0.009530\n",
      "Train Epoch: 28 [48/88 (55%)]\tTrain Loss: 0.000233\n",
      "Train Epoch: 28 [52/88 (59%)]\tTrain Loss: 0.000741\n",
      "Train Epoch: 28 [56/88 (64%)]\tTrain Loss: 0.000538\n",
      "Train Epoch: 28 [60/88 (68%)]\tTrain Loss: 0.000608\n",
      "Train Epoch: 28 [64/88 (73%)]\tTrain Loss: 0.000092\n",
      "Train Epoch: 28 [68/88 (77%)]\tTrain Loss: 0.000356\n",
      "Train Epoch: 28 [72/88 (82%)]\tTrain Loss: 0.000200\n",
      "Train Epoch: 28 [76/88 (86%)]\tTrain Loss: 0.000717\n",
      "Train Epoch: 28 [80/88 (91%)]\tTrain Loss: 0.009986\n",
      "Train Epoch: 28 [84/88 (95%)]\tTrain Loss: 0.010621\n",
      "\n",
      "Train set: Average loss: 0.0060, Accuracy: 346/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00524295 0.00848475 0.00440474 0.01215015 0.00408302 0.30789191\n",
      " 0.03099731 0.0041705  0.0100386  0.01947016 0.00713607 0.00507837\n",
      " 0.00596176 0.00387055 0.0113497  0.0113132  0.16265889 0.09270272\n",
      " 0.05601531 0.00711741 0.00896267 0.00335397 0.3261992  0.10642982\n",
      " 0.01351821 0.00797607 0.14299762 0.15818775 0.03269483 0.0075403\n",
      " 0.0031449  0.00567371 0.99973673 0.99968767 0.99943632 0.99895668\n",
      " 0.99974138 0.99971241 0.99973291 0.99958438 0.99972385 0.99963939\n",
      " 0.99969399 0.99933076 0.99957341 0.99974412 0.99940598 0.9997018\n",
      " 0.99966645 0.9997099  0.99961096 0.99961686 0.99972135 0.99973518\n",
      " 0.99971801 0.99971932 0.99962628 0.99974149 0.99973923 0.99969983\n",
      " 0.99973768 0.99973148 0.99970394 0.99972874 0.99968684 0.99970323\n",
      " 0.99972755 0.99975199 0.99971539 0.99974936 0.99973136 0.99962366\n",
      " 0.99973565 0.99972361 0.99972433 0.99974996 0.99972945 0.99972886\n",
      " 0.99965191 0.99972135 0.99968576 0.99970526 0.9997403  0.99974006\n",
      " 0.99971479 0.99949217 0.99973065 0.99966192 0.99972111 0.9997136\n",
      " 0.9997136  0.99974388 0.99972039 0.99971849 0.99973351 0.99971563\n",
      " 0.99972361 0.99974686 0.99974328]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 29 [0/88 (0%)]\tTrain Loss: 0.032906\n",
      "Train Epoch: 29 [4/88 (5%)]\tTrain Loss: 0.000634\n",
      "Train Epoch: 29 [8/88 (9%)]\tTrain Loss: 0.000136\n",
      "Train Epoch: 29 [12/88 (14%)]\tTrain Loss: 0.001448\n",
      "Train Epoch: 29 [16/88 (18%)]\tTrain Loss: 0.000517\n",
      "Train Epoch: 29 [20/88 (23%)]\tTrain Loss: 0.002727\n",
      "Train Epoch: 29 [24/88 (27%)]\tTrain Loss: 0.000499\n",
      "Train Epoch: 29 [28/88 (32%)]\tTrain Loss: 0.000479\n",
      "Train Epoch: 29 [32/88 (36%)]\tTrain Loss: 0.001301\n",
      "Train Epoch: 29 [36/88 (41%)]\tTrain Loss: 0.000302\n",
      "Train Epoch: 29 [40/88 (45%)]\tTrain Loss: 0.000384\n",
      "Train Epoch: 29 [44/88 (50%)]\tTrain Loss: 0.000706\n",
      "Train Epoch: 29 [48/88 (55%)]\tTrain Loss: 0.090630\n",
      "Train Epoch: 29 [52/88 (59%)]\tTrain Loss: 0.109580\n",
      "Train Epoch: 29 [56/88 (64%)]\tTrain Loss: 0.000949\n",
      "Train Epoch: 29 [60/88 (68%)]\tTrain Loss: 0.006722\n",
      "Train Epoch: 29 [64/88 (73%)]\tTrain Loss: 0.002108\n",
      "Train Epoch: 29 [68/88 (77%)]\tTrain Loss: 0.001000\n",
      "Train Epoch: 29 [72/88 (82%)]\tTrain Loss: 0.001819\n",
      "Train Epoch: 29 [76/88 (86%)]\tTrain Loss: 0.002033\n",
      "Train Epoch: 29 [80/88 (91%)]\tTrain Loss: 0.001592\n",
      "Train Epoch: 29 [84/88 (95%)]\tTrain Loss: 0.001051\n",
      "\n",
      "Train set: Average loss: 0.0081, Accuracy: 345/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00175927 0.00178906 0.00199667 0.00533082 0.00172111 0.02431455\n",
      " 0.00275303 0.00165974 0.00192764 0.00239518 0.00222425 0.00202159\n",
      " 0.00204304 0.0018178  0.00441921 0.00195037 0.00493573 0.00499791\n",
      " 0.00413675 0.00273235 0.00274433 0.00159622 0.02268488 0.00748608\n",
      " 0.00423758 0.00236691 0.00383197 0.00716895 0.00229145 0.00259176\n",
      " 0.00163913 0.00208986 0.99962723 0.99950862 0.99929249 0.99873513\n",
      " 0.99960667 0.9995628  0.99958986 0.99945766 0.99962628 0.9993912\n",
      " 0.99959964 0.99890578 0.99925941 0.9996475  0.99913424 0.99960321\n",
      " 0.99948043 0.99960464 0.9994815  0.99934751 0.99960035 0.99965477\n",
      " 0.9996444  0.99956697 0.9994061  0.99966633 0.99962533 0.99960226\n",
      " 0.99962211 0.99953282 0.999605   0.99961805 0.99956125 0.99957782\n",
      " 0.99957722 0.99964476 0.99955028 0.99968088 0.99959511 0.9993788\n",
      " 0.99963212 0.99962008 0.99962175 0.99962986 0.99962413 0.9996419\n",
      " 0.9994759  0.99962056 0.9994598  0.99957412 0.9996314  0.99961275\n",
      " 0.99956876 0.99915612 0.99963701 0.99948299 0.99956661 0.9995597\n",
      " 0.99960583 0.99965322 0.99961531 0.99960023 0.99961853 0.99952996\n",
      " 0.99961591 0.99963796 0.99962091]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 30 [0/88 (0%)]\tTrain Loss: 0.093479\n",
      "Train Epoch: 30 [4/88 (5%)]\tTrain Loss: 0.000348\n",
      "Train Epoch: 30 [8/88 (9%)]\tTrain Loss: 0.098060\n",
      "Train Epoch: 30 [12/88 (14%)]\tTrain Loss: 0.001639\n",
      "Train Epoch: 30 [16/88 (18%)]\tTrain Loss: 0.000560\n",
      "Train Epoch: 30 [20/88 (23%)]\tTrain Loss: 0.001051\n",
      "Train Epoch: 30 [24/88 (27%)]\tTrain Loss: 0.000473\n",
      "Train Epoch: 30 [28/88 (32%)]\tTrain Loss: 0.001404\n",
      "Train Epoch: 30 [32/88 (36%)]\tTrain Loss: 0.004879\n",
      "Train Epoch: 30 [36/88 (41%)]\tTrain Loss: 0.001095\n",
      "Train Epoch: 30 [40/88 (45%)]\tTrain Loss: 0.000644\n",
      "Train Epoch: 30 [44/88 (50%)]\tTrain Loss: 0.003915\n",
      "Train Epoch: 30 [48/88 (55%)]\tTrain Loss: 0.008364\n",
      "Train Epoch: 30 [52/88 (59%)]\tTrain Loss: 0.000354\n",
      "Train Epoch: 30 [56/88 (64%)]\tTrain Loss: 0.000091\n",
      "Train Epoch: 30 [60/88 (68%)]\tTrain Loss: 0.000822\n",
      "Train Epoch: 30 [64/88 (73%)]\tTrain Loss: 0.000523\n",
      "Train Epoch: 30 [68/88 (77%)]\tTrain Loss: 0.000961\n",
      "Train Epoch: 30 [72/88 (82%)]\tTrain Loss: 0.000352\n",
      "Train Epoch: 30 [76/88 (86%)]\tTrain Loss: 0.000594\n",
      "Train Epoch: 30 [80/88 (91%)]\tTrain Loss: 0.000753\n",
      "Train Epoch: 30 [84/88 (95%)]\tTrain Loss: 0.000081\n",
      "\n",
      "Train set: Average loss: 0.0050, Accuracy: 346/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [9.12146817e-04 9.36347060e-04 1.00097735e-03 2.23377789e-03\n",
      " 8.87068396e-04 1.41596599e-02 1.30068301e-03 9.64084989e-04\n",
      " 1.33200479e-03 1.22210523e-03 1.05224468e-03 1.05568231e-03\n",
      " 1.04831345e-03 9.92945977e-04 1.83585996e-03 1.00094860e-03\n",
      " 2.92850635e-03 2.58094561e-03 2.06564134e-03 1.12080213e-03\n",
      " 1.34056027e-03 8.24297138e-04 1.36689600e-02 3.40817124e-03\n",
      " 2.15325807e-03 1.32097770e-03 2.23982264e-03 3.73045634e-03\n",
      " 1.17072242e-03 1.29883224e-03 9.14749980e-04 1.21382426e-03\n",
      " 9.99760807e-01 9.99673247e-01 9.99441803e-01 9.98993099e-01\n",
      " 9.99733746e-01 9.99709904e-01 9.99739468e-01 9.99608576e-01\n",
      " 9.99745190e-01 9.99577105e-01 9.99733627e-01 9.99223351e-01\n",
      " 9.99495983e-01 9.99771178e-01 9.99357045e-01 9.99745786e-01\n",
      " 9.99633193e-01 9.99734223e-01 9.99598324e-01 9.99559462e-01\n",
      " 9.99729097e-01 9.99772131e-01 9.99745786e-01 9.99706089e-01\n",
      " 9.99590337e-01 9.99773204e-01 9.99756277e-01 9.99752104e-01\n",
      " 9.99748647e-01 9.99688029e-01 9.99749482e-01 9.99762714e-01\n",
      " 9.99695420e-01 9.99744356e-01 9.99712050e-01 9.99765456e-01\n",
      " 9.99707520e-01 9.99780118e-01 9.99732196e-01 9.99582589e-01\n",
      " 9.99767244e-01 9.99738514e-01 9.99738991e-01 9.99759376e-01\n",
      " 9.99762952e-01 9.99772608e-01 9.99631882e-01 9.99758303e-01\n",
      " 9.99653220e-01 9.99710739e-01 9.99764860e-01 9.99750793e-01\n",
      " 9.99724090e-01 9.99449313e-01 9.99764025e-01 9.99646902e-01\n",
      " 9.99720037e-01 9.99709308e-01 9.99747932e-01 9.99762237e-01\n",
      " 9.99747217e-01 9.99743521e-01 9.99745309e-01 9.99677420e-01\n",
      " 9.99758422e-01 9.99763072e-01 9.99755919e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "TP= 67 TN= 32 FN= 0 FP= 0\n",
      "TP+FP 67\n",
      "precision 1.0\n",
      "recall 1.0\n",
      "F1 1.0\n",
      "acc 1.0\n",
      "AUCp 1.0\n",
      "AUC 1.0\n",
      "Train Epoch: 31 [0/88 (0%)]\tTrain Loss: 0.091106\n",
      "Train Epoch: 31 [4/88 (5%)]\tTrain Loss: 0.000340\n",
      "Train Epoch: 31 [8/88 (9%)]\tTrain Loss: 0.000300\n",
      "Train Epoch: 31 [12/88 (14%)]\tTrain Loss: 0.005040\n",
      "Train Epoch: 31 [16/88 (18%)]\tTrain Loss: 0.000422\n",
      "Train Epoch: 31 [20/88 (23%)]\tTrain Loss: 0.000347\n",
      "Train Epoch: 31 [24/88 (27%)]\tTrain Loss: 0.051297\n",
      "Train Epoch: 31 [28/88 (32%)]\tTrain Loss: 0.049269\n",
      "Train Epoch: 31 [32/88 (36%)]\tTrain Loss: 0.050207\n",
      "Train Epoch: 31 [36/88 (41%)]\tTrain Loss: 0.000346\n",
      "Train Epoch: 31 [40/88 (45%)]\tTrain Loss: 0.000885\n",
      "Train Epoch: 31 [44/88 (50%)]\tTrain Loss: 0.000980\n",
      "Train Epoch: 31 [48/88 (55%)]\tTrain Loss: 0.000306\n",
      "Train Epoch: 31 [52/88 (59%)]\tTrain Loss: 0.000217\n",
      "Train Epoch: 31 [56/88 (64%)]\tTrain Loss: 0.000535\n",
      "Train Epoch: 31 [60/88 (68%)]\tTrain Loss: 0.000208\n",
      "Train Epoch: 31 [64/88 (73%)]\tTrain Loss: 0.000158\n",
      "Train Epoch: 31 [68/88 (77%)]\tTrain Loss: 0.010184\n",
      "Train Epoch: 31 [72/88 (82%)]\tTrain Loss: 0.000238\n",
      "Train Epoch: 31 [76/88 (86%)]\tTrain Loss: 0.002064\n",
      "Train Epoch: 31 [80/88 (91%)]\tTrain Loss: 0.000092\n",
      "Train Epoch: 31 [84/88 (95%)]\tTrain Loss: 0.000665\n",
      "\n",
      "Train set: Average loss: 0.0051, Accuracy: 345/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [8.07132048e-04 7.86808960e-04 8.79877014e-04 1.70827145e-03\n",
      " 7.55395158e-04 9.31764673e-03 1.06757530e-03 9.24011343e-04\n",
      " 1.95332291e-03 1.01117755e-03 8.92485084e-04 9.12477670e-04\n",
      " 9.67677624e-04 9.16560763e-04 1.45515846e-03 8.51211022e-04\n",
      " 2.00655544e-03 1.84200692e-03 1.59241888e-03 9.17112571e-04\n",
      " 1.05858815e-03 7.00628676e-04 7.82978721e-03 2.21829163e-03\n",
      " 1.78150844e-03 1.17808953e-03 1.64829521e-03 2.14252947e-03\n",
      " 9.34762124e-04 1.11254642e-03 8.62557557e-04 1.13062491e-03\n",
      " 9.99739826e-01 9.99651909e-01 9.99384761e-01 9.98804927e-01\n",
      " 9.99721825e-01 9.99687195e-01 9.99719799e-01 9.99578893e-01\n",
      " 9.99718845e-01 9.99536991e-01 9.99706566e-01 9.99151230e-01\n",
      " 9.99444664e-01 9.99750912e-01 9.99342859e-01 9.99720395e-01\n",
      " 9.99594867e-01 9.99708712e-01 9.99547422e-01 9.99539137e-01\n",
      " 9.99698639e-01 9.99752104e-01 9.99713957e-01 9.99681473e-01\n",
      " 9.99546707e-01 9.99751031e-01 9.99738038e-01 9.99725044e-01\n",
      " 9.99725163e-01 9.99657154e-01 9.99726832e-01 9.99740303e-01\n",
      " 9.99675989e-01 9.99710143e-01 9.99686718e-01 9.99743879e-01\n",
      " 9.99677300e-01 9.99764264e-01 9.99713719e-01 9.99561489e-01\n",
      " 9.99745309e-01 9.99717891e-01 9.99710500e-01 9.99738991e-01\n",
      " 9.99740183e-01 9.99753296e-01 9.99605954e-01 9.99733746e-01\n",
      " 9.99632239e-01 9.99680758e-01 9.99746144e-01 9.99732554e-01\n",
      " 9.99698520e-01 9.99383569e-01 9.99745786e-01 9.99617577e-01\n",
      " 9.99696255e-01 9.99686122e-01 9.99726593e-01 9.99744236e-01\n",
      " 9.99728978e-01 9.99724329e-01 9.99726474e-01 9.99652982e-01\n",
      " 9.99734700e-01 9.99745071e-01 9.99729216e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 32 [0/88 (0%)]\tTrain Loss: 0.000171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [4/88 (5%)]\tTrain Loss: 0.143494\n",
      "Train Epoch: 32 [8/88 (9%)]\tTrain Loss: 0.000242\n",
      "Train Epoch: 32 [12/88 (14%)]\tTrain Loss: 0.127080\n",
      "Train Epoch: 32 [16/88 (18%)]\tTrain Loss: 0.000290\n",
      "Train Epoch: 32 [20/88 (23%)]\tTrain Loss: 0.031707\n",
      "Train Epoch: 32 [24/88 (27%)]\tTrain Loss: 0.007832\n",
      "Train Epoch: 32 [28/88 (32%)]\tTrain Loss: 0.002441\n",
      "Train Epoch: 32 [32/88 (36%)]\tTrain Loss: 0.000579\n",
      "Train Epoch: 32 [36/88 (41%)]\tTrain Loss: 0.003906\n",
      "Train Epoch: 32 [40/88 (45%)]\tTrain Loss: 0.001155\n",
      "Train Epoch: 32 [44/88 (50%)]\tTrain Loss: 0.002598\n",
      "Train Epoch: 32 [48/88 (55%)]\tTrain Loss: 0.000519\n",
      "Train Epoch: 32 [52/88 (59%)]\tTrain Loss: 0.006959\n",
      "Train Epoch: 32 [56/88 (64%)]\tTrain Loss: 0.001319\n",
      "Train Epoch: 32 [60/88 (68%)]\tTrain Loss: 0.107475\n",
      "Train Epoch: 32 [64/88 (73%)]\tTrain Loss: 0.007142\n",
      "Train Epoch: 32 [68/88 (77%)]\tTrain Loss: 0.010221\n",
      "Train Epoch: 32 [72/88 (82%)]\tTrain Loss: 0.011269\n",
      "Train Epoch: 32 [76/88 (86%)]\tTrain Loss: 0.000479\n",
      "Train Epoch: 32 [80/88 (91%)]\tTrain Loss: 0.000361\n",
      "Train Epoch: 32 [84/88 (95%)]\tTrain Loss: 0.000757\n",
      "\n",
      "Train set: Average loss: 0.0122, Accuracy: 344/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [8.31054873e-04 9.30679555e-04 9.35306540e-04 1.82539376e-03\n",
      " 8.07943812e-04 1.05737401e-02 1.33620854e-03 8.41401925e-04\n",
      " 1.08655996e-03 1.14000461e-03 9.69119312e-04 9.43874649e-04\n",
      " 9.07329668e-04 8.50163749e-04 1.57318846e-03 9.54865362e-04\n",
      " 2.91506224e-03 2.56873411e-03 1.72574248e-03 1.09223172e-03\n",
      " 1.24552113e-03 7.41729513e-04 1.18419947e-02 3.36706056e-03\n",
      " 1.74693239e-03 1.24364276e-03 2.48193066e-03 3.64692393e-03\n",
      " 1.13876176e-03 1.15195371e-03 8.03290168e-04 9.41045699e-04\n",
      " 9.99615431e-01 9.99410152e-01 9.98689950e-01 9.95101154e-01\n",
      " 9.99637127e-01 9.99560773e-01 9.99600589e-01 9.99222875e-01\n",
      " 9.99608338e-01 9.99284923e-01 9.99589264e-01 9.97604430e-01\n",
      " 9.98932540e-01 9.99624610e-01 9.98402417e-01 9.99622464e-01\n",
      " 9.99387145e-01 9.99620795e-01 9.99300361e-01 9.99137878e-01\n",
      " 9.99636769e-01 9.99616027e-01 9.99623299e-01 9.99527812e-01\n",
      " 9.99238491e-01 9.99643207e-01 9.99631047e-01 9.99548972e-01\n",
      " 9.99640465e-01 9.99534726e-01 9.99593914e-01 9.99619365e-01\n",
      " 9.99509215e-01 9.99567449e-01 9.99581754e-01 9.99643683e-01\n",
      " 9.99599755e-01 9.99672651e-01 9.99619722e-01 9.99264538e-01\n",
      " 9.99650598e-01 9.99621153e-01 9.99630690e-01 9.99638557e-01\n",
      " 9.99598205e-01 9.99592245e-01 9.99381661e-01 9.99599755e-01\n",
      " 9.99470651e-01 9.99538660e-01 9.99631047e-01 9.99604523e-01\n",
      " 9.99585927e-01 9.98652399e-01 9.99632716e-01 9.99392271e-01\n",
      " 9.99548972e-01 9.99578178e-01 9.99573410e-01 9.99644876e-01\n",
      " 9.99601781e-01 9.99614477e-01 9.99630928e-01 9.99538422e-01\n",
      " 9.99588072e-01 9.99655128e-01 9.99597013e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 33 [0/88 (0%)]\tTrain Loss: 0.002036\n",
      "Train Epoch: 33 [4/88 (5%)]\tTrain Loss: 0.004006\n",
      "Train Epoch: 33 [8/88 (9%)]\tTrain Loss: 0.000207\n",
      "Train Epoch: 33 [12/88 (14%)]\tTrain Loss: 0.003400\n",
      "Train Epoch: 33 [16/88 (18%)]\tTrain Loss: 0.001542\n",
      "Train Epoch: 33 [20/88 (23%)]\tTrain Loss: 0.000448\n",
      "Train Epoch: 33 [24/88 (27%)]\tTrain Loss: 0.000505\n",
      "Train Epoch: 33 [28/88 (32%)]\tTrain Loss: 0.005790\n",
      "Train Epoch: 33 [32/88 (36%)]\tTrain Loss: 0.000329\n",
      "Train Epoch: 33 [36/88 (41%)]\tTrain Loss: 0.000353\n",
      "Train Epoch: 33 [40/88 (45%)]\tTrain Loss: 0.000542\n",
      "Train Epoch: 33 [44/88 (50%)]\tTrain Loss: 0.000222\n",
      "Train Epoch: 33 [48/88 (55%)]\tTrain Loss: 0.000261\n",
      "Train Epoch: 33 [52/88 (59%)]\tTrain Loss: 0.002127\n",
      "Train Epoch: 33 [56/88 (64%)]\tTrain Loss: 0.000186\n",
      "Train Epoch: 33 [60/88 (68%)]\tTrain Loss: 0.000948\n",
      "Train Epoch: 33 [64/88 (73%)]\tTrain Loss: 0.000399\n",
      "Train Epoch: 33 [68/88 (77%)]\tTrain Loss: 0.000624\n",
      "Train Epoch: 33 [72/88 (82%)]\tTrain Loss: 0.002994\n",
      "Train Epoch: 33 [76/88 (86%)]\tTrain Loss: 0.000489\n",
      "Train Epoch: 33 [80/88 (91%)]\tTrain Loss: 0.000391\n",
      "Train Epoch: 33 [84/88 (95%)]\tTrain Loss: 0.000717\n",
      "\n",
      "Train set: Average loss: 0.0056, Accuracy: 347/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [0.00131791 0.00149002 0.00146323 0.00540582 0.00126224 0.13417299\n",
      " 0.0030129  0.00132039 0.00259677 0.00217943 0.00156595 0.00167202\n",
      " 0.0013778  0.00125917 0.00426298 0.00169088 0.01081222 0.00851179\n",
      " 0.00658745 0.0017513  0.00261413 0.00106176 0.13492343 0.01352535\n",
      " 0.00557987 0.002508   0.00900489 0.02193168 0.00209258 0.00217791\n",
      " 0.00107527 0.00146968 0.99978501 0.99978977 0.99967647 0.99944347\n",
      " 0.99982566 0.99979371 0.99980956 0.99974579 0.99980026 0.99974579\n",
      " 0.99980038 0.99959034 0.99970466 0.99979979 0.99964142 0.99981076\n",
      " 0.99976677 0.99981576 0.99973828 0.99974054 0.99979299 0.99978787\n",
      " 0.99980336 0.99978763 0.99973208 0.99980015 0.99979275 0.99973828\n",
      " 0.99981517 0.99979573 0.99977142 0.99977583 0.99978715 0.99974471\n",
      " 0.99979371 0.99979669 0.99979204 0.99982423 0.99980825 0.9997198\n",
      " 0.99980694 0.99979967 0.99979216 0.99979883 0.9997744  0.99977332\n",
      " 0.99976009 0.99977666 0.99977607 0.99978822 0.99979681 0.99981171\n",
      " 0.99980956 0.99967551 0.99979728 0.99977201 0.99979633 0.99979407\n",
      " 0.99974865 0.99980932 0.99978966 0.99980468 0.99980539 0.99979144\n",
      " 0.99976331 0.99980766 0.99979657]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 34 [0/88 (0%)]\tTrain Loss: 0.001240\n",
      "Train Epoch: 34 [4/88 (5%)]\tTrain Loss: 0.000328\n",
      "Train Epoch: 34 [8/88 (9%)]\tTrain Loss: 0.000435\n",
      "Train Epoch: 34 [12/88 (14%)]\tTrain Loss: 0.011197\n",
      "Train Epoch: 34 [16/88 (18%)]\tTrain Loss: 0.000117\n",
      "Train Epoch: 34 [20/88 (23%)]\tTrain Loss: 0.000776\n",
      "Train Epoch: 34 [24/88 (27%)]\tTrain Loss: 0.000218\n",
      "Train Epoch: 34 [28/88 (32%)]\tTrain Loss: 0.000219\n",
      "Train Epoch: 34 [32/88 (36%)]\tTrain Loss: 0.000215\n",
      "Train Epoch: 34 [36/88 (41%)]\tTrain Loss: 0.000378\n",
      "Train Epoch: 34 [40/88 (45%)]\tTrain Loss: 0.000692\n",
      "Train Epoch: 34 [44/88 (50%)]\tTrain Loss: 0.021353\n",
      "Train Epoch: 34 [48/88 (55%)]\tTrain Loss: 0.000444\n",
      "Train Epoch: 34 [52/88 (59%)]\tTrain Loss: 0.000154\n",
      "Train Epoch: 34 [56/88 (64%)]\tTrain Loss: 0.000211\n",
      "Train Epoch: 34 [60/88 (68%)]\tTrain Loss: 0.002848\n",
      "Train Epoch: 34 [64/88 (73%)]\tTrain Loss: 0.000121\n",
      "Train Epoch: 34 [68/88 (77%)]\tTrain Loss: 0.000148\n",
      "Train Epoch: 34 [72/88 (82%)]\tTrain Loss: 0.000663\n",
      "Train Epoch: 34 [76/88 (86%)]\tTrain Loss: 0.001760\n",
      "Train Epoch: 34 [80/88 (91%)]\tTrain Loss: 0.060547\n",
      "Train Epoch: 34 [84/88 (95%)]\tTrain Loss: 0.048627\n",
      "\n",
      "Train set: Average loss: 0.0055, Accuracy: 346/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [1.09121238e-03 1.31587463e-03 1.23080378e-03 2.75200000e-03\n",
      " 9.98165109e-04 3.68117355e-02 2.45004380e-03 1.10128068e-03\n",
      " 1.35383604e-03 1.71623949e-03 1.21418503e-03 1.24339643e-03\n",
      " 1.15154439e-03 1.12015649e-03 1.89461396e-03 1.43075665e-03\n",
      " 6.82072435e-03 5.93891228e-03 3.24740796e-03 1.43286958e-03\n",
      " 1.71877397e-03 1.08542363e-03 3.97626199e-02 9.23829619e-03\n",
      " 2.76943739e-03 1.80946244e-03 6.29691919e-03 1.37817357e-02\n",
      " 1.95079984e-03 1.58403791e-03 1.10673893e-03 1.66570977e-03\n",
      " 9.99438226e-01 9.99159336e-01 9.97435629e-01 9.92143512e-01\n",
      " 9.99394536e-01 9.99335706e-01 9.99391317e-01 9.98795390e-01\n",
      " 9.99339640e-01 9.98889863e-01 9.99237418e-01 9.96425450e-01\n",
      " 9.98583794e-01 9.99531388e-01 9.97512460e-01 9.99388099e-01\n",
      " 9.98877823e-01 9.99309778e-01 9.98797297e-01 9.98866558e-01\n",
      " 9.99434173e-01 9.99445021e-01 9.99340475e-01 9.99274075e-01\n",
      " 9.98579144e-01 9.99461591e-01 9.99486923e-01 9.99425173e-01\n",
      " 9.99304295e-01 9.99346077e-01 9.99351203e-01 9.99514937e-01\n",
      " 9.99124467e-01 9.99464333e-01 9.99393702e-01 9.99520302e-01\n",
      " 9.99328732e-01 9.99477327e-01 9.99476016e-01 9.99017239e-01\n",
      " 9.99524355e-01 9.99362051e-01 9.99303102e-01 9.99481380e-01\n",
      " 9.99465764e-01 9.99483705e-01 9.98860240e-01 9.99412060e-01\n",
      " 9.99191463e-01 9.99241590e-01 9.99463379e-01 9.99433100e-01\n",
      " 9.99399185e-01 9.98261034e-01 9.99387622e-01 9.99136865e-01\n",
      " 9.99354541e-01 9.99290228e-01 9.99482095e-01 9.99459684e-01\n",
      " 9.99399066e-01 9.99399424e-01 9.99424934e-01 9.99340951e-01\n",
      " 9.99473274e-01 9.99502182e-01 9.99410391e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 35 [0/88 (0%)]\tTrain Loss: 0.000567\n",
      "Train Epoch: 35 [4/88 (5%)]\tTrain Loss: 0.001447\n",
      "Train Epoch: 35 [8/88 (9%)]\tTrain Loss: 0.000303\n",
      "Train Epoch: 35 [12/88 (14%)]\tTrain Loss: 0.003723\n",
      "Train Epoch: 35 [16/88 (18%)]\tTrain Loss: 0.000463\n",
      "Train Epoch: 35 [20/88 (23%)]\tTrain Loss: 0.000720\n",
      "Train Epoch: 35 [24/88 (27%)]\tTrain Loss: 0.003889\n",
      "Train Epoch: 35 [28/88 (32%)]\tTrain Loss: 0.001166\n",
      "Train Epoch: 35 [32/88 (36%)]\tTrain Loss: 0.000708\n",
      "Train Epoch: 35 [36/88 (41%)]\tTrain Loss: 0.000921\n",
      "Train Epoch: 35 [40/88 (45%)]\tTrain Loss: 0.001950\n",
      "Train Epoch: 35 [44/88 (50%)]\tTrain Loss: 0.000965\n",
      "Train Epoch: 35 [48/88 (55%)]\tTrain Loss: 0.028318\n",
      "Train Epoch: 35 [52/88 (59%)]\tTrain Loss: 0.045211\n",
      "Train Epoch: 35 [56/88 (64%)]\tTrain Loss: 0.000283\n",
      "Train Epoch: 35 [60/88 (68%)]\tTrain Loss: 0.000162\n",
      "Train Epoch: 35 [64/88 (73%)]\tTrain Loss: 0.000090\n",
      "Train Epoch: 35 [68/88 (77%)]\tTrain Loss: 0.000557\n",
      "Train Epoch: 35 [72/88 (82%)]\tTrain Loss: 0.000199\n",
      "Train Epoch: 35 [76/88 (86%)]\tTrain Loss: 0.000155\n",
      "Train Epoch: 35 [80/88 (91%)]\tTrain Loss: 0.000346\n",
      "Train Epoch: 35 [84/88 (95%)]\tTrain Loss: 0.000373\n",
      "\n",
      "Train set: Average loss: 0.0061, Accuracy: 345/349 (1%)\n",
      "\n",
      "batch_index:\t 0\n",
      "batch_index:\t 1\n",
      "batch_index:\t 2\n",
      "batch_index:\t 3\n",
      "batch_index:\t 4\n",
      "batch_index:\t 5\n",
      "batch_index:\t 6\n",
      "batch_index:\t 7\n",
      "batch_index:\t 8\n",
      "batch_index:\t 9\n",
      "batch_index:\t 10\n",
      "batch_index:\t 11\n",
      "batch_index:\t 12\n",
      "batch_index:\t 13\n",
      "batch_index:\t 14\n",
      "batch_index:\t 15\n",
      "batch_index:\t 16\n",
      "batch_index:\t 17\n",
      "batch_index:\t 18\n",
      "batch_index:\t 19\n",
      "batch_index:\t 20\n",
      "batch_index:\t 21\n",
      "batch_index:\t 22\n",
      "batch_index:\t 23\n",
      "batch_index:\t 24\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "score [8.43063171e-04 8.77991726e-04 9.50211543e-04 2.32106564e-03\n",
      " 8.67209688e-04 1.30813317e-02 1.33615138e-03 8.52236117e-04\n",
      " 1.03144068e-03 1.27543975e-03 1.04157208e-03 1.07529073e-03\n",
      " 9.44342348e-04 8.74521036e-04 2.09057890e-03 1.06146012e-03\n",
      " 2.92727980e-03 3.18517257e-03 2.40202458e-03 1.21120492e-03\n",
      " 1.28529442e-03 8.30634846e-04 1.42047675e-02 3.58906342e-03\n",
      " 2.14548176e-03 1.31593726e-03 2.38852482e-03 3.47998622e-03\n",
      " 1.20689196e-03 1.40730478e-03 7.86276301e-04 9.67661443e-04\n",
      " 9.99798000e-01 9.99722302e-01 9.99507785e-01 9.98929560e-01\n",
      " 9.99788582e-01 9.99749362e-01 9.99790609e-01 9.99709070e-01\n",
      " 9.99801099e-01 9.99642253e-01 9.99796569e-01 9.99244928e-01\n",
      " 9.99534011e-01 9.99812782e-01 9.99438941e-01 9.99798000e-01\n",
      " 9.99712169e-01 9.99789178e-01 9.99690652e-01 9.99605596e-01\n",
      " 9.99785364e-01 9.99820888e-01 9.99802053e-01 9.99792397e-01\n",
      " 9.99673605e-01 9.99820530e-01 9.99796093e-01 9.99803960e-01\n",
      " 9.99803603e-01 9.99747932e-01 9.99804676e-01 9.99801219e-01\n",
      " 9.99763191e-01 9.99783099e-01 9.99767125e-01 9.99807894e-01\n",
      " 9.99783337e-01 9.99833226e-01 9.99781668e-01 9.99624729e-01\n",
      " 9.99810040e-01 9.99797285e-01 9.99805272e-01 9.99803245e-01\n",
      " 9.99800384e-01 9.99812186e-01 9.99698043e-01 9.99807298e-01\n",
      " 9.99706209e-01 9.99780476e-01 9.99807417e-01 9.99794304e-01\n",
      " 9.99781191e-01 9.99532819e-01 9.99806583e-01 9.99720871e-01\n",
      " 9.99775112e-01 9.99775112e-01 9.99786794e-01 9.99807656e-01\n",
      " 9.99793112e-01 9.99809086e-01 9.99807060e-01 9.99741733e-01\n",
      " 9.99801219e-01 9.99809086e-01 9.99804795e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Train Epoch: 36 [0/88 (0%)]\tTrain Loss: 0.000056\n",
      "Train Epoch: 36 [4/88 (5%)]\tTrain Loss: 0.000479\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# train\n",
    "bs = batchsize\n",
    "votenum = 10\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "r_list = []\n",
    "p_list = []\n",
    "acc_list = []\n",
    "AUC_list = []\n",
    "# TP = 0\n",
    "# TN = 0\n",
    "# FN = 0\n",
    "# FP = 0\n",
    "vote_pred = np.zeros(valset.__len__())\n",
    "vote_score = np.zeros(valset.__len__())\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)\n",
    "lr = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr) \n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
    "best_acc = 0\n",
    "scheduler = StepLR(optimizer, step_size=1)\n",
    "\n",
    "total_epoch = 50\n",
    "for epoch in range(1, total_epoch+1):\n",
    "    train(optimizer, epoch)\n",
    "    \n",
    "    targetlist, scorelist, predlist = val(epoch)\n",
    "    print('target',targetlist)\n",
    "    print('score',scorelist)\n",
    "    print('predict',predlist)\n",
    "    vote_pred = vote_pred + predlist \n",
    "    vote_score = vote_score + scorelist \n",
    "\n",
    "    if epoch % votenum == 0:\n",
    "        \n",
    "        # major vote\n",
    "        vote_pred[vote_pred <= (votenum/2)] = 0\n",
    "        vote_pred[vote_pred > (votenum/2)] = 1\n",
    "        vote_score = vote_score/votenum\n",
    "\n",
    "        print('vote_pred', vote_pred)\n",
    "        print('targetlist', targetlist)\n",
    "        TP = ((vote_pred == 1) & (targetlist == 1)).sum()\n",
    "        TN = ((vote_pred == 0) & (targetlist == 0)).sum()\n",
    "        FN = ((vote_pred == 0) & (targetlist == 1)).sum()\n",
    "        FP = ((vote_pred == 1) & (targetlist == 0)).sum()\n",
    "\n",
    "\n",
    "        print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n",
    "        print('TP+FP',TP+FP)\n",
    "        p = TP / (TP + FP)\n",
    "        print('precision',p)\n",
    "        p = TP / (TP + FP)\n",
    "        r = TP / (TP + FN)\n",
    "        print('recall',r)\n",
    "        F1 = 2 * r * p / (r + p)\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "        print('F1',F1)\n",
    "        print('acc',acc)\n",
    "        AUC = roc_auc_score(targetlist, vote_score)\n",
    "        print('AUCp', roc_auc_score(targetlist, vote_pred))\n",
    "        print('AUC', AUC)\n",
    "\n",
    "        \n",
    "        \n",
    "#         if epoch == total_epoch:\n",
    "        if best_acc < acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"/data/cv_final/CT-Predict/3D-Seg/result/{}_{}_{}_{}.pt\".format(modelname,alpha_name,epoch, datetime.datetime.now()))  \n",
    "\n",
    "            vote_pred = np.zeros(valset.__len__())\n",
    "            vote_score = np.zeros(valset.__len__())\n",
    "            print('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n",
    "    average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
    "            epoch, r, p, F1, acc, AUC))\n",
    "\n",
    "            f = open('/data/cv_final/CT-Predict/3D-Seg/result/{}_{}_{}_{}.txt'.format(modelname,alpha_name, epoch,lr), 'a+')\n",
    "            f.write('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n",
    "    average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
    "            epoch, r, p, F1, acc, AUC))\n",
    "            f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test process is defined here \n",
    "\n",
    "def test(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    \n",
    "    \n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    # Don't update model\n",
    "    with torch.no_grad():\n",
    "        tpr_list = []\n",
    "        fpr_list = []\n",
    "        \n",
    "        predlist=[]\n",
    "        scorelist=[]\n",
    "        targetlist=[]\n",
    "        # Predict\n",
    "        for batch_index, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criteria(output, target.long())\n",
    "            score = F.softmax(output, dim=1)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "#             print('target',target.long()[:, 2].view_as(pred))\n",
    "            correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "            \n",
    "            targetcpu=target.long().cpu().numpy()\n",
    "            predlist=np.append(predlist, pred.cpu().numpy())\n",
    "            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])\n",
    "            targetlist=np.append(targetlist,targetcpu)\n",
    "    \n",
    "    # 记录一个epoch的loss, acc\n",
    "    his['test_loss'].append(test_loss.data.cpu().numpy()/len(test_loader.dataset))\n",
    "    his['test_acc'].append(correct/len(test_loader.dataset))\n",
    "\n",
    "    return targetlist, scorelist, predlist\n",
    "    \n",
    "    # Write to tensorboard\n",
    "#     writer.add_scalar('Test Accuracy', 100.0 * correct / len(test_loader.dataset), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "import datetime\n",
    "bs = 10\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "epoch = 1\n",
    "r_list = []\n",
    "p_list = []\n",
    "acc_list = []\n",
    "AUC_list = []\n",
    "# TP = 0\n",
    "# TN = 0\n",
    "# FN = 0\n",
    "# FP = 0\n",
    "vote_pred = np.zeros(testset.__len__())\n",
    "vote_score = np.zeros(testset.__len__())\n",
    "\n",
    "\n",
    "targetlist, scorelist, predlist = test(epoch)\n",
    "print('target',targetlist)\n",
    "print('score',scorelist)\n",
    "print('predict',predlist)\n",
    "vote_pred = vote_pred + predlist \n",
    "vote_score = vote_score + scorelist \n",
    "\n",
    "TP = ((predlist == 1) & (targetlist == 1)).sum()\n",
    "\n",
    "TN = ((predlist == 0) & (targetlist == 0)).sum()\n",
    "FN = ((predlist == 0) & (targetlist == 1)).sum()\n",
    "FP = ((predlist == 1) & (targetlist == 0)).sum()\n",
    "\n",
    "print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n",
    "print('TP+FP',TP+FP)\n",
    "p = TP / (TP + FP)\n",
    "print('precision',p)\n",
    "p = TP / (TP + FP)\n",
    "r = TP / (TP + FN)\n",
    "print('recall',r)\n",
    "F1 = 2 * r * p / (r + p)\n",
    "acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "print('F1',F1)\n",
    "print('acc',acc)\n",
    "AUC = roc_auc_score(targetlist, vote_score)\n",
    "print('AUC', AUC)\n",
    "\n",
    "# 记录多个epoch的平均值\n",
    "# writer.add_scalar('F1/test', F1, epoch)\n",
    "# writer.add_scalar('acc/test', acc, epoch)\n",
    "# writer.add_scalar('racall/test', r, epoch)\n",
    "# writer.add_scalar('precision/test', p, epoch)\n",
    "\n",
    "save_p = os.path.join(PATH_to_log_dir, f'test_{modelname}_{alpha}_{epoch}_{datetime.datetime.now()}.txt')\n",
    "f = open(save_p, 'a+')\n",
    "f.write('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n",
    "average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
    "epoch, r, p, F1, acc, AUC))\n",
    "f.close()\n",
    "torch.save(model.state_dict(), \"/data/cv_final/CT-Predict/3D-Seg/result/transfer_{}_{}_wuhan.pt\".format(modelname,alpha_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0851469285850197, 0.07077601304368508, 0.06386198901856868, 0.043822307641321744, 0.034708214352670574, 0.01999401158111485, 0.02613348264065718, 0.013969694645835199, 0.00983561075860928, 0.016297869149456735, 0.014113978191911321, 0.011058806689899083, 0.014750360417844232, 0.014795196773671148, 0.009764641267180784, 0.006449441855138216, 0.0073987228480997605, 0.010555313104886381, 0.007517648631317226, 0.0054459892917840735, 0.017965370057988646, 0.01027121721502703, 0.007401881040338117, 0.009943014571181682, 0.0070128434025455684, 0.010293184515398348, 0.007416893213048022, 0.011164747883050694, 0.006496638486583458, 0.007215221836778701]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#这里导入你自己的数据\n",
    "#......\n",
    "#......\n",
    "#x_axix，train_pn_dis这些都是长度相同的list()\n",
    "print(his['train_loss'])\n",
    "x_axix = range(len(his['train_loss']))\n",
    "#开始画图\n",
    "sub_axix = filter(lambda x:x%200 == 0, x_axix)\n",
    "plt.title('Result Analysis')\n",
    "plt.plot(x_axix, his['train_acc'],  label='train accuracy')\n",
    "plt.plot(x_axix, his['val_acc'], label='val accuracy')\n",
    "# plt.plot(x_axix, his['train_loss'],  color='skyblue', label='train loss')\n",
    "# plt.plot(x_axix, his['val_loss'], color='blue', label='val loss')\n",
    "plt.legend() # 显示图例\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Rate')\n",
    "plt.show()\n",
    "#python 一个折线图绘制多个曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(his)\n",
    "df.to_csv('/data/cv_final/CT-Predict/3D-Seg/result/his.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}