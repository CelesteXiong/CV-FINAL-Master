{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "PATH_to_log_dir = '/data/cv_final/CT-Predict/2D-Pretrain/result' # 如果输出路径不存在会被自动创建\n",
    "# writer = SummaryWriter(PATH_to_log_dir)\n",
    "# writer = SummaryWriter()\n",
    "\n",
    "\n",
    "# for n_iter in range(100):\n",
    "#     writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "#     writer.add_scalar('Accuracy/train', np.random.random(), n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import random \n",
    "from torchvision.datasets import ImageFolder\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from skimage.io import imread, imsave\n",
    "import skimage\n",
    "from PIL import ImageFile\n",
    "from PIL import Image\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "########## Mean and std are calculated from the train dataset\n",
    "normalize = transforms.Normalize(mean=[0.45271412, 0.45271412, 0.45271412],\n",
    "                                     std=[0.33165374, 0.33165374, 0.33165374])\n",
    "train_transformer = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop((224),scale=(0.5,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(90),\n",
    "    # random brightness and random contrast\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "val_transformer = transforms.Compose([\n",
    "#     transforms.Resize(224),\n",
    "#     transforms.CenterCrop(224),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425\n",
      "118\n",
      "203\n"
     ]
    }
   ],
   "source": [
    "batchsize=8\n",
    "def read_txt(txt_path):\n",
    "    with open(txt_path) as f:\n",
    "        lines = f.readlines()\n",
    "    txt_data = [line.strip() for line in lines]\n",
    "    return txt_data\n",
    "\n",
    "class CovidCTDataset(Dataset):\n",
    "    def __init__(self, root_dir, txt_COVID, txt_NonCOVID, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            txt_path (string): Path to the txt file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        File structure:\n",
    "        - root_dir\n",
    "            - CT_COVID\n",
    "                - img1.png\n",
    "                - img2.png\n",
    "                - ......\n",
    "            - CT_NonCOVID\n",
    "                - img1.png\n",
    "                - img2.png\n",
    "                - ......\n",
    "        - root_dir /data/Data/\n",
    "            - COVID19\n",
    "                - img1.png\n",
    "                - img2.png\n",
    "                - ......\n",
    "            - Normal\n",
    "                - img1.png\n",
    "                - img2.png\n",
    "                - ......\n",
    "        txt_path:\n",
    "        - COVID19\n",
    "            - test_COVID.txt\n",
    "            - train_COVID.txt\n",
    "            - val_COVID.txt\n",
    "        - Normal\n",
    "            - ...\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.txt_path = [txt_COVID,txt_NonCOVID]\n",
    "        # 2019\n",
    "#         self.classes = ['COVID19', 'Normal']\n",
    "        # UCSD\n",
    "        self.classes = ['CT_COVID', 'CT_NonCOVID']\n",
    "        self.num_cls = len(self.classes)\n",
    "        self.img_list = []\n",
    "        for c in range(self.num_cls):\n",
    "            cls_list = [[os.path.join(self.root_dir,self.classes[c],item), c] for item in read_txt(self.txt_path[c])]\n",
    "#             print(cls_list)\n",
    "            self.img_list += cls_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.img_list[idx][0]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        sample = {'img': image,\n",
    "                  'label': int(self.img_list[idx][1])}\n",
    "        return sample\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainset = CovidCTDataset(root_dir='/data/COVID-CT/Data',\n",
    "                              txt_COVID='/data/COVID-CT/Data-split/COVID/trainCT_COVID.txt',\n",
    "                              txt_NonCOVID='/data/COVID-CT/Data-split/NonCOVID/trainCT_NonCOVID.txt',\n",
    "                              transform= train_transformer)\n",
    "    valset = CovidCTDataset(root_dir='/data/COVID-CT/Data',\n",
    "                              txt_COVID='/data/COVID-CT/Data-split/COVID/valCT_COVID.txt',\n",
    "                              txt_NonCOVID='/data/COVID-CT/Data-split/NonCOVID/valCT_NonCOVID.txt',\n",
    "                              transform= val_transformer)\n",
    "    testset = CovidCTDataset(root_dir='/data/COVID-CT/Data',\n",
    "                              txt_COVID='/data/COVID-CT/Data-split/COVID/testCT_COVID.txt',\n",
    "                              txt_NonCOVID='/data/COVID-CT/Data-split/NonCOVID/testCT_NonCOVID.txt',\n",
    "                              transform= val_transformer)\n",
    "    print(trainset.__len__())\n",
    "    print(valset.__len__())\n",
    "    print(testset.__len__())\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True)\n",
    "    val_loader = DataLoader(valset, batch_size=batchsize, drop_last=False, shuffle=False)\n",
    "    test_loader = DataLoader(testset, batch_size=batchsize, drop_last=False, shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch_index, batch_samples in enumerate(train_loader):      \n",
    "        data, target = batch_samples['img'], batch_samples['label']\n",
    "        break\n",
    "# io.imread 读出图片格式是 uint8(unsigned int) ；value是 numpy array ；\n",
    "# 图像数据是以 RGB 的格式进行存储的，通道值默认范围0-255\n",
    "skimage.io.imshow(data[0,1,:,:].numpy())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixup\n",
    "'''Use mixup to do data augmentation'''\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "#         print('lam',lam)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "#     print(pred)\n",
    "#     print(y_a)\n",
    "#     print('criterion',criterion(pred, y_a))\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training process is defined here \n",
    "\n",
    "alpha = None\n",
    "## alpha is None if mixup is not used\n",
    "alpha_name = f'{alpha}'\n",
    "device = 'cuda'\n",
    "\n",
    "def train(optimizer, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    for batch_index, batch_samples in enumerate(train_loader):\n",
    "        \n",
    "        # move data to device\n",
    "        data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
    "        \n",
    "        ## adjust data to meet the input dimension of model\n",
    "#         data = data[:, 0, :, :]\n",
    "#         data = data[:, None, :, :]    \n",
    "        \n",
    "        #mixup\n",
    "#         data, targets_a, targets_b, lam = mixup_data(data, target, alpha, use_cuda=True)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        criteria = nn.CrossEntropyLoss()\n",
    "        loss = criteria(output, target.long())\n",
    "        \n",
    "        #mixup loss\n",
    "#         loss = mixup_criterion(criteria, output, targets_a, targets_b, lam)\n",
    "\n",
    "        train_loss += criteria(output, target.long())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        train_correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "    \n",
    "        # Display progress and write to tensorboard\n",
    "        if batch_index % bs == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}'.format(\n",
    "                epoch, batch_index, len(train_loader),\n",
    "                100.0 * batch_index / len(train_loader), loss.item()/ bs))\n",
    "    his['train_loss'].append(train_loss.data.cpu().numpy()/len(train_loader.dataset))\n",
    "    his['train_acc'].append(train_correct / len(train_loader.dataset))\n",
    "    \n",
    "#     print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         train_loss/len(train_loader.dataset), train_correct, len(train_loader.dataset),\n",
    "#         100.0 * train_correct / len(train_loader.dataset)))\n",
    "    p = os.path.join(PATH_to_log_dir,'/baseline_{}.txt'.format(modelname))\n",
    "    f = open(p, 'a+')\n",
    "    f.write('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        train_loss/len(train_loader.dataset), train_correct, len(train_loader.dataset),\n",
    "        100.0 * train_correct / len(train_loader.dataset)))\n",
    "    f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #val process is defined here\n",
    "\n",
    "def val(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    \n",
    "    \n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    # Don't update model\n",
    "    with torch.no_grad():\n",
    "        tpr_list = []\n",
    "        fpr_list = []\n",
    "        \n",
    "        predlist=[]\n",
    "        scorelist=[]\n",
    "        targetlist=[]\n",
    "        # Predict\n",
    "        for batch_index, batch_samples in enumerate(val_loader):\n",
    "            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
    "            \n",
    "#             data = data[:, 0, :, :]\n",
    "#             data = data[:, None, :, :]\n",
    "            output = model(data)\n",
    "            \n",
    "            val_loss += criteria(output, target.long())\n",
    "            score = F.softmax(output, dim=1)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "#             print('target',target.long()[:, 2].view_as(pred))\n",
    "            correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "            \n",
    "#             print(output[:,1].cpu().numpy())\n",
    "#             print((output[:,1]+output[:,0]).cpu().numpy())\n",
    "#             predcpu=(output[:,1].cpu().numpy())/((output[:,1]+output[:,0]).cpu().numpy())\n",
    "            targetcpu=target.long().cpu().numpy()\n",
    "            predlist=np.append(predlist, pred.cpu().numpy())\n",
    "            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])\n",
    "            targetlist=np.append(targetlist,targetcpu)\n",
    "        his['val_loss'].append(val_loss.data.cpu().numpy()/len(val_loader.dataset))\n",
    "        his['val_acc'].append(correct/len(val_loader.dataset))       \n",
    "    return targetlist, scorelist, predlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test process is defined here \n",
    "\n",
    "def test(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    \n",
    "    \n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    # Don't update model\n",
    "    with torch.no_grad():\n",
    "        tpr_list = []\n",
    "        fpr_list = []\n",
    "        \n",
    "        predlist=[]\n",
    "        scorelist=[]\n",
    "        targetlist=[]\n",
    "        # Predict\n",
    "        for batch_index, batch_samples in enumerate(test_loader):\n",
    "            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
    "#             data = data[:, 0, :, :]\n",
    "#             data = data[:, None, :, :]\n",
    "#             print(target)\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criteria(output, target.long())\n",
    "            score = F.softmax(output, dim=1)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "#             print('target',target.long()[:, 2].view_as(pred))\n",
    "            correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "#             TP += ((pred == 1) & (target.long()[:, 2].view_as(pred).data == 1)).cpu().sum()\n",
    "#             TN += ((pred == 0) & (target.long()[:, 2].view_as(pred) == 0)).cpu().sum()\n",
    "# #             # FN    predict 0 label 1\n",
    "#             FN += ((pred == 0) & (target.long()[:, 2].view_as(pred) == 1)).cpu().sum()\n",
    "# #             # FP    predict 1 label 0\n",
    "#             FP += ((pred == 1) & (target.long()[:, 2].view_as(pred) == 0)).cpu().sum()\n",
    "#             print(TP,TN,FN,FP)\n",
    "            \n",
    "            \n",
    "#             print(output[:,1].cpu().numpy())\n",
    "#             print((output[:,1]+output[:,0]).cpu().numpy())\n",
    "#             predcpu=(output[:,1].cpu().numpy())/((output[:,1]+output[:,0]).cpu().numpy())\n",
    "            targetcpu=target.long().cpu().numpy()\n",
    "            predlist=np.append(predlist, pred.cpu().numpy())\n",
    "            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])\n",
    "            targetlist=np.append(targetlist,targetcpu)\n",
    "        his['test_loss'].append(test_loss.data.cpu().numpy()/len(test_loader.dataset))\n",
    "        his['test_acc'].append(correct/len(test_loader.dataset))\n",
    "    return targetlist, scorelist, predlist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Load Self-Trans model\"\"\"\n",
    "# \"\"\"Change names and locations to the Self-Trans.pt\"\"\"\n",
    "\"baseline\"\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.densenet169(pretrained=True).cuda()\n",
    "\n",
    "modelname = 'Dense169_baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "his = {}\n",
    "his['train_loss'] = []\n",
    "his['train_acc'] = []\n",
    "his['val_loss'] = []\n",
    "his['val_acc'] = []\n",
    "his['test_loss'] = []\n",
    "his['test_acc'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/54 (0%)]\tTrain Loss: 1.068091\n",
      "Train Epoch: 1 [8/54 (15%)]\tTrain Loss: 0.328159\n",
      "Train Epoch: 1 [16/54 (30%)]\tTrain Loss: 0.173053\n",
      "Train Epoch: 1 [24/54 (44%)]\tTrain Loss: 0.045815\n",
      "Train Epoch: 1 [32/54 (59%)]\tTrain Loss: 0.118430\n",
      "Train Epoch: 1 [40/54 (74%)]\tTrain Loss: 0.048406\n",
      "Train Epoch: 1 [48/54 (89%)]\tTrain Loss: 0.064237\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.62010178e-01 9.57466245e-01 9.21583951e-01 5.07854760e-01\n",
      " 3.75627950e-02 8.82275850e-02 9.68153298e-01 1.78135321e-01\n",
      " 3.65944393e-02 4.39214054e-05 1.43312471e-04 3.17853710e-06\n",
      " 1.03099681e-02 9.31692719e-01 9.93846953e-01 7.05711835e-04\n",
      " 3.50472110e-05 2.48444546e-03 1.96969759e-05 6.36248216e-02\n",
      " 1.30179944e-02 1.86706355e-04 5.05123556e-01 1.88284546e-01\n",
      " 2.46512832e-06 1.39770824e-02 3.02231491e-01 1.04176402e-01\n",
      " 2.74351728e-03 2.80248784e-02 8.46991420e-01 7.12575376e-01\n",
      " 3.95611346e-01 1.66163172e-05 4.20361897e-03 1.54784604e-04\n",
      " 2.54355196e-04 1.00946228e-03 5.42418333e-04 8.82124994e-04\n",
      " 1.61874853e-03 9.36506782e-04 7.87207682e-05 2.56626430e-04\n",
      " 1.32086635e-01 9.02021229e-01 5.87030172e-01 4.18685377e-01\n",
      " 9.37843919e-01 1.38397560e-01 1.52947262e-01 1.57226223e-05\n",
      " 4.39409435e-01 7.73254083e-04 7.54836559e-01 6.55781757e-03\n",
      " 9.71174121e-01 4.74559255e-02 3.53820869e-05 8.28588367e-01\n",
      " 9.99963284e-01 9.55453455e-01 9.99701679e-01 9.99989271e-01\n",
      " 9.19066846e-01 7.84587488e-03 1.72214229e-02 1.13183156e-01\n",
      " 9.12326157e-01 3.60208191e-02 9.19525504e-01 9.67874527e-01\n",
      " 9.95287418e-01 9.80729282e-01 6.67142808e-01 7.29016066e-01\n",
      " 9.92833853e-01 9.56748068e-01 9.99663115e-01 9.87100601e-01\n",
      " 9.94542599e-01 9.99094367e-01 9.88077521e-01 1.82048231e-01\n",
      " 8.37314129e-03 1.01979747e-01 5.73450923e-02 4.74351168e-01\n",
      " 4.30471487e-02 1.06808720e-02 2.33998545e-03 6.76743150e-01\n",
      " 6.88383520e-01 6.90654144e-02 8.24936724e-04 6.04102062e-03\n",
      " 1.36012211e-04 3.64101329e-03 1.79761782e-01 1.04361540e-02\n",
      " 3.12813193e-01 9.99837041e-01 3.31425779e-02 2.08484754e-01\n",
      " 4.14076746e-02 8.70399654e-01 7.82604992e-01 2.71697668e-03\n",
      " 1.55710534e-03 7.12935105e-02 2.40362044e-02 8.59300024e-04\n",
      " 1.62401970e-03 1.39981667e-02 6.76833987e-02 1.32973595e-02\n",
      " 5.38663685e-01 5.15431046e-01]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "Train Epoch: 2 [0/54 (0%)]\tTrain Loss: 0.117378\n",
      "Train Epoch: 2 [8/54 (15%)]\tTrain Loss: 0.075787\n",
      "Train Epoch: 2 [16/54 (30%)]\tTrain Loss: 0.171507\n",
      "Train Epoch: 2 [24/54 (44%)]\tTrain Loss: 0.084924\n",
      "Train Epoch: 2 [32/54 (59%)]\tTrain Loss: 0.057937\n",
      "Train Epoch: 2 [40/54 (74%)]\tTrain Loss: 0.065572\n",
      "Train Epoch: 2 [48/54 (89%)]\tTrain Loss: 0.071241\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.79987603 0.96404171 0.95483238 0.66112041 0.73266518 0.93502796\n",
      " 0.9362613  0.73176771 0.73017931 0.60301036 0.60797226 0.47773361\n",
      " 0.70082223 0.8419708  0.92285877 0.68665498 0.7656092  0.3372246\n",
      " 0.36439461 0.47309816 0.65004349 0.52797014 0.53109246 0.68507379\n",
      " 0.67004472 0.68221015 0.69584924 0.64813316 0.59533167 0.6347059\n",
      " 0.71454006 0.59705228 0.57549137 0.7741909  0.67174983 0.73975348\n",
      " 0.71616656 0.56873196 0.94116694 0.8347103  0.92108119 0.80243951\n",
      " 0.72451234 0.56538701 0.51793164 0.87732452 0.87663347 0.93452847\n",
      " 0.91241115 0.6539644  0.89010161 0.48947585 0.8662166  0.8204127\n",
      " 0.36151743 0.52258492 0.74532366 0.51583225 0.66522413 0.77461958\n",
      " 0.99965    0.87773138 0.99745363 0.9998129  0.80770135 0.6661877\n",
      " 0.7057187  0.56829697 0.73076999 0.96367258 0.98414665 0.99077058\n",
      " 0.98991174 0.95106453 0.98341954 0.9721806  0.98676223 0.98432356\n",
      " 0.99782383 0.96903259 0.98207963 0.99467045 0.74863482 0.98032916\n",
      " 0.98628831 0.99499065 0.87894064 0.97193277 0.72592402 0.60900432\n",
      " 0.75666201 0.65502638 0.63777846 0.72506756 0.71978456 0.69690055\n",
      " 0.69399917 0.92792428 0.64662248 0.81862861 0.87954098 0.9990263\n",
      " 0.57055175 0.62245429 0.78633356 0.87146282 0.92374492 0.16841047\n",
      " 0.27709311 0.1012121  0.17859247 0.31080684 0.35247016 0.65580338\n",
      " 0.82024747 0.65573275 0.84760904 0.65184891]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 3 [0/54 (0%)]\tTrain Loss: 0.084413\n",
      "Train Epoch: 3 [8/54 (15%)]\tTrain Loss: 0.103272\n",
      "Train Epoch: 3 [16/54 (30%)]\tTrain Loss: 0.063897\n",
      "Train Epoch: 3 [24/54 (44%)]\tTrain Loss: 0.031961\n",
      "Train Epoch: 3 [32/54 (59%)]\tTrain Loss: 0.103759\n",
      "Train Epoch: 3 [40/54 (74%)]\tTrain Loss: 0.086353\n",
      "Train Epoch: 3 [48/54 (89%)]\tTrain Loss: 0.074900\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.         1.         1.         0.99999893 0.99997222 0.99854809\n",
      " 1.         0.99999785 0.90968794 0.25396761 0.35687789 0.21520734\n",
      " 0.57514101 0.9353689  0.99999988 0.96781331 0.73175967 0.39306971\n",
      " 0.35579377 0.43187922 0.4243525  0.46489263 0.53691518 0.39982501\n",
      " 0.45334131 0.43047509 0.31850612 0.43766752 0.39746717 0.40868673\n",
      " 0.51685792 0.72762161 0.70850801 0.33649528 0.31859881 0.38593197\n",
      " 0.36523691 0.57288831 0.46113083 0.51724058 0.56469691 0.47242132\n",
      " 0.57237774 0.95765734 0.9952122  0.99963582 1.         1.\n",
      " 1.         0.78872854 1.         0.53248918 0.54241943 0.9172498\n",
      " 0.54948485 0.90353817 0.45204803 0.71117777 0.31461313 1.\n",
      " 0.94719785 0.85939288 0.91447639 0.93686181 0.81962228 0.39914012\n",
      " 0.42892665 0.86169982 0.98017919 0.46307951 0.99998987 0.99996591\n",
      " 0.66902107 0.66423237 0.5365749  0.99897516 1.         1.\n",
      " 1.         1.         1.         1.         1.         0.60256958\n",
      " 0.82059181 0.50675315 0.99517316 0.99382317 0.47254404 0.49500456\n",
      " 0.50401956 0.43046948 0.39152774 0.45956028 0.44580275 0.43768111\n",
      " 0.35113558 0.68216473 0.37159821 0.48643544 0.43776122 0.99998331\n",
      " 0.9867093  0.69566262 0.78484017 0.972875   0.94645035 0.11627049\n",
      " 0.25975218 0.10105582 0.11709178 0.13415559 0.22088045 0.5058955\n",
      " 0.94744748 0.78728175 1.         1.        ]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 4 [0/54 (0%)]\tTrain Loss: 0.069901\n",
      "Train Epoch: 4 [8/54 (15%)]\tTrain Loss: 0.075041\n",
      "Train Epoch: 4 [16/54 (30%)]\tTrain Loss: 0.094514\n",
      "Train Epoch: 4 [24/54 (44%)]\tTrain Loss: 0.069595\n",
      "Train Epoch: 4 [32/54 (59%)]\tTrain Loss: 0.115234\n",
      "Train Epoch: 4 [40/54 (74%)]\tTrain Loss: 0.081961\n",
      "Train Epoch: 4 [48/54 (89%)]\tTrain Loss: 0.074556\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.00703251e-01 1.33966103e-01 4.08013850e-01 2.39414528e-01\n",
      " 2.57988423e-01 3.52841139e-01 2.65521789e-03 2.61349529e-01\n",
      " 2.53624856e-01 4.09626275e-01 2.47475773e-01 4.28711921e-01\n",
      " 2.02831551e-01 2.43030131e-01 2.22445816e-01 1.63258657e-01\n",
      " 1.49399310e-01 3.10977310e-01 2.42059335e-01 2.25728258e-01\n",
      " 3.71867031e-01 3.18432719e-01 3.67186040e-01 4.88106966e-01\n",
      " 3.06566566e-01 3.25037062e-01 5.02157032e-01 2.52600521e-01\n",
      " 2.03753203e-01 1.93736434e-01 3.40095878e-01 3.32967371e-01\n",
      " 2.30138898e-02 1.69599041e-01 1.70852065e-01 1.43166661e-01\n",
      " 1.31132081e-01 2.31101900e-01 3.38236749e-01 3.58798146e-01\n",
      " 3.98127854e-01 2.73445308e-01 2.87155300e-01 1.91052705e-01\n",
      " 2.12048233e-01 3.85188162e-01 2.63076872e-01 4.11090180e-02\n",
      " 4.02545393e-01 5.46954572e-01 5.06040454e-01 1.94328278e-01\n",
      " 1.69137791e-01 2.44249552e-01 3.46853465e-01 1.37769550e-01\n",
      " 3.75412524e-01 1.15989394e-01 3.21467280e-01 1.94969490e-01\n",
      " 5.11930525e-01 4.61115479e-01 4.71661448e-01 5.38040638e-01\n",
      " 2.93659419e-01 2.34361231e-01 2.50343859e-01 3.83131444e-01\n",
      " 3.76947731e-01 5.13306916e-01 5.99111676e-01 5.27995169e-01\n",
      " 5.05544543e-01 3.69869888e-01 3.37642282e-01 3.94177318e-01\n",
      " 3.47038061e-08 5.69245762e-10 3.86745393e-01 5.08297205e-01\n",
      " 6.11052632e-01 5.56025207e-01 3.83765221e-01 4.68846589e-01\n",
      " 5.54693401e-01 6.36485219e-01 4.10742313e-01 4.34843391e-01\n",
      " 2.69480228e-01 2.80290306e-01 4.64251906e-01 2.87478209e-01\n",
      " 2.74551451e-01 5.76591492e-01 5.11398494e-01 5.16785562e-01\n",
      " 2.29664221e-01 3.34878325e-01 2.31180102e-01 5.70127249e-01\n",
      " 2.48557001e-01 6.05303943e-01 2.09754184e-01 4.11002100e-01\n",
      " 4.75108534e-01 4.54612821e-01 5.49624205e-01 2.98756566e-02\n",
      " 3.03907339e-02 4.72867936e-02 4.08845656e-02 5.92268333e-02\n",
      " 4.52880822e-02 5.93368828e-01 3.87480408e-01 2.99200416e-01\n",
      " 3.05921465e-01 2.59096891e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/54 (0%)]\tTrain Loss: 0.066584\n",
      "Train Epoch: 5 [8/54 (15%)]\tTrain Loss: 0.072480\n",
      "Train Epoch: 5 [16/54 (30%)]\tTrain Loss: 0.053115\n",
      "Train Epoch: 5 [24/54 (44%)]\tTrain Loss: 0.082473\n",
      "Train Epoch: 5 [32/54 (59%)]\tTrain Loss: 0.065363\n",
      "Train Epoch: 5 [40/54 (74%)]\tTrain Loss: 0.050917\n",
      "Train Epoch: 5 [48/54 (89%)]\tTrain Loss: 0.086234\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.99275827 0.9991473  0.99932587 0.88073969 0.95470512 0.92970526\n",
      " 0.99930632 0.93536693 0.26607555 0.60843384 0.44271886 0.60559416\n",
      " 0.74174613 0.80639058 0.86493951 0.66330177 0.56314749 0.84928602\n",
      " 0.7219432  0.67941487 0.81622612 0.87049043 0.93854225 0.78657591\n",
      " 0.73022306 0.85741073 0.8023296  0.84152788 0.88818991 0.88791573\n",
      " 0.99549484 0.99792385 0.9994061  0.24185143 0.27651879 0.26808998\n",
      " 0.35016978 0.68191594 0.82104415 0.71746564 0.85127008 0.63896036\n",
      " 0.73756242 0.88446289 0.91493696 0.99750996 0.99999678 0.99999809\n",
      " 0.9999975  0.89239401 0.9999907  0.3619931  0.4331108  0.64318597\n",
      " 0.67814571 0.60941607 0.96621978 0.40567288 0.50087106 0.93723965\n",
      " 0.99067616 0.96610826 0.98958778 0.99838614 0.96330905 0.84619945\n",
      " 0.85667247 0.95363897 0.97946244 0.67812359 0.98633283 0.99103141\n",
      " 0.99757546 0.88869888 0.85838932 0.99797791 0.9994843  0.9991374\n",
      " 0.99983764 0.99983501 0.99952209 0.99940479 0.99898201 0.95553619\n",
      " 0.99022865 0.99888164 0.99655932 0.99882585 0.59346592 0.5932954\n",
      " 0.77660418 0.85737199 0.81417006 0.83464217 0.75992429 0.79170728\n",
      " 0.30756989 0.97024912 0.47454643 0.88215196 0.40241304 0.99881858\n",
      " 0.43154031 0.91412121 0.88643861 0.97615463 0.98209518 0.47979447\n",
      " 0.80774862 0.2230954  0.56296551 0.53773969 0.87996417 0.83060384\n",
      " 0.88128662 0.8180238  0.9251104  0.8347249 ]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 6 [0/54 (0%)]\tTrain Loss: 0.078721\n",
      "Train Epoch: 6 [8/54 (15%)]\tTrain Loss: 0.055358\n",
      "Train Epoch: 6 [16/54 (30%)]\tTrain Loss: 0.065861\n",
      "Train Epoch: 6 [24/54 (44%)]\tTrain Loss: 0.083266\n",
      "Train Epoch: 6 [32/54 (59%)]\tTrain Loss: 0.080467\n",
      "Train Epoch: 6 [40/54 (74%)]\tTrain Loss: 0.108168\n",
      "Train Epoch: 6 [48/54 (89%)]\tTrain Loss: 0.055705\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.70894837 1.         0.99999928 0.89167941 0.8189553  0.73076308\n",
      " 1.         0.87287343 0.71868408 0.17695443 0.82695693 0.4905929\n",
      " 0.8204295  0.56897515 0.65838498 0.75312287 0.76659817 0.79827011\n",
      " 0.81054366 0.87146837 0.890481   0.89993548 0.90693098 0.85530186\n",
      " 0.90330577 0.90000159 0.89863247 0.85338706 0.80979007 0.7445634\n",
      " 0.65323901 0.77045333 0.6659978  0.69901097 0.70654476 0.78213429\n",
      " 0.75445724 0.83599311 0.89389974 0.86077076 0.87290186 0.85938245\n",
      " 0.82745093 0.83796024 0.8626824  0.7746678  1.         0.99999988\n",
      " 1.         0.88523191 0.99999893 0.51483512 0.70777363 0.71047902\n",
      " 0.63887477 0.75944239 0.88769746 0.66269702 0.43952283 0.8721748\n",
      " 0.73568702 0.63085914 0.80249578 0.87618971 0.70594579 0.6342262\n",
      " 0.67483312 0.93637508 0.91798717 0.75409395 0.91670299 0.87929326\n",
      " 0.71168786 0.69009972 0.65909082 0.98077607 1.         1.\n",
      " 1.         0.96161854 0.96751964 0.97604179 0.9890058  0.86754364\n",
      " 0.83564192 0.78147084 0.99986684 0.99888557 0.70772052 0.87715513\n",
      " 0.9107371  0.810987   0.8128233  0.90019786 0.91195333 0.88443822\n",
      " 0.71753222 0.85672742 0.61622167 0.89674586 0.72117281 0.88888812\n",
      " 0.84820402 0.8338176  0.8418678  0.72804868 0.64104629 0.64723229\n",
      " 0.8384757  0.19181885 0.81486964 0.83990824 0.89504826 0.91137123\n",
      " 0.88069534 0.89186049 0.88274467 0.84642446]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 7 [0/54 (0%)]\tTrain Loss: 0.049479\n",
      "Train Epoch: 7 [8/54 (15%)]\tTrain Loss: 0.076838\n",
      "Train Epoch: 7 [16/54 (30%)]\tTrain Loss: 0.039872\n",
      "Train Epoch: 7 [24/54 (44%)]\tTrain Loss: 0.074104\n",
      "Train Epoch: 7 [32/54 (59%)]\tTrain Loss: 0.075722\n",
      "Train Epoch: 7 [40/54 (74%)]\tTrain Loss: 0.104445\n",
      "Train Epoch: 7 [48/54 (89%)]\tTrain Loss: 0.084758\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.12982281 0.89591795 0.85610908 0.24404961 0.23582764 0.08418597\n",
      " 0.89987308 0.33376369 0.07232126 0.03969387 0.06921556 0.04531679\n",
      " 0.11526649 0.39746287 0.39272103 0.09309696 0.14837852 0.18783449\n",
      " 0.35508263 0.48745355 0.45171571 0.50997448 0.495736   0.27365372\n",
      " 0.62528008 0.49966502 0.31186366 0.60354733 0.50947595 0.48472899\n",
      " 0.51518649 0.56805509 0.40356293 0.25081483 0.21814056 0.23507549\n",
      " 0.20767836 0.13299714 0.50129914 0.51585227 0.48925713 0.47477636\n",
      " 0.14639783 0.11590313 0.35218212 0.34600732 0.37873185 0.55819941\n",
      " 0.45204967 0.09496881 0.42872408 0.0500022  0.17395343 0.10941461\n",
      " 0.25915888 0.21039018 0.47354257 0.21905175 0.0649185  0.35378358\n",
      " 0.47554958 0.41763106 0.54207069 0.55571568 0.58925873 0.4444817\n",
      " 0.62085181 0.72312623 0.70934653 0.31244692 0.45272273 0.48250917\n",
      " 0.4897812  0.50598896 0.48941192 0.64508986 0.97300988 0.97297263\n",
      " 0.80164284 0.68973076 0.7719968  0.78104126 0.65852296 0.19570918\n",
      " 0.25250393 0.3713077  0.60627675 0.5195291  0.99645787 0.45862284\n",
      " 0.99886143 0.22275293 0.36144751 0.72002381 0.78327447 0.70225638\n",
      " 0.12695083 0.74727386 0.28284374 0.63817114 0.55103737 0.58231127\n",
      " 0.30027333 0.25502944 0.25988832 0.39048663 0.2796506  0.17781311\n",
      " 0.65551627 0.1686004  0.39786583 0.63433957 0.76557535 0.58671826\n",
      " 0.77361196 0.53432536 0.73316073 0.57647353]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 8 [0/54 (0%)]\tTrain Loss: 0.077048\n",
      "Train Epoch: 8 [8/54 (15%)]\tTrain Loss: 0.101068\n",
      "Train Epoch: 8 [16/54 (30%)]\tTrain Loss: 0.072162\n",
      "Train Epoch: 8 [24/54 (44%)]\tTrain Loss: 0.086874\n",
      "Train Epoch: 8 [32/54 (59%)]\tTrain Loss: 0.060724\n",
      "Train Epoch: 8 [40/54 (74%)]\tTrain Loss: 0.078118\n",
      "Train Epoch: 8 [48/54 (89%)]\tTrain Loss: 0.095908\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.9762733  0.99961829 0.9978528  0.89097065 0.8838191  0.92064524\n",
      " 0.99915922 0.94383466 0.85720026 0.74669373 0.76587051 0.75008965\n",
      " 0.76571351 0.83136016 0.90570223 0.80906034 0.75421834 0.76847947\n",
      " 0.68901324 0.74148101 0.70697331 0.75849998 0.76536345 0.75219351\n",
      " 0.7617994  0.76161194 0.75111037 0.76065606 0.76629603 0.77703989\n",
      " 0.79932547 0.80668175 0.84469187 0.70027792 0.74931151 0.75235319\n",
      " 0.70205235 0.76619494 0.78175753 0.7635054  0.77028757 0.7556228\n",
      " 0.79893553 0.78298223 0.81952274 0.91999549 0.99277014 0.9952997\n",
      " 0.99447674 0.83460689 0.99213034 0.7766729  0.79155368 0.80874181\n",
      " 0.75450182 0.80821365 0.79969507 0.76395583 0.68000513 0.91950285\n",
      " 0.84367895 0.80096251 0.84778982 0.85282606 0.8546226  0.82351923\n",
      " 0.8460415  0.82644194 0.88968551 0.81991649 0.92198962 0.912296\n",
      " 0.79322654 0.77572429 0.78941339 0.8925271  0.99998045 0.99998438\n",
      " 0.99606663 0.93205005 0.92665702 0.93814129 0.93230879 0.77622181\n",
      " 0.77220368 0.81037873 0.79918438 0.81401074 0.70096403 0.71142262\n",
      " 0.75163746 0.78126401 0.77699333 0.74509901 0.75125206 0.75986439\n",
      " 0.68608814 0.7603153  0.74437904 0.75409025 0.7220192  0.86966401\n",
      " 0.71653891 0.78260118 0.79775518 0.81471443 0.79856128 0.73260003\n",
      " 0.75770026 0.71842134 0.72867423 0.75133872 0.7658214  0.76637441\n",
      " 0.79435003 0.72014314 0.97338879 0.95058465]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [0/54 (0%)]\tTrain Loss: 0.085707\n",
      "Train Epoch: 9 [8/54 (15%)]\tTrain Loss: 0.071456\n",
      "Train Epoch: 9 [16/54 (30%)]\tTrain Loss: 0.088313\n",
      "Train Epoch: 9 [24/54 (44%)]\tTrain Loss: 0.057509\n",
      "Train Epoch: 9 [32/54 (59%)]\tTrain Loss: 0.064636\n",
      "Train Epoch: 9 [40/54 (74%)]\tTrain Loss: 0.065840\n",
      "Train Epoch: 9 [48/54 (89%)]\tTrain Loss: 0.074820\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.5056479  0.96332246 0.94821697 0.9981668  1.         0.54361707\n",
      " 0.93943471 0.99939823 0.94668329 0.9999944  1.         0.99996316\n",
      " 1.         0.76045787 0.71724445 1.         0.99995542 0.71931672\n",
      " 0.96316993 0.8026306  0.99852657 0.68297654 0.58582222 0.34936407\n",
      " 0.87803549 0.55235475 0.80049497 0.80814809 0.93694705 0.94212705\n",
      " 0.92204869 0.91029465 0.92803937 1.         1.         1.\n",
      " 1.         0.59291428 0.99999988 0.99999976 0.9999243  1.\n",
      " 0.68589711 0.99933088 0.83841169 0.98414707 0.98806912 0.99956101\n",
      " 0.8396458  0.89720583 0.84546781 1.         0.55576199 0.99962723\n",
      " 0.99894041 0.97774726 0.51953751 0.99920779 0.66033149 0.65365142\n",
      " 0.76008439 0.75853324 0.79920632 0.78902298 0.82729989 0.99992847\n",
      " 0.99998713 0.88969362 0.99985588 1.         0.94647199 0.99806505\n",
      " 0.980645   0.99204224 0.97282648 0.77134788 0.99216813 0.99191284\n",
      " 0.92746669 0.9941929  0.99965191 0.97480088 0.89754671 0.80764437\n",
      " 0.69327921 0.7215997  0.73167676 0.75369602 0.82552963 0.99992943\n",
      " 0.71729928 1.         1.         0.97270298 0.98191857 0.99653167\n",
      " 0.67447263 0.99695158 0.99995315 1.         0.98623985 0.81968755\n",
      " 0.74922687 0.99999917 1.         0.95854807 0.89664549 0.99999249\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.97630978 0.71029615 0.72819299 0.74954963]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 10 [0/54 (0%)]\tTrain Loss: 0.100659\n",
      "Train Epoch: 10 [8/54 (15%)]\tTrain Loss: 0.092032\n",
      "Train Epoch: 10 [16/54 (30%)]\tTrain Loss: 0.062482\n",
      "Train Epoch: 10 [24/54 (44%)]\tTrain Loss: 0.068852\n",
      "Train Epoch: 10 [32/54 (59%)]\tTrain Loss: 0.092299\n",
      "Train Epoch: 10 [40/54 (74%)]\tTrain Loss: 0.109630\n",
      "Train Epoch: 10 [48/54 (89%)]\tTrain Loss: 0.093435\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [7.47344851e-01 2.95698583e-01 6.63804710e-01 5.28569400e-01\n",
      " 5.05619347e-01 5.78078985e-01 1.14163943e-01 5.38855910e-01\n",
      " 3.19230437e-01 3.87249678e-01 4.31673318e-01 3.87182444e-01\n",
      " 3.90275091e-01 4.96947318e-01 5.46751916e-01 2.50096637e-04\n",
      " 5.88472722e-06 1.22961152e-04 7.79787806e-05 5.54031739e-03\n",
      " 3.67120624e-01 1.78525001e-02 4.29422796e-01 4.19149369e-01\n",
      " 3.52770905e-04 4.23902899e-01 4.27235007e-01 3.62096936e-01\n",
      " 2.80148163e-02 2.90132482e-02 4.33879167e-01 6.26564175e-02\n",
      " 7.91958869e-01 1.40567601e-01 4.08343792e-01 3.83740455e-01\n",
      " 2.37559085e-03 3.51314098e-01 3.44628602e-01 3.73559177e-01\n",
      " 4.04801786e-01 3.41314048e-01 4.00825031e-02 1.23377276e-05\n",
      " 2.43986831e-09 5.03663495e-02 3.75006944e-01 2.61126488e-01\n",
      " 7.82215178e-01 4.36935276e-01 8.65076303e-01 6.88232258e-02\n",
      " 4.50652003e-01 3.42979074e-01 3.16214502e-01 1.36734988e-03\n",
      " 2.55985439e-01 4.37110275e-01 3.91222209e-01 3.83207411e-01\n",
      " 6.95021808e-01 5.94648004e-01 7.03864217e-01 7.20535159e-01\n",
      " 3.83920759e-01 5.38388908e-01 6.06609643e-01 4.91846979e-01\n",
      " 5.91106176e-01 5.60280204e-01 6.33851528e-01 5.22267878e-01\n",
      " 3.37689459e-01 5.03272593e-01 2.36843050e-01 9.33871070e-06\n",
      " 9.23849106e-01 9.09370959e-01 7.70698965e-01 7.02395320e-01\n",
      " 7.66824484e-01 4.42944944e-01 1.32581398e-01 5.80609321e-06\n",
      " 4.47396876e-11 3.44641894e-01 1.19305709e-02 1.09940350e-01\n",
      " 3.18311602e-01 3.56091917e-01 2.20045775e-01 3.01280439e-01\n",
      " 2.74334550e-01 4.44407642e-01 4.51458663e-01 4.51682389e-01\n",
      " 4.40362185e-01 1.85738406e-06 6.39148382e-03 3.45018804e-01\n",
      " 8.65884009e-04 4.77865338e-01 2.44612277e-08 3.03068221e-01\n",
      " 4.71528649e-01 1.65848404e-01 1.56193480e-01 3.91969770e-01\n",
      " 4.50429708e-01 1.05828948e-01 4.28458750e-01 2.87741274e-01\n",
      " 4.13484871e-01 3.04534882e-01 4.83322054e-01 1.06470369e-01\n",
      " 4.57240105e-01 4.93215233e-01]\n",
      "predict [1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "vote_pred [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 47 TN= 20 FN= 11 FP= 40\n",
      "TP+FP 87\n",
      "precision 0.5402298850574713\n",
      "recall 0.8103448275862069\n",
      "F1 0.6482758620689655\n",
      "acc 0.5677966101694916\n",
      "AUCp 0.5718390804597702\n",
      "AUC 0.6824712643678161\n",
      "\n",
      " The epoch is 10, average recall: 0.8103, average precision: 0.5402,average F1: 0.6483, average accuracy: 0.5678, average AUC: 0.6825\n",
      "Train Epoch: 11 [0/54 (0%)]\tTrain Loss: 0.071975\n",
      "Train Epoch: 11 [8/54 (15%)]\tTrain Loss: 0.062078\n",
      "Train Epoch: 11 [16/54 (30%)]\tTrain Loss: 0.100592\n",
      "Train Epoch: 11 [24/54 (44%)]\tTrain Loss: 0.066694\n",
      "Train Epoch: 11 [32/54 (59%)]\tTrain Loss: 0.063525\n",
      "Train Epoch: 11 [40/54 (74%)]\tTrain Loss: 0.073322\n",
      "Train Epoch: 11 [48/54 (89%)]\tTrain Loss: 0.093635\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.9709307  0.93103659 0.86471957 0.8089022  0.80027121 0.8335253\n",
      " 0.81096572 0.82797909 0.6210199  0.67870945 0.66230327 0.63681763\n",
      " 0.69189119 0.85258347 0.87412745 0.7237891  0.70935935 0.64190447\n",
      " 0.62448722 0.70833409 0.63186187 0.65666169 0.80565137 0.83769554\n",
      " 0.7343266  0.81047261 0.87593412 0.7753306  0.74274462 0.67088681\n",
      " 0.97699785 0.92548907 0.95590192 0.73969018 0.77798241 0.70780551\n",
      " 0.6294024  0.70788163 0.70220381 0.72215694 0.75131476 0.71172148\n",
      " 0.68997216 0.72023171 0.73584324 0.84341711 0.98727489 0.96859163\n",
      " 0.93302196 0.80935842 0.99014246 0.65514076 0.71114427 0.79657423\n",
      " 0.76445955 0.70253557 0.81901902 0.74496371 0.74911976 0.72969121\n",
      " 0.91017056 0.84644485 0.92858577 0.92716533 0.79802644 0.8607322\n",
      " 0.89630497 0.85189807 0.89008605 0.91892117 0.89131474 0.8833707\n",
      " 0.89788115 0.86250567 0.83997673 0.92271614 0.9771648  0.94790685\n",
      " 0.93782693 0.96498668 0.96625108 0.97754914 0.97394401 0.68545628\n",
      " 0.7319811  0.78450501 0.85901481 0.88334566 0.80126733 0.72768098\n",
      " 0.81733918 0.80505317 0.77724099 0.89108175 0.82991087 0.84768754\n",
      " 0.73591095 0.70859963 0.73516512 0.81361181 0.7487042  0.97156286\n",
      " 0.65811557 0.7285071  0.76745266 0.80475038 0.77503031 0.69662857\n",
      " 0.76577646 0.74111158 0.77454579 0.78170425 0.89850426 0.76114011\n",
      " 0.72738135 0.64664572 0.77335316 0.77364892]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [0/54 (0%)]\tTrain Loss: 0.082212\n",
      "Train Epoch: 12 [8/54 (15%)]\tTrain Loss: 0.117085\n",
      "Train Epoch: 12 [16/54 (30%)]\tTrain Loss: 0.077389\n",
      "Train Epoch: 12 [24/54 (44%)]\tTrain Loss: 0.107172\n",
      "Train Epoch: 12 [32/54 (59%)]\tTrain Loss: 0.054865\n",
      "Train Epoch: 12 [40/54 (74%)]\tTrain Loss: 0.112333\n",
      "Train Epoch: 12 [48/54 (89%)]\tTrain Loss: 0.037097\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.87886316 0.91272807 0.73604417 0.55468529 0.35917065 0.82072496\n",
      " 0.86077976 0.85249126 0.82431096 0.1656151  0.36018518 0.14962392\n",
      " 0.481177   0.74233031 0.79568791 0.37954816 0.35080457 0.40534335\n",
      " 0.55405504 0.57173252 0.47044817 0.57226223 0.58535838 0.32046375\n",
      " 0.61322147 0.48337209 0.31189796 0.58449781 0.58007812 0.63654512\n",
      " 0.8414669  0.86219543 0.8264097  0.31056318 0.31481808 0.44128564\n",
      " 0.45507962 0.57569802 0.53675753 0.50725263 0.61584568 0.51131481\n",
      " 0.33628964 0.52443552 0.62741655 0.73582417 0.84193504 0.91455543\n",
      " 0.99752647 0.41079292 0.9880116  0.29089338 0.45144609 0.64530498\n",
      " 0.41359761 0.50476736 0.71051478 0.73049504 0.19820175 0.77621311\n",
      " 0.92570066 0.8153944  0.87803513 0.89853644 0.81317055 0.81297803\n",
      " 0.85954648 0.82927424 0.75316    0.92377388 0.75606406 0.71221131\n",
      " 0.76700872 0.60400206 0.71677959 0.86752272 0.9768312  0.95631635\n",
      " 0.95635873 0.9338882  0.90829837 0.89826757 0.919406   0.92612433\n",
      " 0.6711067  0.90966779 0.68502074 0.71916759 0.6065672  0.59348625\n",
      " 0.50588596 0.57742113 0.59074146 0.43186781 0.43627524 0.49061778\n",
      " 0.37566346 0.48367804 0.42609298 0.46651807 0.3245132  0.85021198\n",
      " 0.5857954  0.80440468 0.48513556 0.64006776 0.76373738 0.37756273\n",
      " 0.55384916 0.43573755 0.40972221 0.46495807 0.44608805 0.52971822\n",
      " 0.71442389 0.71257716 0.69656676 0.72110063]\n",
      "predict [1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 13 [0/54 (0%)]\tTrain Loss: 0.101542\n",
      "Train Epoch: 13 [8/54 (15%)]\tTrain Loss: 0.058652\n",
      "Train Epoch: 13 [16/54 (30%)]\tTrain Loss: 0.084915\n",
      "Train Epoch: 13 [24/54 (44%)]\tTrain Loss: 0.118148\n",
      "Train Epoch: 13 [32/54 (59%)]\tTrain Loss: 0.062200\n",
      "Train Epoch: 13 [40/54 (74%)]\tTrain Loss: 0.086053\n",
      "Train Epoch: 13 [48/54 (89%)]\tTrain Loss: 0.110990\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.6879788  0.73509419 0.68731123 0.59523571 0.55466127 0.64845991\n",
      " 0.67221457 0.60324025 0.63874137 0.3794637  0.36735955 0.366532\n",
      " 0.30474749 0.64106125 0.69138551 0.52500433 0.55811125 0.6235252\n",
      " 0.39152297 0.4934288  0.47546944 0.37569964 0.43125302 0.99706525\n",
      " 0.36927587 0.51267886 0.99999988 0.65367222 0.39392149 0.46935895\n",
      " 0.55671161 0.57226282 0.59846914 0.44864467 0.51252741 0.36242828\n",
      " 0.39145231 0.47376797 0.6295976  0.61194181 0.41832164 0.53249002\n",
      " 0.49761191 0.44558802 0.51624167 0.67268306 0.6731053  0.6728586\n",
      " 0.71155053 0.48911378 0.68930745 0.36917445 0.62472618 0.44080001\n",
      " 0.4607833  0.43965077 0.5522967  0.35856748 0.40200537 0.73278528\n",
      " 0.97318506 0.99851519 0.76775724 0.8911823  0.73996365 0.51261228\n",
      " 0.57180113 0.61241907 0.68031603 0.65107405 0.97342038 0.97090536\n",
      " 0.56527942 0.60152876 0.58914787 0.55550784 0.84744817 0.83244586\n",
      " 0.80446446 0.64291245 0.64152157 0.79274404 0.99927467 0.97451448\n",
      " 0.99794894 0.9996897  0.9999572  0.99999964 0.99999225 0.53021806\n",
      " 1.         0.5828644  0.6026085  0.4752281  0.34191051 0.42135996\n",
      " 0.33763701 0.62227041 0.58560008 0.42690676 0.46853787 0.57919937\n",
      " 0.54332018 0.51771683 0.51502371 0.63329071 0.46646339 0.39380857\n",
      " 0.43121621 0.44953245 0.44678548 0.62642211 0.38637891 0.4100261\n",
      " 0.52280641 0.52887863 0.46271271 0.53267068]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
      "Train Epoch: 14 [0/54 (0%)]\tTrain Loss: 0.089131\n",
      "Train Epoch: 14 [8/54 (15%)]\tTrain Loss: 0.067341\n",
      "Train Epoch: 14 [16/54 (30%)]\tTrain Loss: 0.118881\n",
      "Train Epoch: 14 [24/54 (44%)]\tTrain Loss: 0.115902\n",
      "Train Epoch: 14 [32/54 (59%)]\tTrain Loss: 0.054049\n",
      "Train Epoch: 14 [40/54 (74%)]\tTrain Loss: 0.117180\n",
      "Train Epoch: 14 [48/54 (89%)]\tTrain Loss: 0.105216\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.61501473 0.89956158 0.8555516  0.85705644 0.71036249 0.40033689\n",
      " 0.88855886 0.92775267 0.30957288 0.2399147  0.67268127 0.33149576\n",
      " 0.74901456 0.77937049 0.87712926 0.71034366 0.65569919 0.44447643\n",
      " 0.60367644 0.51197302 0.55414474 0.57196075 0.68877715 0.40646297\n",
      " 0.66211665 0.66009337 0.60796511 0.72402149 0.71982867 0.75216985\n",
      " 0.71920562 0.8163628  0.78621614 0.64908576 0.62170821 0.68275702\n",
      " 0.64834487 0.88944536 0.77592313 0.72842014 0.71617562 0.67945755\n",
      " 0.58202028 0.79755086 0.86229557 0.88943732 0.83950657 0.87568289\n",
      " 0.90161973 0.41967103 0.82633692 0.58598435 0.78266847 0.87421548\n",
      " 0.37733498 0.73167825 0.715994   0.78574926 0.26590011 0.88913506\n",
      " 0.69347566 0.60207748 0.67102873 0.67770958 0.87551504 0.90381813\n",
      " 0.81650394 0.75301903 0.87537324 0.82408595 0.53285062 0.54564792\n",
      " 0.76157618 0.79101539 0.77990383 0.76603186 0.94388849 0.92176884\n",
      " 0.94556475 0.85736662 0.84946322 0.8148396  0.7829324  0.89926428\n",
      " 0.68812168 0.63369048 0.70863271 0.73233306 0.84724092 0.65917212\n",
      " 0.72400284 0.76625645 0.83220005 0.7927646  0.73861694 0.74067187\n",
      " 0.64367664 0.55130386 0.52882206 0.70680213 0.42746028 0.7589559\n",
      " 0.61074841 0.78361344 0.62753487 0.72887754 0.64285165 0.58133125\n",
      " 0.62137055 0.54463363 0.62285131 0.7617541  0.77756172 0.56111908\n",
      " 0.69391257 0.60410154 0.51761603 0.55958635]\n",
      "predict [1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 15 [0/54 (0%)]\tTrain Loss: 0.065545\n",
      "Train Epoch: 15 [8/54 (15%)]\tTrain Loss: 0.086672\n",
      "Train Epoch: 15 [16/54 (30%)]\tTrain Loss: 0.075674\n",
      "Train Epoch: 15 [24/54 (44%)]\tTrain Loss: 0.053549\n",
      "Train Epoch: 15 [32/54 (59%)]\tTrain Loss: 0.064501\n",
      "Train Epoch: 15 [40/54 (74%)]\tTrain Loss: 0.078106\n",
      "Train Epoch: 15 [48/54 (89%)]\tTrain Loss: 0.082888\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.62066555 0.88161778 0.83485758 0.79555899 0.71007872 0.55648863\n",
      " 0.82988709 0.8843655  0.38680801 0.26212367 0.55132198 0.26655352\n",
      " 0.66889006 0.89183605 0.91307729 0.72843874 0.63345063 0.50956887\n",
      " 0.36996028 0.48202822 0.49470869 0.65132624 0.76526499 0.73893303\n",
      " 0.69219595 0.75578249 0.85130733 0.69507718 0.70920581 0.78417903\n",
      " 0.78857982 0.80716115 0.8488937  0.51808774 0.53202993 0.69947928\n",
      " 0.47239059 0.8083964  0.71310306 0.72753805 0.74559504 0.70118403\n",
      " 0.59412193 0.76884961 0.81910843 0.86050606 0.81084067 0.85181731\n",
      " 0.90400374 0.56833494 0.83478379 0.56614429 0.80379486 0.79636055\n",
      " 0.56242275 0.65476453 0.75402343 0.6775347  0.3605139  0.89048809\n",
      " 0.85377115 0.82870483 0.84499025 0.84543628 0.88931888 0.90950692\n",
      " 0.84987259 0.87111723 0.91109198 0.69794476 0.65552723 0.64301193\n",
      " 0.83310688 0.82160085 0.81271338 0.83651185 0.95861381 0.95072079\n",
      " 0.95484626 0.90073323 0.90569788 0.89637423 0.84472132 0.89277202\n",
      " 0.78855896 0.63108486 0.80044872 0.83481055 0.75150841 0.50309753\n",
      " 0.7616623  0.76270479 0.81446409 0.87012637 0.81711829 0.77729565\n",
      " 0.68561041 0.62217551 0.48393011 0.77539897 0.41789946 0.81350946\n",
      " 0.61089849 0.75875115 0.63731289 0.77821225 0.64455432 0.36526978\n",
      " 0.56254059 0.49079031 0.43179864 0.80135345 0.83174628 0.51287782\n",
      " 0.76929027 0.73434168 0.79965597 0.8071143 ]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16 [0/54 (0%)]\tTrain Loss: 0.102760\n",
      "Train Epoch: 16 [8/54 (15%)]\tTrain Loss: 0.073690\n",
      "Train Epoch: 16 [16/54 (30%)]\tTrain Loss: 0.052729\n",
      "Train Epoch: 16 [24/54 (44%)]\tTrain Loss: 0.096236\n",
      "Train Epoch: 16 [32/54 (59%)]\tTrain Loss: 0.074952\n",
      "Train Epoch: 16 [40/54 (74%)]\tTrain Loss: 0.039784\n",
      "Train Epoch: 16 [48/54 (89%)]\tTrain Loss: 0.040080\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.79227167 0.94674057 0.85115707 0.7480287  0.59475631 0.53879249\n",
      " 0.83248347 0.8346132  0.3549228  0.1493452  0.38193575 0.15782863\n",
      " 0.44753098 0.84932035 0.90926707 0.56180662 0.47644335 0.42631388\n",
      " 0.37494379 0.40911758 0.42980009 0.50100034 0.55717307 0.47058135\n",
      " 0.55814445 0.52250898 0.59651953 0.57010585 0.56108761 0.60411459\n",
      " 0.88447922 0.85513228 0.89004546 0.35221663 0.40805256 0.47304031\n",
      " 0.38554221 0.66323292 0.58348948 0.61676109 0.63784015 0.59637976\n",
      " 0.39163914 0.57638299 0.69860435 0.79111814 0.93142653 0.90872151\n",
      " 0.98528659 0.39473367 0.9659245  0.32910746 0.57176828 0.63561422\n",
      " 0.38284972 0.48558092 0.70654082 0.49193665 0.24627683 0.80675906\n",
      " 0.92845213 0.8585633  0.92958111 0.95382744 0.88109428 0.88389552\n",
      " 0.84693623 0.88402706 0.87351876 0.7288363  0.84187841 0.82375181\n",
      " 0.84732842 0.78514367 0.77655733 0.87341446 0.99263036 0.98737049\n",
      " 0.97595882 0.97097325 0.96941423 0.9682644  0.94288749 0.93919456\n",
      " 0.63974994 0.7163133  0.81932771 0.86253434 0.63348979 0.49605259\n",
      " 0.65288794 0.60540521 0.68226242 0.62753391 0.63738942 0.60620129\n",
      " 0.44530317 0.53418463 0.45661503 0.65212142 0.43428066 0.91627526\n",
      " 0.55490041 0.62665993 0.52881181 0.74277306 0.66078079 0.36956334\n",
      " 0.53001404 0.35012776 0.41492081 0.56767946 0.61641502 0.45443669\n",
      " 0.63858449 0.56329376 0.64446408 0.66666955]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
      "Train Epoch: 17 [0/54 (0%)]\tTrain Loss: 0.047666\n",
      "Train Epoch: 17 [8/54 (15%)]\tTrain Loss: 0.124242\n",
      "Train Epoch: 17 [16/54 (30%)]\tTrain Loss: 0.060055\n",
      "Train Epoch: 17 [24/54 (44%)]\tTrain Loss: 0.071383\n",
      "Train Epoch: 17 [32/54 (59%)]\tTrain Loss: 0.056267\n",
      "Train Epoch: 17 [40/54 (74%)]\tTrain Loss: 0.090797\n",
      "Train Epoch: 17 [48/54 (89%)]\tTrain Loss: 0.066262\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.79028809 0.82324702 0.64951372 0.53961349 0.41080406 0.6472187\n",
      " 0.57299024 0.6713292  0.30105564 0.31766948 0.19185136 0.95249271\n",
      " 0.27416435 0.61853367 0.67107075 0.31326327 0.2920748  0.16923055\n",
      " 0.12965599 0.22961825 0.32288212 0.24497265 0.68996537 0.86583638\n",
      " 0.29272926 0.91258717 0.99895287 0.31994754 0.31461203 0.49238309\n",
      " 0.57475108 0.52336079 0.48277029 0.09880649 0.16450891 0.16353096\n",
      " 0.16302328 0.45043373 0.44748738 0.41652384 0.50192124 0.38518134\n",
      " 0.26759568 0.33639911 0.43988675 0.67855167 0.8592416  0.84264445\n",
      " 0.90720797 0.86449248 0.92437541 0.17837615 0.40691325 0.40602723\n",
      " 0.54340863 0.18243802 0.54215682 0.21556187 0.08556984 0.69481379\n",
      " 0.77065951 0.58301079 0.75278819 0.76156473 0.65125173 0.6729936\n",
      " 0.63518828 0.65254903 0.61236298 0.77414542 0.70219523 0.68314892\n",
      " 0.57635856 0.55624348 0.58119458 0.6981017  0.96556735 0.93382639\n",
      " 0.93825746 0.8332656  0.83761019 0.86414737 0.86789834 0.84211463\n",
      " 0.53513592 0.48338819 0.88835597 0.8862924  0.66829795 0.24733901\n",
      " 0.97971636 0.30732891 0.40866095 0.96189737 0.6376639  0.78593111\n",
      " 0.15159795 0.31614527 0.24865364 0.43657604 0.25836366 0.76456916\n",
      " 0.29603419 0.45951575 0.35272226 0.51841992 0.48957199 0.19723566\n",
      " 0.30598289 0.17506938 0.18936528 0.23817398 0.25630662 0.2770184\n",
      " 0.46123207 0.32310024 0.35450357 0.54087764]\n",
      "predict [1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 18 [0/54 (0%)]\tTrain Loss: 0.050042\n",
      "Train Epoch: 18 [8/54 (15%)]\tTrain Loss: 0.077445\n",
      "Train Epoch: 18 [16/54 (30%)]\tTrain Loss: 0.105311\n",
      "Train Epoch: 18 [24/54 (44%)]\tTrain Loss: 0.072177\n",
      "Train Epoch: 18 [32/54 (59%)]\tTrain Loss: 0.098295\n",
      "Train Epoch: 18 [40/54 (74%)]\tTrain Loss: 0.199111\n",
      "Train Epoch: 18 [48/54 (89%)]\tTrain Loss: 0.055620\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.64820695 0.92535961 0.87776613 0.61686754 0.54482198 0.55154079\n",
      " 0.85466629 0.64948064 0.46293321 0.16342251 0.15227969 0.11467371\n",
      " 0.34966952 0.67429084 0.7824651  0.28877679 0.18445645 0.36916986\n",
      " 0.46272501 0.57545167 0.54891902 0.45434096 0.56204492 0.46029493\n",
      " 0.43625095 0.53163159 0.43744856 0.5572089  0.4897806  0.53468859\n",
      " 0.71093971 0.66478038 0.73194355 0.40772963 0.42867962 0.42590669\n",
      " 0.37366283 0.52411991 0.44125065 0.44363657 0.51310587 0.48755202\n",
      " 0.4184584  0.50142521 0.55682808 0.74400353 0.82641906 0.7675246\n",
      " 0.80148619 0.32246462 0.86083502 0.26796538 0.52622569 0.50268573\n",
      " 0.05067618 0.52464205 0.42642769 0.48651776 0.34060329 0.68466246\n",
      " 0.79340112 0.6336056  0.75022036 0.77739263 0.7132706  0.52005017\n",
      " 0.5267905  0.73621511 0.69470036 0.71402359 0.74400228 0.73433465\n",
      " 0.58386934 0.58195788 0.58345741 0.74145311 0.97491294 0.95895493\n",
      " 0.9002701  0.84119177 0.84233063 0.86302507 0.79467243 0.61756337\n",
      " 0.52402902 0.46687859 0.69568026 0.75000006 0.49922782 0.49441078\n",
      " 0.4143402  0.41908136 0.5910089  0.50989437 0.33293703 0.44246823\n",
      " 0.3295463  0.47999129 0.15695311 0.51861078 0.02119974 0.77177674\n",
      " 0.39726627 0.47024295 0.37307006 0.60171854 0.40285069 0.1796115\n",
      " 0.42879513 0.27520588 0.12906523 0.34522513 0.51198518 0.44753182\n",
      " 0.47501895 0.41249719 0.47630724 0.60145319]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 19 [0/54 (0%)]\tTrain Loss: 0.074507\n",
      "Train Epoch: 19 [8/54 (15%)]\tTrain Loss: 0.069769\n",
      "Train Epoch: 19 [16/54 (30%)]\tTrain Loss: 0.094177\n",
      "Train Epoch: 19 [24/54 (44%)]\tTrain Loss: 0.076399\n",
      "Train Epoch: 19 [32/54 (59%)]\tTrain Loss: 0.079973\n",
      "Train Epoch: 19 [40/54 (74%)]\tTrain Loss: 0.095678\n",
      "Train Epoch: 19 [48/54 (89%)]\tTrain Loss: 0.080168\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.85007566 0.94466019 0.92431259 0.81833732 0.76487929 0.85845381\n",
      " 0.88933408 0.86350906 0.64910007 0.13479598 0.3793577  0.31082052\n",
      " 0.56296712 0.81146801 0.88233095 0.53925705 0.53277415 0.52859807\n",
      " 0.45329729 0.47109187 0.53474116 0.54303306 0.57747704 0.39217746\n",
      " 0.59920317 0.56401289 0.47393054 0.54413122 0.563133   0.61555219\n",
      " 0.82743639 0.72943467 0.69312978 0.76767075 0.88564134 0.73082078\n",
      " 0.50253373 0.54027849 0.69694418 0.87400162 0.90071428 0.90481788\n",
      " 0.51124591 0.6997149  0.79161119 0.93203068 0.98660362 0.97735006\n",
      " 0.98300385 0.55152583 0.98244125 0.46662945 0.68786103 0.78750432\n",
      " 0.70076007 0.57421559 0.73104519 0.54107213 0.36374545 0.90877753\n",
      " 0.94417447 0.79886067 0.92370027 0.9505856  0.85660303 0.78313297\n",
      " 0.80492985 0.83442932 0.87490046 0.87531441 0.9318409  0.9279837\n",
      " 0.76430136 0.59788305 0.63689059 0.89122641 0.98720491 0.97287422\n",
      " 0.98310709 0.95686233 0.97182995 0.97318518 0.94944364 0.73963976\n",
      " 0.71703172 0.7159183  0.88064408 0.89891553 0.44730905 0.62131715\n",
      " 0.75200254 0.46613878 0.69279391 0.78083289 0.68146837 0.56943446\n",
      " 0.40473649 0.81020075 0.67513597 0.95731002 0.78068495 0.95455909\n",
      " 0.55399519 0.67756844 0.60265535 0.91926527 0.64246023 0.59625548\n",
      " 0.97993225 0.5221144  0.7961809  0.87977576 0.98508102 0.91629517\n",
      " 0.71428066 0.63038665 0.80635518 0.68422973]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 [0/54 (0%)]\tTrain Loss: 0.078907\n",
      "Train Epoch: 20 [8/54 (15%)]\tTrain Loss: 0.068854\n",
      "Train Epoch: 20 [16/54 (30%)]\tTrain Loss: 0.106400\n",
      "Train Epoch: 20 [24/54 (44%)]\tTrain Loss: 0.093773\n",
      "Train Epoch: 20 [32/54 (59%)]\tTrain Loss: 0.115764\n",
      "Train Epoch: 20 [40/54 (74%)]\tTrain Loss: 0.088996\n",
      "Train Epoch: 20 [48/54 (89%)]\tTrain Loss: 0.050585\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.54272825 1.         1.         1.         0.99926454 0.73853332\n",
      " 1.         1.         0.99497426 0.28174174 0.9549976  0.83618665\n",
      " 0.99991834 1.         1.         1.         1.         0.96251225\n",
      " 0.99253803 0.99670106 0.99997604 0.8892265  0.99427116 0.99855226\n",
      " 0.99616086 0.9998318  0.83430237 0.99999964 0.98164111 0.99991679\n",
      " 0.99923372 0.83380008 0.9188658  0.99991655 0.99996936 0.9955278\n",
      " 0.99931836 0.99999046 0.99860448 0.72116148 0.82192367 0.96967626\n",
      " 0.99993765 0.99998951 0.9999882  0.66274148 0.98064065 0.9751026\n",
      " 0.95767438 0.20947945 0.91862935 0.99999988 1.         1.\n",
      " 0.30703461 1.         0.99120975 1.         1.         1.\n",
      " 0.70363384 0.57692564 0.65711892 0.6085853  1.         1.\n",
      " 1.         0.94676042 1.         0.58515811 0.79212189 0.75159776\n",
      " 0.98412353 0.71502233 0.99873811 0.9660362  1.         1.\n",
      " 1.         0.99820256 0.99915028 0.99999571 0.94387025 0.82108665\n",
      " 0.48081678 0.42089888 0.91388279 0.87222219 1.         0.99996388\n",
      " 0.99999452 0.9862873  0.99995017 1.         0.99997473 1.\n",
      " 1.         0.86283451 0.98919243 0.99966288 0.89125782 0.70391768\n",
      " 0.99648702 0.99752289 0.77173144 0.93432665 0.43613756 0.81422347\n",
      " 0.98258334 0.93268406 0.992782   0.38746679 1.         0.36129543\n",
      " 0.99863333 0.98570907 1.         1.        ]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
      "vote_pred [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 51 TN= 18 FN= 7 FP= 42\n",
      "TP+FP 93\n",
      "precision 0.5483870967741935\n",
      "recall 0.8793103448275862\n",
      "F1 0.6754966887417218\n",
      "acc 0.5847457627118644\n",
      "AUCp 0.5896551724137931\n",
      "AUC 0.6459770114942529\n",
      "\n",
      " The epoch is 20, average recall: 0.8793, average precision: 0.5484,average F1: 0.6755, average accuracy: 0.5847, average AUC: 0.6460\n",
      "Train Epoch: 21 [0/54 (0%)]\tTrain Loss: 0.109677\n",
      "Train Epoch: 21 [8/54 (15%)]\tTrain Loss: 0.100028\n",
      "Train Epoch: 21 [16/54 (30%)]\tTrain Loss: 0.079659\n",
      "Train Epoch: 21 [24/54 (44%)]\tTrain Loss: 0.047005\n",
      "Train Epoch: 21 [32/54 (59%)]\tTrain Loss: 0.044183\n",
      "Train Epoch: 21 [40/54 (74%)]\tTrain Loss: 0.073860\n",
      "Train Epoch: 21 [48/54 (89%)]\tTrain Loss: 0.098311\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.48698241 0.27604276 0.45869806 0.4346666  0.33592579 0.58465135\n",
      " 0.28093603 0.38450831 0.46186638 0.25850961 0.39947039 0.29515666\n",
      " 0.25587261 0.35087752 0.36967188 0.34436098 0.08112831 0.36212945\n",
      " 0.30341253 0.4145852  0.41729537 0.40023676 0.46490502 0.43920118\n",
      " 0.37319979 0.4786737  0.47827286 0.47711766 0.35628143 0.30516103\n",
      " 0.63659167 0.61348301 0.425284   0.07523289 0.21209629 0.18274774\n",
      " 0.2900213  0.43302572 0.45208177 0.46051347 0.44195724 0.41920143\n",
      " 0.382835   0.1854891  0.43773594 0.56579143 0.67230737 0.66006345\n",
      " 0.7131511  0.42807645 0.74722403 0.22744407 0.5284161  0.20738925\n",
      " 0.44413087 0.26782447 0.56706798 0.00401034 0.30334532 0.49647987\n",
      " 0.67534566 0.59057242 0.62293816 0.62434113 0.64168787 0.17059943\n",
      " 0.19754399 0.55820882 0.20137833 0.6765793  0.60557473 0.59045362\n",
      " 0.54152346 0.55876976 0.54997087 0.64704067 0.55707502 0.74288219\n",
      " 0.63530165 0.6991905  0.62793571 0.67776167 0.69263619 0.72893029\n",
      " 0.46283615 0.57121283 0.57001168 0.61378771 0.32979274 0.43796605\n",
      " 0.40394467 0.33894506 0.31318134 0.23908171 0.15040182 0.03461358\n",
      " 0.20300616 0.4776125  0.4768692  0.49560997 0.38906562 0.65571749\n",
      " 0.43684593 0.58330441 0.47872022 0.54875833 0.58972347 0.34344682\n",
      " 0.37150419 0.36610448 0.21609388 0.40215167 0.00874931 0.39395711\n",
      " 0.49054328 0.47092181 0.53562742 0.42814651]\n",
      "predict [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "Train Epoch: 22 [0/54 (0%)]\tTrain Loss: 0.108240\n",
      "Train Epoch: 22 [8/54 (15%)]\tTrain Loss: 0.095996\n",
      "Train Epoch: 22 [16/54 (30%)]\tTrain Loss: 0.077485\n",
      "Train Epoch: 22 [24/54 (44%)]\tTrain Loss: 0.084567\n",
      "Train Epoch: 22 [32/54 (59%)]\tTrain Loss: 0.089415\n",
      "Train Epoch: 22 [40/54 (74%)]\tTrain Loss: 0.049513\n",
      "Train Epoch: 22 [48/54 (89%)]\tTrain Loss: 0.090825\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.91823667 0.99306387 0.96093786 0.54082632 0.46273103 0.87193322\n",
      " 0.91200703 0.75310612 0.23420952 0.15389806 0.3307206  0.17846975\n",
      " 0.31505609 0.81361264 0.86535126 0.30989972 0.29719308 0.37706944\n",
      " 0.40210375 0.54096395 0.50218052 0.53918093 0.60509789 0.4355576\n",
      " 0.54165751 0.57728791 0.39382601 0.5707947  0.52435625 0.59302986\n",
      " 0.64041752 0.79246545 0.74818796 0.30709219 0.28554776 0.35578969\n",
      " 0.35481766 0.50345111 0.611027   0.46667704 0.4646104  0.38838866\n",
      " 0.30562127 0.41734457 0.54570621 0.87571043 0.99920255 0.99935144\n",
      " 0.99958068 0.29068619 0.99956912 0.21210858 0.88675082 0.68655229\n",
      " 0.38658974 0.43642753 0.57785285 0.46027333 0.26880109 0.81095588\n",
      " 0.95706648 0.87283486 0.92433435 0.8715952  0.8757109  0.71411443\n",
      " 0.80862206 0.81736398 0.74621242 0.82329971 0.75600171 0.71404761\n",
      " 0.57977068 0.48110363 0.73513359 0.99562413 0.99947661 0.99964118\n",
      " 0.99735922 0.97245479 0.89502847 0.98851216 0.92013222 0.9986425\n",
      " 0.92002153 0.41098732 0.78251529 0.99251199 0.59078139 0.5166474\n",
      " 0.50194758 0.49286407 0.5067606  0.53235328 0.47499821 0.49489075\n",
      " 0.41573182 0.38325813 0.33928007 0.39352012 0.30290994 0.85249829\n",
      " 0.39595464 0.81072497 0.36742863 0.7321679  0.73496813 0.28267357\n",
      " 0.34606323 0.25818932 0.26013103 0.38751015 0.37849182 0.4460361\n",
      " 0.53475928 0.48829466 0.6826064  0.68886769]\n",
      "predict [1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 23 [0/54 (0%)]\tTrain Loss: 0.122564\n",
      "Train Epoch: 23 [8/54 (15%)]\tTrain Loss: 0.050632\n",
      "Train Epoch: 23 [16/54 (30%)]\tTrain Loss: 0.076345\n",
      "Train Epoch: 23 [24/54 (44%)]\tTrain Loss: 0.075755\n",
      "Train Epoch: 23 [32/54 (59%)]\tTrain Loss: 0.060962\n",
      "Train Epoch: 23 [40/54 (74%)]\tTrain Loss: 0.083662\n",
      "Train Epoch: 23 [48/54 (89%)]\tTrain Loss: 0.155416\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.40538496 0.64244246 0.64175093 0.58882982 0.57505226 0.52586555\n",
      " 0.63413244 0.56957841 0.69024926 0.82295281 0.66121459 0.73107785\n",
      " 0.65011346 0.59817702 0.59742582 0.45762953 0.49524996 0.32408091\n",
      " 0.474839   0.65200865 0.55605406 0.45496204 0.58671361 0.52076352\n",
      " 0.46507961 0.60029185 0.45716983 0.55163234 0.47614688 0.5317083\n",
      " 0.59835792 0.45828611 0.58740276 0.712053   0.74595463 0.61964029\n",
      " 0.60157692 0.48179963 0.27785045 0.34464344 0.36337841 0.37646493\n",
      " 0.68845713 0.47908968 0.53764248 0.61352342 0.64744925 0.56778914\n",
      " 0.62566137 0.27458647 0.5738126  0.80633301 0.61499584 0.69954115\n",
      " 0.64884156 0.60840559 0.63520372 0.66718167 0.78848165 0.54942006\n",
      " 0.4115136  0.40209544 0.41746846 0.4206304  0.64309049 0.40867707\n",
      " 0.31884572 0.51873672 0.66745573 0.48974332 0.22616562 0.22450228\n",
      " 0.54283273 0.57975578 0.53343296 0.41207188 0.64597094 0.64522898\n",
      " 0.56198996 0.33017462 0.44601208 0.517995   0.54424131 0.25167501\n",
      " 0.24444835 0.25599316 0.32656613 0.31284061 0.54731101 0.4431361\n",
      " 0.46833378 0.58092093 0.42721438 0.59460294 0.51975459 0.5761354\n",
      " 0.58163923 0.30236107 0.36534065 0.48420346 0.28311947 0.25407031\n",
      " 0.28650939 0.3771652  0.55987477 0.40069118 0.39363527 0.58331728\n",
      " 0.49374369 0.5762766  0.58750337 0.65002269 0.62833476 0.60519892\n",
      " 0.47801474 0.30549395 0.6320464  0.6139372 ]\n",
      "predict [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 24 [0/54 (0%)]\tTrain Loss: 0.081762\n",
      "Train Epoch: 24 [8/54 (15%)]\tTrain Loss: 0.086610\n",
      "Train Epoch: 24 [16/54 (30%)]\tTrain Loss: 0.059842\n",
      "Train Epoch: 24 [24/54 (44%)]\tTrain Loss: 0.097952\n",
      "Train Epoch: 24 [32/54 (59%)]\tTrain Loss: 0.098605\n",
      "Train Epoch: 24 [40/54 (74%)]\tTrain Loss: 0.084128\n",
      "Train Epoch: 24 [48/54 (89%)]\tTrain Loss: 0.078658\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.53556347 0.73008651 0.72651142 0.4808251  0.42584315 0.36491573\n",
      " 0.72410345 0.60015845 0.36729559 0.23193926 0.42276287 0.25963321\n",
      " 0.41610795 0.67704642 0.68129158 0.47015291 0.49202168 0.47147912\n",
      " 0.49271974 0.59992558 0.60429651 0.55363876 0.65373468 0.68201077\n",
      " 0.56871462 0.67944545 0.66862124 0.65079558 0.56380534 0.60830361\n",
      " 0.66500193 0.60434312 0.70522827 0.55202132 0.49197191 0.55175698\n",
      " 0.4972654  0.44568276 0.46039701 0.51601291 0.52339292 0.50688559\n",
      " 0.40061301 0.5177145  0.54630172 0.58036596 0.56148326 0.59933203\n",
      " 0.69497293 0.58701998 0.6699307  0.27648488 0.57559204 0.46061474\n",
      " 0.53822738 0.50701785 0.6238628  0.54752302 0.40412915 0.59909183\n",
      " 0.63769895 0.59976727 0.63433158 0.64639753 0.67100692 0.55560571\n",
      " 0.52511388 0.63966602 0.64258754 0.51201785 0.57582146 0.557886\n",
      " 0.59831727 0.60911727 0.58961147 0.61959392 0.73298287 0.73278844\n",
      " 0.67781436 0.62263602 0.64632511 0.66029227 0.64197421 0.57126045\n",
      " 0.57320529 0.55407536 0.59594017 0.60972971 0.62654221 0.56314027\n",
      " 0.6744675  0.47257462 0.56612837 0.67719811 0.61771762 0.62306464\n",
      " 0.50979149 0.53930408 0.51114911 0.57292283 0.50757712 0.58482414\n",
      " 0.49927223 0.46083522 0.49221396 0.55114818 0.58774352 0.4907186\n",
      " 0.54039967 0.44873115 0.49543262 0.63493544 0.62894827 0.56772339\n",
      " 0.58890355 0.57604384 0.64780772 0.64197588]\n",
      "predict [1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 25 [0/54 (0%)]\tTrain Loss: 0.074094\n",
      "Train Epoch: 25 [8/54 (15%)]\tTrain Loss: 0.092079\n",
      "Train Epoch: 25 [16/54 (30%)]\tTrain Loss: 0.077031\n",
      "Train Epoch: 25 [24/54 (44%)]\tTrain Loss: 0.044996\n",
      "Train Epoch: 25 [32/54 (59%)]\tTrain Loss: 0.099405\n",
      "Train Epoch: 25 [40/54 (74%)]\tTrain Loss: 0.117445\n",
      "Train Epoch: 25 [48/54 (89%)]\tTrain Loss: 0.093338\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.68720156 0.92149919 0.92990273 0.59015346 0.54021484 0.53423506\n",
      " 0.8896718  0.64237148 0.48739895 0.36321115 0.49358061 0.37046763\n",
      " 0.48255315 0.6829167  0.72962707 0.46492282 0.460244   0.44731358\n",
      " 0.49188781 0.63919628 0.62979901 0.52786487 0.6860075  0.6406101\n",
      " 0.49116126 0.62350458 0.53630376 0.67685521 0.54635513 0.69514817\n",
      " 0.76032346 0.76030731 0.88648772 0.48265421 0.52580422 0.43364662\n",
      " 0.50090373 0.52605188 0.5049628  0.49066135 0.51454055 0.47643748\n",
      " 0.46274298 0.53940654 0.59643656 0.74261469 0.75600106 0.77776295\n",
      " 0.80097365 0.53533137 0.75580394 0.40100423 0.65856141 0.59721255\n",
      " 0.57902485 0.56429529 0.78282028 0.64550006 0.41366112 0.75613147\n",
      " 0.84781671 0.77238846 0.82078528 0.81841654 0.78647524 0.6604178\n",
      " 0.60310936 0.67626137 0.7893101  0.60187799 0.73494095 0.71865124\n",
      " 0.71819031 0.75177675 0.63703966 0.82388592 0.92744792 0.92462415\n",
      " 0.85390967 0.8109324  0.84210831 0.84966135 0.81694627 0.78991032\n",
      " 0.60856211 0.75625288 0.71384555 0.78615278 0.59346581 0.4705632\n",
      " 0.53191757 0.62115937 0.65806627 0.67018598 0.57165956 0.59857297\n",
      " 0.52333361 0.53880507 0.60931265 0.54889035 0.41967988 0.80493444\n",
      " 0.54113954 0.70890796 0.5963133  0.72082537 0.69434458 0.46981251\n",
      " 0.50097072 0.50058222 0.45086914 0.50647891 0.53086382 0.52985281\n",
      " 0.52402151 0.44805223 0.59202796 0.5851559 ]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 26 [0/54 (0%)]\tTrain Loss: 0.059751\n",
      "Train Epoch: 26 [8/54 (15%)]\tTrain Loss: 0.073054\n",
      "Train Epoch: 26 [16/54 (30%)]\tTrain Loss: 0.119645\n",
      "Train Epoch: 26 [24/54 (44%)]\tTrain Loss: 0.087610\n",
      "Train Epoch: 26 [32/54 (59%)]\tTrain Loss: 0.078573\n",
      "Train Epoch: 26 [40/54 (74%)]\tTrain Loss: 0.083556\n",
      "Train Epoch: 26 [48/54 (89%)]\tTrain Loss: 0.063989\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.65837401 0.80862659 0.82298636 0.79789776 0.81010407 0.56930268\n",
      " 0.85748106 0.86448467 0.72582459 0.47918513 0.70506608 0.57425255\n",
      " 0.86522561 0.97452229 0.95825636 0.94890827 0.88667518 0.60826617\n",
      " 0.61768985 0.82480478 0.91106457 0.7150858  0.8243078  0.89040881\n",
      " 0.83981657 0.90474898 0.9186278  0.95211661 0.79596293 0.88969213\n",
      " 0.97003585 0.86403567 0.94190747 0.89819038 0.95250279 0.88012058\n",
      " 0.89352232 0.79387796 0.91073906 0.93816763 0.9292199  0.94472754\n",
      " 0.62564623 0.80034655 0.74752796 0.91612202 0.83347297 0.89335066\n",
      " 0.92313826 0.65494007 0.85502976 0.60993439 0.81974572 0.77733117\n",
      " 0.70796448 0.76788586 0.88278568 0.91600293 0.5715785  0.87758869\n",
      " 0.88251448 0.80162746 0.88563156 0.89413649 0.96966141 0.96796328\n",
      " 0.93857145 0.94521165 0.96051645 0.9020232  0.79420865 0.79695255\n",
      " 0.92225212 0.92538279 0.96209276 0.84327519 0.57226551 0.56652224\n",
      " 0.90677714 0.89426059 0.94495082 0.9512974  0.90386426 0.81938618\n",
      " 0.85720491 0.83219433 0.84147477 0.84981811 0.94846183 0.95964795\n",
      " 0.93205315 0.73926497 0.8367939  0.96410733 0.92144197 0.93540311\n",
      " 0.74366492 0.8433274  0.89904994 0.97259998 0.88916177 0.88032645\n",
      " 0.82386196 0.87486619 0.8540858  0.89138788 0.82497263 0.94844884\n",
      " 0.97229058 0.86563683 0.93602812 0.96358782 0.96141362 0.93772\n",
      " 0.89672381 0.77500576 0.7345323  0.88456428]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 27 [0/54 (0%)]\tTrain Loss: 0.118145\n",
      "Train Epoch: 27 [8/54 (15%)]\tTrain Loss: 0.089661\n",
      "Train Epoch: 27 [16/54 (30%)]\tTrain Loss: 0.094957\n",
      "Train Epoch: 27 [24/54 (44%)]\tTrain Loss: 0.091854\n",
      "Train Epoch: 27 [32/54 (59%)]\tTrain Loss: 0.070085\n",
      "Train Epoch: 27 [40/54 (74%)]\tTrain Loss: 0.048863\n",
      "Train Epoch: 27 [48/54 (89%)]\tTrain Loss: 0.069357\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [8.53587687e-01 9.42690194e-01 9.15289283e-01 7.75497735e-01\n",
      " 7.07215130e-01 7.08347261e-01 7.38135755e-01 8.32830787e-01\n",
      " 6.80210829e-01 1.53205469e-01 2.18008727e-01 2.26996154e-01\n",
      " 5.03553033e-01 7.62935400e-01 8.60909224e-01 6.62112594e-01\n",
      " 5.66339672e-01 4.19474036e-01 3.57090175e-01 3.96857888e-01\n",
      " 4.36631441e-01 4.49165553e-01 6.82550430e-01 5.93271434e-01\n",
      " 4.11797374e-01 6.48689091e-01 6.07522249e-01 6.47309959e-01\n",
      " 5.81138611e-01 6.84632540e-01 8.15623462e-01 7.58963883e-01\n",
      " 7.93942392e-01 3.97868812e-01 3.98850441e-01 4.35274243e-01\n",
      " 3.74940634e-01 6.29371583e-01 6.57175899e-01 6.05547190e-01\n",
      " 6.38887107e-01 5.65933108e-01 4.86751229e-01 6.89382493e-01\n",
      " 7.52455950e-01 8.30049932e-01 9.26688135e-01 9.36036050e-01\n",
      " 9.34352279e-01 4.77049738e-01 9.01313663e-01 3.13885272e-01\n",
      " 6.67297781e-01 5.79188347e-01 1.59473345e-01 5.20206809e-01\n",
      " 2.17512295e-01 5.72348118e-01 2.80660897e-01 8.96663785e-01\n",
      " 8.98349702e-01 7.90882230e-01 8.86727810e-01 8.91805530e-01\n",
      " 8.17952931e-01 7.38876343e-01 7.43461609e-01 6.69918776e-01\n",
      " 7.68315613e-01 6.13099635e-01 7.89017439e-01 7.81509578e-01\n",
      " 7.82117844e-01 7.64552653e-01 7.96317697e-01 8.76767933e-01\n",
      " 9.70029116e-01 9.64411914e-01 9.50111985e-01 9.39443946e-01\n",
      " 9.33881581e-01 9.45259511e-01 9.02068973e-01 7.38449991e-01\n",
      " 7.62686372e-01 7.58560538e-01 4.23787594e-01 6.03822589e-01\n",
      " 5.37585557e-01 4.01852101e-01 6.01020992e-01 6.23760223e-01\n",
      " 7.52338886e-01 6.21727407e-01 6.39613450e-01 4.93045837e-01\n",
      " 3.65255803e-01 6.71318531e-01 4.85029250e-01 5.53357780e-01\n",
      " 4.72632140e-01 9.25263166e-01 5.31162918e-01 6.22207642e-01\n",
      " 5.72172225e-01 7.05874026e-01 5.72812319e-01 1.30171403e-01\n",
      " 4.04451579e-01 4.55211120e-05 4.30571944e-01 4.28099364e-01\n",
      " 5.24651229e-01 4.58717257e-01 6.47706211e-01 6.49505496e-01\n",
      " 5.11099041e-01 5.77072322e-01]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.]\n",
      "Train Epoch: 28 [0/54 (0%)]\tTrain Loss: 0.113513\n",
      "Train Epoch: 28 [8/54 (15%)]\tTrain Loss: 0.060828\n",
      "Train Epoch: 28 [16/54 (30%)]\tTrain Loss: 0.086494\n",
      "Train Epoch: 28 [24/54 (44%)]\tTrain Loss: 0.101360\n",
      "Train Epoch: 28 [32/54 (59%)]\tTrain Loss: 0.096199\n",
      "Train Epoch: 28 [40/54 (74%)]\tTrain Loss: 0.072247\n",
      "Train Epoch: 28 [48/54 (89%)]\tTrain Loss: 0.073858\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.00000000e+00 1.26324883e-02 1.03382561e-02 9.06466901e-01\n",
      " 9.97749627e-01 9.99996901e-01 1.71923228e-02 9.99990582e-01\n",
      " 1.00000000e+00 9.67113525e-02 4.46121424e-01 2.15068310e-01\n",
      " 4.86345292e-04 2.15548343e-11 3.28434762e-13 1.61247316e-03\n",
      " 1.87817344e-03 1.05582075e-02 2.70377457e-01 2.45576193e-05\n",
      " 1.03287615e-01 1.11956626e-06 8.44895002e-03 6.65705681e-01\n",
      " 8.15604108e-06 8.35549414e-01 9.20281589e-01 3.14062268e-01\n",
      " 2.86047367e-07 1.34497498e-07 9.99999881e-01 9.99998569e-01\n",
      " 2.06300115e-06 4.34242010e-01 2.98590302e-01 1.21289417e-02\n",
      " 1.20518208e-02 2.34533614e-03 7.80761540e-01 5.53885102e-01\n",
      " 6.30583286e-01 4.63283628e-01 9.38996964e-05 4.37635754e-05\n",
      " 3.19095943e-06 1.00000000e+00 1.10767834e-01 3.30176950e-03\n",
      " 8.65759254e-01 7.80259252e-01 9.99232173e-01 8.82034659e-01\n",
      " 4.18797977e-19 7.87503041e-09 4.80327278e-01 1.47471452e-14\n",
      " 1.83247728e-04 7.13065037e-06 2.55359914e-15 1.38807073e-01\n",
      " 9.89810824e-01 1.90041661e-01 7.57528067e-01 9.99468863e-01\n",
      " 1.55342743e-08 9.34017181e-01 9.64960814e-01 2.17595607e-05\n",
      " 2.10549477e-02 9.99998450e-01 9.98158038e-01 9.84582424e-01\n",
      " 1.00000000e+00 9.99726951e-01 9.87560928e-01 1.26067254e-08\n",
      " 2.53243685e-01 3.14533059e-03 1.24528970e-05 1.00000000e+00\n",
      " 9.99998450e-01 9.99996543e-01 9.96714234e-01 4.51737680e-02\n",
      " 9.04954910e-01 1.00000000e+00 1.83182046e-01 1.06901877e-01\n",
      " 4.80107119e-05 4.09323305e-01 8.57503772e-01 9.96132135e-01\n",
      " 9.98504996e-01 7.48000383e-01 7.30525792e-01 6.67932451e-01\n",
      " 1.62878929e-11 8.70322049e-01 7.30017781e-01 4.71749157e-01\n",
      " 5.05760312e-02 9.99999881e-01 1.57681711e-07 6.66929483e-01\n",
      " 9.82732892e-01 9.97673690e-01 9.99972343e-01 3.38274240e-01\n",
      " 3.68399054e-01 7.29747474e-01 2.93699324e-01 4.33567822e-01\n",
      " 5.05064726e-01 4.08306211e-01 9.69822764e-01 7.62630634e-06\n",
      " 2.86729095e-11 2.77589783e-02]\n",
      "predict [1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "Train Epoch: 29 [0/54 (0%)]\tTrain Loss: 0.071697\n",
      "Train Epoch: 29 [8/54 (15%)]\tTrain Loss: 0.068819\n",
      "Train Epoch: 29 [16/54 (30%)]\tTrain Loss: 0.078123\n",
      "Train Epoch: 29 [24/54 (44%)]\tTrain Loss: 0.113449\n",
      "Train Epoch: 29 [32/54 (59%)]\tTrain Loss: 0.064834\n",
      "Train Epoch: 29 [40/54 (74%)]\tTrain Loss: 0.072740\n",
      "Train Epoch: 29 [48/54 (89%)]\tTrain Loss: 0.054246\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.67062071e-02 7.59994655e-22 1.12578686e-12 3.56208563e-01\n",
      " 2.23395392e-01 2.71266490e-01 3.73389229e-23 4.29920584e-01\n",
      " 1.92077264e-01 2.31092677e-01 1.21008024e-01 3.61969382e-01\n",
      " 2.08048701e-01 2.90865242e-01 6.23340070e-01 1.03918612e-01\n",
      " 8.12485516e-02 1.16235942e-01 1.58411354e-01 3.17397207e-01\n",
      " 3.62858087e-01 2.34391168e-01 5.97787261e-01 8.57157350e-01\n",
      " 2.17923045e-01 6.07062459e-01 7.16360569e-01 4.17615563e-01\n",
      " 2.65862644e-01 4.52519476e-01 6.81380928e-01 6.58302009e-01\n",
      " 8.30489635e-01 2.58986473e-01 1.19168721e-01 1.09913565e-01\n",
      " 1.27265036e-01 2.96393812e-01 1.55356973e-01 1.56286955e-01\n",
      " 1.50712550e-01 1.45350248e-01 1.71121731e-01 2.75923848e-01\n",
      " 3.98479491e-01 6.20992362e-01 6.73851569e-17 1.18309136e-19\n",
      " 1.27122722e-11 5.48481107e-01 1.80328896e-09 1.06415965e-01\n",
      " 3.17309529e-01 2.71271378e-01 3.10052752e-01 1.98206604e-01\n",
      " 6.08270109e-01 2.01598004e-01 2.47554481e-01 6.57431036e-02\n",
      " 5.37295818e-01 3.70427847e-01 5.94563246e-01 6.00479662e-01\n",
      " 7.06571639e-01 4.29590791e-01 2.72408932e-01 5.82637250e-01\n",
      " 4.56316471e-01 3.25838923e-01 3.14611197e-01 2.70381361e-01\n",
      " 4.56617236e-01 4.32447106e-01 4.85302776e-01 7.57094920e-01\n",
      " 1.49898578e-38 0.00000000e+00 2.76451738e-05 7.65585542e-01\n",
      " 7.12342203e-01 7.26448238e-01 7.38719583e-01 3.63625824e-01\n",
      " 4.15837914e-01 3.29039097e-01 6.54299140e-01 5.62911332e-01\n",
      " 2.63951898e-01 1.15385957e-01 6.55098736e-01 2.54802734e-01\n",
      " 3.62600118e-01 6.41548932e-01 3.30265075e-01 3.86665463e-01\n",
      " 3.54106724e-01 1.86955541e-01 1.53371572e-01 2.89417505e-01\n",
      " 1.10177927e-01 7.05718338e-01 1.09492660e-01 2.29460493e-01\n",
      " 3.24802250e-01 2.36468226e-01 2.86473781e-01 1.16471879e-01\n",
      " 1.16266079e-01 1.61571890e-01 1.02297433e-01 5.87501824e-01\n",
      " 2.76151150e-01 3.00103039e-01 1.89259604e-01 1.82647690e-01\n",
      " 1.85582161e-01 4.14823294e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 30 [0/54 (0%)]\tTrain Loss: 0.085793\n",
      "Train Epoch: 30 [8/54 (15%)]\tTrain Loss: 0.082417\n",
      "Train Epoch: 30 [16/54 (30%)]\tTrain Loss: 0.070434\n",
      "Train Epoch: 30 [24/54 (44%)]\tTrain Loss: 0.100938\n",
      "Train Epoch: 30 [32/54 (59%)]\tTrain Loss: 0.081627\n",
      "Train Epoch: 30 [40/54 (74%)]\tTrain Loss: 0.061732\n",
      "Train Epoch: 30 [48/54 (89%)]\tTrain Loss: 0.079095\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.75429201 0.80856407 0.795699   0.7755881  0.74679554 0.7170161\n",
      " 0.79384542 0.79461128 0.72135812 0.43030357 0.70984632 0.52809316\n",
      " 0.71696746 0.8310653  0.83140409 0.76274872 0.77474964 0.71301633\n",
      " 0.73210758 0.71400517 0.65514553 0.74629033 0.76452559 0.68947917\n",
      " 0.76186931 0.75152004 0.73368639 0.77872556 0.76241714 0.7479254\n",
      " 0.81140441 0.7833569  0.74513727 0.74994165 0.75204021 0.76937175\n",
      " 0.75973177 0.74666786 0.76139426 0.77656513 0.77623266 0.77073741\n",
      " 0.66909826 0.78356433 0.7843883  0.76074922 0.77848488 0.78132594\n",
      " 0.77895653 0.71130514 0.77497512 0.61235142 0.78974342 0.76740712\n",
      " 0.60770142 0.73260462 0.78074962 0.76635677 0.58154941 0.80487752\n",
      " 0.76010174 0.75378525 0.79928172 0.78998113 0.79572201 0.81655568\n",
      " 0.79656607 0.80351329 0.77963096 0.76194358 0.75258011 0.75215876\n",
      " 0.80882275 0.79302573 0.80311751 0.79285842 0.81438363 0.81517684\n",
      " 0.82661426 0.80034584 0.80862075 0.80283409 0.74550045 0.78548139\n",
      " 0.78557992 0.76603115 0.65549499 0.56303114 0.79119682 0.77773649\n",
      " 0.74989003 0.75103647 0.78004014 0.79153657 0.8027218  0.79633588\n",
      " 0.73963016 0.72999221 0.6954717  0.77493417 0.73739213 0.80145025\n",
      " 0.76553601 0.70720243 0.73279577 0.67949533 0.70606953 0.75163633\n",
      " 0.75759947 0.70406026 0.74894178 0.77916443 0.78995079 0.774921\n",
      " 0.7795307  0.75312632 0.7820999  0.81333345]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 46 TN= 23 FN= 12 FP= 37\n",
      "TP+FP 83\n",
      "precision 0.5542168674698795\n",
      "recall 0.7931034482758621\n",
      "F1 0.652482269503546\n",
      "acc 0.5847457627118644\n",
      "AUCp 0.5882183908045977\n",
      "AUC 0.6770114942528737\n",
      "\n",
      " The epoch is 30, average recall: 0.7931, average precision: 0.5542,average F1: 0.6525, average accuracy: 0.5847, average AUC: 0.6770\n",
      "Train Epoch: 31 [0/54 (0%)]\tTrain Loss: 0.077773\n",
      "Train Epoch: 31 [8/54 (15%)]\tTrain Loss: 0.092630\n",
      "Train Epoch: 31 [16/54 (30%)]\tTrain Loss: 0.061204\n",
      "Train Epoch: 31 [24/54 (44%)]\tTrain Loss: 0.110526\n",
      "Train Epoch: 31 [32/54 (59%)]\tTrain Loss: 0.090275\n",
      "Train Epoch: 31 [40/54 (74%)]\tTrain Loss: 0.072423\n",
      "Train Epoch: 31 [48/54 (89%)]\tTrain Loss: 0.054302\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.96669102 0.88237756 0.86221677 0.22654967 0.202999   0.98317647\n",
      " 0.85640794 0.49445134 0.11587483 0.99998522 0.18834321 0.99999273\n",
      " 0.34473479 0.75580728 0.81752783 0.38581505 0.36153793 0.15694705\n",
      " 0.99883562 0.99974233 0.9999882  0.37511641 0.99910235 0.99999928\n",
      " 0.54445505 0.99974662 1.         0.77702969 0.62099445 0.54664654\n",
      " 0.83611649 0.76021838 0.86339146 0.67425859 0.42842078 0.30330157\n",
      " 0.25103846 0.27700061 0.39915246 0.64444155 0.64396989 0.59763813\n",
      " 0.12793262 0.30682415 0.45057392 0.85048962 0.74623388 0.66068166\n",
      " 0.76695949 1.         0.9372738  0.03774465 0.99937809 0.29554915\n",
      " 0.99999988 0.97701949 0.87066388 0.89152128 0.81442624 0.91240305\n",
      " 0.77060258 0.8737002  0.81631023 0.83744091 0.88188064 0.74207121\n",
      " 0.67656606 0.81528878 0.74391872 0.99496502 0.3657324  0.32775861\n",
      " 0.61188024 0.73875713 0.81844652 0.77797014 0.94041848 0.94867343\n",
      " 0.8493278  0.85858154 0.82527238 0.90144759 0.99389392 0.45398813\n",
      " 0.3463884  0.61088157 0.99893707 0.99891961 0.6091854  0.58811271\n",
      " 0.59318006 0.47573033 0.32259268 0.99765873 0.99993861 0.99987447\n",
      " 0.99999619 0.40003097 0.84815431 0.61375403 0.31538656 0.75175142\n",
      " 0.48027137 0.16849001 0.85195041 0.99992776 0.99993384 0.33557183\n",
      " 0.47892135 0.99290532 0.37919155 0.30625722 0.9262411  0.57962877\n",
      " 0.63958633 0.57785481 0.75689995 0.75549483]\n",
      "predict [1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 32 [0/54 (0%)]\tTrain Loss: 0.060259\n",
      "Train Epoch: 32 [8/54 (15%)]\tTrain Loss: 0.104478\n",
      "Train Epoch: 32 [16/54 (30%)]\tTrain Loss: 0.082271\n",
      "Train Epoch: 32 [24/54 (44%)]\tTrain Loss: 0.064311\n",
      "Train Epoch: 32 [32/54 (59%)]\tTrain Loss: 0.096363\n",
      "Train Epoch: 32 [40/54 (74%)]\tTrain Loss: 0.079587\n",
      "Train Epoch: 32 [48/54 (89%)]\tTrain Loss: 0.089305\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.86522728 0.95989472 0.93109435 0.40209594 0.35254601 0.32520491\n",
      " 0.94000286 0.666403   0.12778935 0.12702531 0.19861358 0.08030173\n",
      " 0.16690563 0.74917024 0.83370245 0.55296153 0.43798605 0.29813686\n",
      " 0.45099592 0.65251923 0.67238718 0.519041   0.73066318 0.75879991\n",
      " 0.59674102 0.72077733 0.77058131 0.70967001 0.59531897 0.58310372\n",
      " 0.88784045 0.86056161 0.87178618 0.38850915 0.35747907 0.30346\n",
      " 0.2580359  0.43607321 0.56520736 0.7045067  0.69862503 0.67721957\n",
      " 0.25746536 0.40884539 0.55681813 0.62811577 0.93578482 0.91254824\n",
      " 0.94423127 0.55911183 0.95309818 0.09975882 0.49063733 0.41672677\n",
      " 0.40949124 0.22991253 0.78978002 0.44205117 0.28291923 0.60346687\n",
      " 0.82278126 0.75056183 0.89209527 0.90050459 0.81310385 0.79276013\n",
      " 0.76913244 0.82845104 0.81402135 0.65685314 0.71439821 0.70274621\n",
      " 0.74828053 0.74574691 0.78291649 0.83870959 0.9931491  0.99378139\n",
      " 0.94158733 0.93691278 0.93688691 0.94637859 0.93876785 0.6493479\n",
      " 0.53899622 0.7334013  0.79409242 0.82243204 0.68856734 0.64037776\n",
      " 0.72141874 0.4056401  0.43538791 0.76959825 0.75372493 0.71291614\n",
      " 0.47659454 0.55511618 0.49018273 0.72281736 0.58242863 0.91876376\n",
      " 0.61203796 0.57830656 0.56769294 0.65650982 0.5432604  0.40427279\n",
      " 0.71358931 0.26964647 0.53379428 0.45534128 0.67907047 0.62025809\n",
      " 0.72972691 0.64711922 0.76777893 0.7742511 ]\n",
      "predict [1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 33 [0/54 (0%)]\tTrain Loss: 0.098437\n",
      "Train Epoch: 33 [8/54 (15%)]\tTrain Loss: 0.129646\n",
      "Train Epoch: 33 [16/54 (30%)]\tTrain Loss: 0.081873\n",
      "Train Epoch: 33 [24/54 (44%)]\tTrain Loss: 0.090786\n",
      "Train Epoch: 33 [32/54 (59%)]\tTrain Loss: 0.099500\n",
      "Train Epoch: 33 [40/54 (74%)]\tTrain Loss: 0.095316\n",
      "Train Epoch: 33 [48/54 (89%)]\tTrain Loss: 0.061235\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.58151293 0.86518663 0.85508919 0.30654272 0.17783527 0.18539603\n",
      " 0.77262044 0.51874608 0.12602548 0.04576751 0.28409818 0.05410649\n",
      " 0.16699749 0.57687271 0.68413115 0.18206131 0.17063393 0.1557304\n",
      " 0.21134292 0.33922628 0.38354817 0.31113318 0.35659435 0.19822076\n",
      " 0.34593874 0.31694555 0.18563412 0.43837488 0.29288137 0.40078214\n",
      " 0.83078688 0.65882462 0.71685874 0.18135501 0.24629213 0.21540534\n",
      " 0.30164343 0.28253132 0.31434351 0.40780899 0.47085819 0.37809965\n",
      " 0.18981765 0.29345807 0.34858385 0.54954594 0.79649395 0.70813191\n",
      " 0.81704235 0.05997633 0.81201059 0.08710291 0.30556387 0.26010826\n",
      " 0.26452711 0.22338963 0.49192572 0.30841243 0.16681316 0.51288211\n",
      " 0.53515458 0.37112477 0.6293667  0.65420252 0.60610676 0.54562795\n",
      " 0.50163096 0.69826496 0.54345471 0.25440848 0.21407118 0.19711296\n",
      " 0.6225121  0.48806033 0.67597789 0.61966336 0.90005285 0.90419453\n",
      " 0.77101427 0.83106387 0.82055438 0.85463399 0.59504712 0.25219184\n",
      " 0.30522132 0.47020897 0.48739764 0.67742962 0.37169096 0.30105418\n",
      " 0.2608386  0.18769294 0.308348   0.54025781 0.29913938 0.4489556\n",
      " 0.22679488 0.27198741 0.21124534 0.28625333 0.15807515 0.65961003\n",
      " 0.24718061 0.36772299 0.28044456 0.32265446 0.25190243 0.45291823\n",
      " 0.52892071 0.29343683 0.3741549  0.12663968 0.36712459 0.50986171\n",
      " 0.21790044 0.2211052  0.47191277 0.5555861 ]\n",
      "predict [1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "Train Epoch: 34 [0/54 (0%)]\tTrain Loss: 0.076705\n",
      "Train Epoch: 34 [8/54 (15%)]\tTrain Loss: 0.108070\n",
      "Train Epoch: 34 [16/54 (30%)]\tTrain Loss: 0.088144\n",
      "Train Epoch: 34 [24/54 (44%)]\tTrain Loss: 0.053046\n",
      "Train Epoch: 34 [32/54 (59%)]\tTrain Loss: 0.113045\n",
      "Train Epoch: 34 [40/54 (74%)]\tTrain Loss: 0.073430\n",
      "Train Epoch: 34 [48/54 (89%)]\tTrain Loss: 0.091960\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.91429436 0.69183141 0.75561178 0.99927288 0.99034661 0.99999976\n",
      " 0.67146498 0.99747723 1.         1.         0.99971825 1.\n",
      " 1.         0.73006916 0.58392501 0.91989559 1.         0.73567724\n",
      " 0.80130571 0.80544066 0.83401185 0.75409651 0.8003701  0.71867371\n",
      " 0.751912   0.82305419 0.99969018 0.87141305 0.73969102 0.78629392\n",
      " 0.88404226 0.82816237 0.7969594  1.         1.         1.\n",
      " 0.99789596 0.92720562 0.99865401 0.89451313 0.75850451 0.97443771\n",
      " 0.88066167 0.65178919 0.75375921 0.99999487 0.92961347 0.93805873\n",
      " 0.76598167 1.         0.86724567 0.99397957 0.97856301 1.\n",
      " 0.99662858 0.99999976 0.84933144 0.99340677 1.         0.72200054\n",
      " 0.65226465 0.67088979 0.63929248 0.54332328 0.87083727 0.85218757\n",
      " 0.79572618 0.87526643 0.90740335 1.         0.76033801 0.76119733\n",
      " 0.98215646 0.99999964 0.83638585 0.46307859 0.72752213 0.75650066\n",
      " 0.74494284 0.81176335 0.81215155 0.80507678 0.75831938 0.63001573\n",
      " 0.6314438  0.76124793 0.66859478 0.70615536 0.83782768 0.82338893\n",
      " 0.9994784  1.         0.8488887  0.93913895 0.99999595 0.99999034\n",
      " 0.78925061 0.99999821 0.8209818  0.83765668 0.99697173 0.80865014\n",
      " 0.58534515 0.81603175 0.93176037 0.76172763 0.70092839 0.87352204\n",
      " 0.99541533 0.92084432 1.         0.96778977 1.         1.\n",
      " 0.7066564  0.530536   0.56475955 0.55522418]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 35 [0/54 (0%)]\tTrain Loss: 0.107374\n",
      "Train Epoch: 35 [8/54 (15%)]\tTrain Loss: 0.064896\n",
      "Train Epoch: 35 [16/54 (30%)]\tTrain Loss: 0.069155\n",
      "Train Epoch: 35 [24/54 (44%)]\tTrain Loss: 0.100535\n",
      "Train Epoch: 35 [32/54 (59%)]\tTrain Loss: 0.048500\n",
      "Train Epoch: 35 [40/54 (74%)]\tTrain Loss: 0.091603\n",
      "Train Epoch: 35 [48/54 (89%)]\tTrain Loss: 0.107606\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.88212073 0.9330706  0.85508621 0.67666131 0.60294348 0.67314118\n",
      " 0.87524664 0.79997367 0.54081851 0.37363786 0.49645963 0.4544985\n",
      " 0.47123429 0.61079806 0.71249276 0.51961935 0.50089258 0.43115556\n",
      " 0.4688215  0.50941217 0.51533061 0.45304251 0.52745688 0.54557407\n",
      " 0.47029635 0.54090255 0.58622754 0.53815407 0.48039135 0.49747398\n",
      " 0.77747416 0.68121737 0.61622405 0.50537348 0.50313514 0.4885093\n",
      " 0.48094898 0.58823031 0.50188828 0.49362934 0.48165056 0.47137564\n",
      " 0.51792437 0.54867458 0.58611506 0.68016928 0.95877117 0.9386245\n",
      " 0.90966225 0.55060887 0.94787771 0.54192466 0.60376579 0.70554239\n",
      " 0.47487959 0.56461561 0.64784628 0.56225073 0.58853698 0.78179586\n",
      " 0.77611977 0.6647712  0.83776724 0.84756291 0.69142979 0.66541082\n",
      " 0.71311063 0.70457578 0.73074698 0.7345289  0.77520037 0.73457038\n",
      " 0.63233012 0.6013276  0.61263853 0.74016255 0.95759493 0.97640777\n",
      " 0.91980058 0.86126089 0.85986948 0.85898089 0.85447818 0.54777551\n",
      " 0.47949323 0.59549427 0.61452043 0.68121827 0.53589904 0.4845303\n",
      " 0.5443669  0.57983792 0.56786883 0.56813812 0.55163848 0.51694822\n",
      " 0.55951899 0.45016682 0.44426543 0.48958719 0.43491212 0.82998741\n",
      " 0.50457561 0.56595367 0.54318064 0.56290805 0.61533815 0.46350986\n",
      " 0.43364957 0.47899634 0.46738201 0.48971555 0.49572152 0.44991165\n",
      " 0.49812227 0.52402675 0.64101326 0.64396268]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "Train Epoch: 36 [0/54 (0%)]\tTrain Loss: 0.098922\n",
      "Train Epoch: 36 [8/54 (15%)]\tTrain Loss: 0.064778\n",
      "Train Epoch: 36 [16/54 (30%)]\tTrain Loss: 0.074353\n",
      "Train Epoch: 36 [24/54 (44%)]\tTrain Loss: 0.108167\n",
      "Train Epoch: 36 [32/54 (59%)]\tTrain Loss: 0.098088\n",
      "Train Epoch: 36 [40/54 (74%)]\tTrain Loss: 0.076126\n",
      "Train Epoch: 36 [48/54 (89%)]\tTrain Loss: 0.080282\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.86932933 0.95509303 0.90719903 0.61590773 0.43338433 0.54941094\n",
      " 0.91015536 0.84955776 0.24444328 0.03698438 0.29119432 0.07133052\n",
      " 0.24988659 0.65556961 0.75552726 0.42400753 0.40972543 0.32215759\n",
      " 0.34401396 0.50644791 0.39340472 0.45467567 0.62313426 0.72749597\n",
      " 0.50993454 0.66288894 0.73873883 0.60270298 0.52542967 0.54871166\n",
      " 0.6973877  0.61904734 0.65236413 0.27440995 0.2463292  0.35704717\n",
      " 0.35838872 0.49578524 0.49607921 0.53074229 0.54250914 0.49963716\n",
      " 0.34177127 0.50610763 0.57511586 0.64589721 0.9374525  0.94125366\n",
      " 0.95953202 0.58839667 0.96909952 0.18833433 0.57587683 0.62158209\n",
      " 0.11751535 0.44098625 0.62757546 0.50789827 0.25297016 0.79142016\n",
      " 0.6698249  0.60510123 0.73579854 0.74848747 0.76463056 0.7000196\n",
      " 0.6886481  0.77878118 0.74976373 0.3807036  0.75648874 0.68480617\n",
      " 0.60504967 0.59291178 0.62772727 0.75248814 0.99540436 0.99682826\n",
      " 0.96418113 0.81757855 0.79850084 0.83690256 0.77760679 0.59114343\n",
      " 0.5657112  0.54169887 0.71372616 0.75976765 0.64790493 0.53699166\n",
      " 0.70585155 0.49746072 0.51891333 0.68890226 0.6107018  0.60718364\n",
      " 0.5004881  0.36078    0.29964226 0.48136657 0.47662562 0.73101902\n",
      " 0.52714688 0.44809052 0.34270695 0.54940355 0.52373892 0.4104138\n",
      " 0.47921771 0.29587054 0.39043665 0.22743268 0.41582817 0.43142742\n",
      " 0.6117329  0.53310657 0.64556301 0.65047628]\n",
      "predict [1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 37 [0/54 (0%)]\tTrain Loss: 0.107601\n",
      "Train Epoch: 37 [8/54 (15%)]\tTrain Loss: 0.088593\n",
      "Train Epoch: 37 [16/54 (30%)]\tTrain Loss: 0.065668\n",
      "Train Epoch: 37 [24/54 (44%)]\tTrain Loss: 0.077821\n",
      "Train Epoch: 37 [32/54 (59%)]\tTrain Loss: 0.073244\n",
      "Train Epoch: 37 [40/54 (74%)]\tTrain Loss: 0.105116\n",
      "Train Epoch: 37 [48/54 (89%)]\tTrain Loss: 0.082982\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.44006535 0.89230484 0.7607106  0.25657961 0.20639087 0.28859499\n",
      " 0.77983886 0.52825344 0.16798654 0.08388256 0.07535111 0.06645706\n",
      " 0.05815893 0.54774243 0.68017018 0.15439522 0.12523614 0.03930765\n",
      " 0.26599208 0.28654999 0.33121219 0.19178683 0.40874061 0.45824498\n",
      " 0.23953328 0.41052356 0.53148377 0.38023013 0.2869952  0.28263927\n",
      " 0.54619569 0.48842391 0.38796329 0.15996148 0.1155457  0.12224907\n",
      " 0.10771846 0.30334851 0.19083369 0.22683261 0.23817421 0.19417673\n",
      " 0.1044643  0.20633352 0.32974029 0.4785783  0.67092365 0.68623698\n",
      " 0.71985078 0.38711551 0.75280499 0.07214807 0.30849952 0.31988215\n",
      " 0.19162695 0.16820467 0.52153713 0.19969118 0.16847752 0.47673774\n",
      " 0.58832675 0.46535742 0.59777522 0.60393262 0.6356414  0.54670215\n",
      " 0.52089542 0.56152332 0.47617817 0.63321912 0.48471951 0.39520738\n",
      " 0.46585849 0.4095611  0.44022119 0.62422007 0.86129647 0.88481641\n",
      " 0.82599813 0.67812598 0.68259561 0.68232554 0.71875054 0.30986458\n",
      " 0.26518384 0.42798948 0.53228241 0.55282992 0.41414484 0.28509358\n",
      " 0.45740592 0.26688772 0.21312828 0.42837515 0.42818984 0.3772549\n",
      " 0.22948694 0.15338387 0.07444063 0.24798965 0.21196949 0.53279895\n",
      " 0.19266106 0.28081322 0.27141529 0.3697449  0.54941392 0.1643218\n",
      " 0.23585486 0.01903682 0.19968918 0.2487189  0.26941919 0.2467133\n",
      " 0.34086281 0.24246308 0.39145803 0.4544909 ]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 38 [0/54 (0%)]\tTrain Loss: 0.109138\n",
      "Train Epoch: 38 [8/54 (15%)]\tTrain Loss: 0.102831\n",
      "Train Epoch: 38 [16/54 (30%)]\tTrain Loss: 0.157348\n",
      "Train Epoch: 38 [24/54 (44%)]\tTrain Loss: 0.073436\n",
      "Train Epoch: 38 [32/54 (59%)]\tTrain Loss: 0.088628\n",
      "Train Epoch: 38 [40/54 (74%)]\tTrain Loss: 0.069177\n",
      "Train Epoch: 38 [48/54 (89%)]\tTrain Loss: 0.052387\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.74844426 0.82395399 0.72452271 0.68956828 0.59401339 0.6274358\n",
      " 0.72424364 0.80213791 0.56201845 0.52783662 0.61094332 0.57272315\n",
      " 0.55662781 0.66560727 0.68252504 0.44637159 0.38533902 0.501858\n",
      " 0.48712736 0.53518629 0.5424391  0.48575991 0.62614489 0.78758121\n",
      " 0.44646573 0.6810928  0.78742886 0.6325233  0.49867383 0.50264972\n",
      " 0.62487799 0.637734   0.6025117  0.46751538 0.48112646 0.44445178\n",
      " 0.47507477 0.73179042 0.53553116 0.5188967  0.47221023 0.48136076\n",
      " 0.60666388 0.65633178 0.66721249 0.64827234 0.79589367 0.70898449\n",
      " 0.83438385 0.73248225 0.77263969 0.72027946 0.79680729 0.80738157\n",
      " 0.56683111 0.59256554 0.7558192  0.60358965 0.68306905 0.80821419\n",
      " 0.75877291 0.69247919 0.70869619 0.71378177 0.74651343 0.68311471\n",
      " 0.6349057  0.72027457 0.7116679  0.7866469  0.90309173 0.85565054\n",
      " 0.61937696 0.6203987  0.61265332 0.68315065 0.81058639 0.74756336\n",
      " 0.84347141 0.72900754 0.77574569 0.73266184 0.83308494 0.73392153\n",
      " 0.64561939 0.71438676 0.7733354  0.7820639  0.67500675 0.49983567\n",
      " 0.93997681 0.73130298 0.64302701 0.73194396 0.59845448 0.63510734\n",
      " 0.63212901 0.5234043  0.50519192 0.5966664  0.66216624 0.65353173\n",
      " 0.66379368 0.68409246 0.5578481  0.61745906 0.80098462 0.47415441\n",
      " 0.72998708 0.52969134 0.39346412 0.76668274 0.55583555 0.73312259\n",
      " 0.56297749 0.42841008 0.58219796 0.55439049]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 39 [0/54 (0%)]\tTrain Loss: 0.127960\n",
      "Train Epoch: 39 [8/54 (15%)]\tTrain Loss: 0.073785\n",
      "Train Epoch: 39 [16/54 (30%)]\tTrain Loss: 0.053101\n",
      "Train Epoch: 39 [24/54 (44%)]\tTrain Loss: 0.072681\n",
      "Train Epoch: 39 [32/54 (59%)]\tTrain Loss: 0.061755\n",
      "Train Epoch: 39 [40/54 (74%)]\tTrain Loss: 0.078592\n",
      "Train Epoch: 39 [48/54 (89%)]\tTrain Loss: 0.073848\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.9524824  0.98936975 0.96059138 0.87839341 0.77452737 0.78641254\n",
      " 0.96746117 0.95744497 0.57776517 0.27283719 0.49530488 0.25147003\n",
      " 0.42838925 0.91584367 0.93856251 0.56915236 0.46030283 0.39606258\n",
      " 0.33614025 0.59190351 0.13139553 0.61524308 0.66145533 0.77825791\n",
      " 0.58986306 0.73806322 0.84128004 0.86604297 0.73462015 0.69159544\n",
      " 0.93941134 0.91714519 0.88968605 0.37823153 0.1268798  0.32195535\n",
      " 0.35901201 0.85282278 0.46200961 0.53521764 0.66907048 0.52467972\n",
      " 0.64026999 0.77658725 0.8747378  0.72484505 0.95123976 0.95173961\n",
      " 0.99441373 0.77958441 0.98670226 0.50426644 0.83822453 0.77874029\n",
      " 0.04855948 0.49968219 0.87233263 0.50222266 0.55817908 0.96296185\n",
      " 0.90954292 0.84467196 0.95662212 0.96555948 0.96579593 0.95048457\n",
      " 0.95502388 0.94509727 0.93745834 0.73116732 0.93917018 0.91835868\n",
      " 0.91925758 0.70681632 0.90762305 0.94226569 0.9966917  0.99332112\n",
      " 0.99010193 0.98939109 0.98268962 0.98053819 0.98653084 0.87475741\n",
      " 0.6873998  0.82202601 0.93689257 0.95235318 0.70221043 0.60319912\n",
      " 0.3579011  0.65347964 0.81557196 0.81741828 0.46027204 0.18819755\n",
      " 0.44220531 0.33316773 0.26078913 0.63829213 0.52853101 0.97856754\n",
      " 0.56144738 0.68775821 0.4427866  0.8495155  0.88747132 0.47749507\n",
      " 0.32004493 0.1992528  0.08754711 0.45550054 0.34036261 0.1980516\n",
      " 0.72768611 0.67984265 0.62883592 0.70997149]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "Train Epoch: 40 [0/54 (0%)]\tTrain Loss: 0.054058\n",
      "Train Epoch: 40 [8/54 (15%)]\tTrain Loss: 0.069792\n",
      "Train Epoch: 40 [16/54 (30%)]\tTrain Loss: 0.090220\n",
      "Train Epoch: 40 [24/54 (44%)]\tTrain Loss: 0.069382\n",
      "Train Epoch: 40 [32/54 (59%)]\tTrain Loss: 0.136551\n",
      "Train Epoch: 40 [40/54 (74%)]\tTrain Loss: 0.115707\n",
      "Train Epoch: 40 [48/54 (89%)]\tTrain Loss: 0.150046\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.91040421 0.86362052 0.87497652 0.84368891 0.85284674 0.91076308\n",
      " 0.84675729 0.8593871  0.85393399 0.66280007 0.61838353 0.61932337\n",
      " 0.63626623 0.70399106 0.79293114 0.65628773 0.59411091 0.58949852\n",
      " 0.56754208 0.5410831  0.58189237 0.57777494 0.65535778 0.66339284\n",
      " 0.55196524 0.69657093 0.75542766 0.61117196 0.5596453  0.56019747\n",
      " 0.78500646 0.78187799 0.59841073 0.5840283  0.57540071 0.54585439\n",
      " 0.5342865  0.58555603 0.6245808  0.61883932 0.64860386 0.59485567\n",
      " 0.59999347 0.6438247  0.68404722 0.76525784 0.96420968 0.92555064\n",
      " 0.95852202 0.78779793 0.95478016 0.67611068 0.66202724 0.83478743\n",
      " 0.6383937  0.58513671 0.72632664 0.60282433 0.57832617 0.87344795\n",
      " 0.95230216 0.93384236 0.86949509 0.87324256 0.78594548 0.79520035\n",
      " 0.83318573 0.87950677 0.87806207 0.67818719 0.83182389 0.81438875\n",
      " 0.6982277  0.65794367 0.67087269 0.73060739 0.96015453 0.93863761\n",
      " 0.94666713 0.88030672 0.90662074 0.87775159 0.86110735 0.65504193\n",
      " 0.66536558 0.72060496 0.81345373 0.85253489 0.57646817 0.54511875\n",
      " 0.61528236 0.61211354 0.57669508 0.63481104 0.6464445  0.62141478\n",
      " 0.56996167 0.6451335  0.56204641 0.62566811 0.5231905  0.87630856\n",
      " 0.51581961 0.62625605 0.63665867 0.62886423 0.65536046 0.54957086\n",
      " 0.59840065 0.55147648 0.57388252 0.60336429 0.61713189 0.618195\n",
      " 0.64209378 0.60857767 0.57228732 0.59733891]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 48 TN= 22 FN= 10 FP= 38\n",
      "TP+FP 86\n",
      "precision 0.5581395348837209\n",
      "recall 0.8275862068965517\n",
      "F1 0.6666666666666666\n",
      "acc 0.5932203389830508\n",
      "AUCp 0.5971264367816091\n",
      "AUC 0.6057471264367815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The epoch is 40, average recall: 0.8276, average precision: 0.5581,average F1: 0.6667, average accuracy: 0.5932, average AUC: 0.6057\n",
      "Train Epoch: 41 [0/54 (0%)]\tTrain Loss: 0.093739\n",
      "Train Epoch: 41 [8/54 (15%)]\tTrain Loss: 0.130031\n",
      "Train Epoch: 41 [16/54 (30%)]\tTrain Loss: 0.091262\n",
      "Train Epoch: 41 [24/54 (44%)]\tTrain Loss: 0.075295\n",
      "Train Epoch: 41 [32/54 (59%)]\tTrain Loss: 0.094715\n",
      "Train Epoch: 41 [40/54 (74%)]\tTrain Loss: 0.071474\n",
      "Train Epoch: 41 [48/54 (89%)]\tTrain Loss: 0.061870\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.76167786 0.92101556 0.90749246 0.71986157 0.55707341 0.56910694\n",
      " 0.86208749 0.83645856 0.50982475 0.22649556 0.40943217 0.23216204\n",
      " 0.38723576 0.72938812 0.79523855 0.40249857 0.44387931 0.27559781\n",
      " 0.42400077 0.52342457 0.50678968 0.52818352 0.60616922 0.5093326\n",
      " 0.54826409 0.62768829 0.53351253 0.56274492 0.50693733 0.52614903\n",
      " 0.75449765 0.66448116 0.66235274 0.36459568 0.35015187 0.40583628\n",
      " 0.4196766  0.467969   0.40249202 0.46523938 0.48584822 0.44218871\n",
      " 0.34981588 0.51678741 0.59914881 0.64443243 0.96422875 0.92115313\n",
      " 0.94767398 0.31013659 0.95268166 0.3371532  0.58462191 0.7129187\n",
      " 0.35451964 0.47095188 0.63166422 0.50029397 0.42352417 0.82073087\n",
      " 0.5352028  0.50904405 0.68765789 0.66073793 0.78471589 0.78922886\n",
      " 0.81796372 0.86850613 0.85469359 0.47819543 0.83284003 0.80453902\n",
      " 0.61360723 0.56532484 0.59520447 0.57777345 0.97967315 0.95826548\n",
      " 0.92762351 0.8104189  0.81748736 0.80864507 0.81976241 0.47362509\n",
      " 0.45905498 0.36873078 0.5940311  0.66789186 0.4602052  0.42247969\n",
      " 0.53025705 0.39696193 0.45077524 0.61498004 0.52906001 0.56487703\n",
      " 0.4670023  0.41179925 0.31878078 0.42841506 0.38035202 0.80569565\n",
      " 0.35175556 0.42207152 0.42188528 0.35824698 0.33196175 0.44702029\n",
      " 0.52704042 0.36495116 0.40886313 0.40095022 0.47610986 0.52786839\n",
      " 0.35428905 0.29488072 0.57913303 0.62043202]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 42 [0/54 (0%)]\tTrain Loss: 0.083916\n",
      "Train Epoch: 42 [8/54 (15%)]\tTrain Loss: 0.070608\n",
      "Train Epoch: 42 [16/54 (30%)]\tTrain Loss: 0.054567\n",
      "Train Epoch: 42 [24/54 (44%)]\tTrain Loss: 0.135777\n",
      "Train Epoch: 42 [32/54 (59%)]\tTrain Loss: 0.079288\n",
      "Train Epoch: 42 [40/54 (74%)]\tTrain Loss: 0.065966\n",
      "Train Epoch: 42 [48/54 (89%)]\tTrain Loss: 0.094007\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.51695025 0.61869091 0.64780778 0.51142579 0.38623929 0.34972188\n",
      " 0.61129975 0.55784148 0.47226325 0.29345831 0.39158857 0.32372713\n",
      " 0.35082147 0.69823134 0.68915325 0.36094865 0.56911916 0.31299815\n",
      " 0.4978081  0.60699069 0.59049881 0.51479256 0.72650677 0.76298702\n",
      " 0.56493741 0.80681646 0.83287144 0.55824935 0.54577804 0.5154947\n",
      " 0.78036785 0.68480098 0.64766204 0.42556021 0.36613643 0.42275351\n",
      " 0.40161505 0.39490035 0.45182225 0.56997842 0.57431656 0.55069804\n",
      " 0.31502202 0.49447823 0.55375975 0.56563926 0.74224991 0.57820261\n",
      " 0.77706277 0.42451739 0.76989526 0.27671087 0.51246631 0.48115185\n",
      " 0.51468259 0.49910933 0.66965908 0.52863789 0.41151857 0.60577518\n",
      " 0.74033374 0.65976596 0.76925784 0.67032546 0.60582918 0.62317348\n",
      " 0.61797673 0.77223706 0.62243301 0.53034264 0.56048697 0.54447883\n",
      " 0.75258392 0.66564041 0.61564088 0.56631875 0.60461271 0.55595803\n",
      " 0.67773354 0.65944058 0.72588867 0.7370348  0.81772131 0.69909298\n",
      " 0.69104457 0.53968507 0.74661928 0.69900453 0.62073535 0.53266811\n",
      " 0.64392477 0.41490072 0.4568156  0.77423716 0.72315425 0.66485852\n",
      " 0.50698662 0.38578168 0.62992513 0.48741287 0.32002059 0.66524786\n",
      " 0.4414925  0.37349644 0.42927721 0.36153504 0.45864171 0.43508795\n",
      " 0.56450516 0.41862017 0.46588024 0.45117897 0.62007344 0.55739832\n",
      " 0.56990021 0.44905385 0.5525521  0.59060371]\n",
      "predict [1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 43 [0/54 (0%)]\tTrain Loss: 0.062630\n",
      "Train Epoch: 43 [8/54 (15%)]\tTrain Loss: 0.103881\n",
      "Train Epoch: 43 [16/54 (30%)]\tTrain Loss: 0.093283\n",
      "Train Epoch: 43 [24/54 (44%)]\tTrain Loss: 0.123374\n",
      "Train Epoch: 43 [32/54 (59%)]\tTrain Loss: 0.070614\n",
      "Train Epoch: 43 [40/54 (74%)]\tTrain Loss: 0.051638\n",
      "Train Epoch: 43 [48/54 (89%)]\tTrain Loss: 0.089143\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.99119616 0.98784328 0.92352647 0.87673414 0.80425817 0.95794541\n",
      " 0.80169439 0.97983503 0.94949567 0.1166409  0.38376075 0.11900335\n",
      " 0.14814416 0.80452359 0.90787679 0.35699514 0.59778285 0.50365758\n",
      " 0.21258891 0.364169   0.28144336 0.40409151 0.69537711 0.91978496\n",
      " 0.40167478 0.87395406 0.97474176 0.3956475  0.52191043 0.52647519\n",
      " 0.98975277 0.89412117 0.69798815 0.28501302 0.22403093 0.35311824\n",
      " 0.27511913 0.38199422 0.73531067 0.6217401  0.67319709 0.57029712\n",
      " 0.35417289 0.62150061 0.52194434 0.84951806 0.9922111  0.95737249\n",
      " 0.99576366 0.83991534 0.99962628 0.15467915 0.8582316  0.82690281\n",
      " 0.28248334 0.23329745 0.89802396 0.40665665 0.29037854 0.96143311\n",
      " 0.99934036 0.96752208 0.99808156 0.99853659 0.95746636 0.90596908\n",
      " 0.96217531 0.93079972 0.94141889 0.92438602 0.99659508 0.99768841\n",
      " 0.93740559 0.95483887 0.90929282 0.96780652 0.99984729 0.99692494\n",
      " 0.98656183 0.9976725  0.99874312 0.99923074 0.99896395 0.93151277\n",
      " 0.83654881 0.84021151 0.96121711 0.98952484 0.3680779  0.35379633\n",
      " 0.42238092 0.6380558  0.9134509  0.75432765 0.75295907 0.35806251\n",
      " 0.31533945 0.61643559 0.41048795 0.44899595 0.50626701 0.99852282\n",
      " 0.52224231 0.69181341 0.18749781 0.91185981 0.95795274 0.28502128\n",
      " 0.346205   0.18364172 0.28956872 0.31758571 0.24855813 0.34855929\n",
      " 0.42128479 0.4959918  0.32742223 0.32671392]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 44 [0/54 (0%)]\tTrain Loss: 0.052134\n",
      "Train Epoch: 44 [8/54 (15%)]\tTrain Loss: 0.076387\n",
      "Train Epoch: 44 [16/54 (30%)]\tTrain Loss: 0.043109\n",
      "Train Epoch: 44 [24/54 (44%)]\tTrain Loss: 0.065313\n",
      "Train Epoch: 44 [32/54 (59%)]\tTrain Loss: 0.058040\n",
      "Train Epoch: 44 [40/54 (74%)]\tTrain Loss: 0.077902\n",
      "Train Epoch: 44 [48/54 (89%)]\tTrain Loss: 0.069965\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.64816076 0.93375975 0.92906833 0.8651939  0.76275826 0.51548457\n",
      " 0.82083476 0.91137427 0.70905972 0.15000068 0.53911912 0.12992519\n",
      " 0.45212054 0.91709638 0.87820774 0.23547032 0.34125394 0.24352922\n",
      " 0.37609598 0.60166258 0.40008745 0.36449024 0.71018809 0.87481183\n",
      " 0.34000972 0.85397393 0.88588142 0.54208213 0.4639504  0.47696471\n",
      " 0.588139   0.48794934 0.46845022 0.26703927 0.28125247 0.23668034\n",
      " 0.24506314 0.35604644 0.34339806 0.44945571 0.42346293 0.38341165\n",
      " 0.21861403 0.42203644 0.52566874 0.82382077 0.91489875 0.87893164\n",
      " 0.96499491 0.33868882 0.96152121 0.46336088 0.73479784 0.68432319\n",
      " 0.37619525 0.53174263 0.5279991  0.66336274 0.2217162  0.92019027\n",
      " 0.86956912 0.70036364 0.81096286 0.7666862  0.91165835 0.90726662\n",
      " 0.90489078 0.9647724  0.93307364 0.39901745 0.92641991 0.91920179\n",
      " 0.64165902 0.57711917 0.60791856 0.61861718 0.98458821 0.95065391\n",
      " 0.97480619 0.84042323 0.89589298 0.91081047 0.89839625 0.64932275\n",
      " 0.53076017 0.50517178 0.85733187 0.88445574 0.63325584 0.36662918\n",
      " 0.84182513 0.40347531 0.52845544 0.8188321  0.69776344 0.52833349\n",
      " 0.34885153 0.28555307 0.34659651 0.43285748 0.40293053 0.59333104\n",
      " 0.49723804 0.29169312 0.27982315 0.31139761 0.37544805 0.2799339\n",
      " 0.44069722 0.26411507 0.29070085 0.37790662 0.45104638 0.3539893\n",
      " 0.40661773 0.31491822 0.56349927 0.55367815]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 45 [0/54 (0%)]\tTrain Loss: 0.156913\n",
      "Train Epoch: 45 [8/54 (15%)]\tTrain Loss: 0.097287\n",
      "Train Epoch: 45 [16/54 (30%)]\tTrain Loss: 0.047526\n",
      "Train Epoch: 45 [24/54 (44%)]\tTrain Loss: 0.058574\n",
      "Train Epoch: 45 [32/54 (59%)]\tTrain Loss: 0.067037\n",
      "Train Epoch: 45 [40/54 (74%)]\tTrain Loss: 0.090190\n",
      "Train Epoch: 45 [48/54 (89%)]\tTrain Loss: 0.093522\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.99998927 0.99999344 1.         1.         0.99999988 0.99999845\n",
      " 0.9998461  1.         1.         0.44879141 0.91940844 0.34261155\n",
      " 0.98048651 0.99998331 0.9999969  0.99397111 0.96340418 0.99503869\n",
      " 0.6464389  0.8040697  0.92856324 0.81233484 0.9975841  0.99567223\n",
      " 0.69349104 0.98941994 0.98949081 0.9990139  0.99184906 0.98101532\n",
      " 0.9994216  1.         0.9999193  0.7773056  0.8238436  0.76094294\n",
      " 0.68698943 0.98587507 0.96536094 0.97198606 0.98536927 0.95533919\n",
      " 0.97168058 0.99991393 0.99999034 0.99999964 0.99999857 0.99999988\n",
      " 1.         0.99992239 0.99999928 0.9375931  0.97316056 0.99708015\n",
      " 0.93555051 0.98433346 0.9996208  0.99042612 0.50213075 0.99963999\n",
      " 0.99997997 0.99978465 0.99996364 0.99998331 0.99999201 0.99999881\n",
      " 0.99999118 0.99945313 1.         0.99443012 1.         1.\n",
      " 1.         1.         0.99705529 0.99998546 1.         1.\n",
      " 1.         1.         1.         1.         1.         0.87773013\n",
      " 0.96429962 0.99108326 0.99999595 0.99999607 0.86656415 0.78469849\n",
      " 0.91702217 0.99989474 0.99991727 0.94263798 0.99132288 0.90792352\n",
      " 0.61337715 0.97420233 0.95250535 0.99054396 0.68709528 1.\n",
      " 0.91533792 0.99835509 0.99897885 0.99982846 0.99951196 0.70810598\n",
      " 0.90005529 0.96097219 0.80166072 0.75447935 0.96132988 0.89161628\n",
      " 0.99757081 0.99069291 0.83187419 0.92659199]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 46 [0/54 (0%)]\tTrain Loss: 0.084770\n",
      "Train Epoch: 46 [8/54 (15%)]\tTrain Loss: 0.064105\n",
      "Train Epoch: 46 [16/54 (30%)]\tTrain Loss: 0.070956\n",
      "Train Epoch: 46 [24/54 (44%)]\tTrain Loss: 0.110076\n",
      "Train Epoch: 46 [32/54 (59%)]\tTrain Loss: 0.056484\n",
      "Train Epoch: 46 [40/54 (74%)]\tTrain Loss: 0.053660\n",
      "Train Epoch: 46 [48/54 (89%)]\tTrain Loss: 0.086415\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.92403316 0.71463823 0.04079012 0.61913973 0.81421983 0.48208359\n",
      " 0.83565706 0.81973803 0.406266   0.23930195 0.43800944 0.26182753\n",
      " 0.52990997 0.7406494  0.81973761 0.58780915 0.47803259 0.39831918\n",
      " 0.38408518 0.40549615 0.44907001 0.51698178 0.63890177 0.7732498\n",
      " 0.42084628 0.70325232 0.88161099 0.55379754 0.45087427 0.49375385\n",
      " 0.75013232 0.54019082 0.36336923 0.45104155 0.43973485 0.44760367\n",
      " 0.40165469 0.43816403 0.48293281 0.53262562 0.53461629 0.49291176\n",
      " 0.37168205 0.53838569 0.53170604 0.5549078  0.83762997 0.24289024\n",
      " 0.52256006 0.74546957 0.75090337 0.34891185 0.48115692 0.75893283\n",
      " 0.49844587 0.38763767 0.67784649 0.44091848 0.37418991 0.87098122\n",
      " 0.83700293 0.71686655 0.86341602 0.91287994 0.65491378 0.63928694\n",
      " 0.56718725 0.93127412 0.40170202 0.58458257 0.56869584 0.51021671\n",
      " 0.655487   0.41188049 0.5613206  0.64045864 0.50050282 0.09660167\n",
      " 0.39317426 0.65579003 0.24523793 0.11865554 0.27969754 0.53556961\n",
      " 0.56510293 0.59343189 0.679178   0.8253178  0.53616035 0.41367036\n",
      " 0.73761386 0.59001607 0.54366446 0.72537863 0.63482952 0.58541578\n",
      " 0.36436859 0.57561946 0.48260188 0.56066912 0.51254779 0.9655149\n",
      " 0.49480689 0.43633386 0.38661876 0.37245116 0.63909602 0.4149583\n",
      " 0.50015777 0.45913082 0.43985555 0.57664299 0.55042964 0.47713262\n",
      " 0.4905265  0.45375893 0.64034379 0.64852834]\n",
      "predict [1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1.]\n",
      "Train Epoch: 47 [0/54 (0%)]\tTrain Loss: 0.097196\n",
      "Train Epoch: 47 [8/54 (15%)]\tTrain Loss: 0.069980\n",
      "Train Epoch: 47 [16/54 (30%)]\tTrain Loss: 0.042094\n",
      "Train Epoch: 47 [24/54 (44%)]\tTrain Loss: 0.046928\n",
      "Train Epoch: 47 [32/54 (59%)]\tTrain Loss: 0.051789\n",
      "Train Epoch: 47 [40/54 (74%)]\tTrain Loss: 0.056201\n",
      "Train Epoch: 47 [48/54 (89%)]\tTrain Loss: 0.089521\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.67034602 0.8927151  0.9635129  0.64324814 0.67640853 0.68104988\n",
      " 0.74865049 0.78964794 0.45808327 0.16536821 0.08720277 0.30833462\n",
      " 0.11159111 0.7657764  0.73879695 0.24529725 0.1857422  0.0997955\n",
      " 0.2628442  0.38633794 0.54982924 0.18497594 0.7664004  0.94049841\n",
      " 0.21066254 0.83921915 0.96522039 0.57467824 0.4166241  0.31186423\n",
      " 0.63135248 0.71869087 0.77757961 0.17764328 0.16669725 0.19390661\n",
      " 0.09675717 0.14763063 0.23327659 0.40853834 0.45650494 0.3537192\n",
      " 0.14827465 0.25862244 0.45583841 0.77161723 0.77913606 0.89340127\n",
      " 0.99030745 0.8866787  0.99026978 0.04036327 0.31200999 0.21619102\n",
      " 0.50708592 0.26805261 0.68326581 0.30146578 0.45836958 0.45934644\n",
      " 0.79324162 0.75181627 0.86660361 0.88424444 0.82998937 0.67980498\n",
      " 0.57209057 0.74415576 0.86355531 0.55801743 0.94459742 0.89292878\n",
      " 0.81047559 0.82511693 0.55536538 0.7069056  0.93915439 0.95649439\n",
      " 0.96909392 0.90436649 0.97449601 0.98166323 0.99368507 0.59502655\n",
      " 0.79819947 0.61241513 0.94802988 0.96609026 0.62376136 0.30916831\n",
      " 0.90946519 0.35908541 0.24560207 0.92832178 0.6560753  0.69097197\n",
      " 0.43307418 0.32175431 0.24148527 0.4808448  0.43851361 0.83820599\n",
      " 0.22649066 0.13466072 0.35806638 0.49993017 0.84313303 0.20939621\n",
      " 0.28899798 0.18411936 0.24222989 0.45263594 0.47434717 0.36097702\n",
      " 0.40291426 0.27457792 0.49659374 0.34162882]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 48 [0/54 (0%)]\tTrain Loss: 0.066709\n",
      "Train Epoch: 48 [8/54 (15%)]\tTrain Loss: 0.085086\n",
      "Train Epoch: 48 [16/54 (30%)]\tTrain Loss: 0.112351\n",
      "Train Epoch: 48 [24/54 (44%)]\tTrain Loss: 0.123602\n",
      "Train Epoch: 48 [32/54 (59%)]\tTrain Loss: 0.053811\n",
      "Train Epoch: 48 [40/54 (74%)]\tTrain Loss: 0.092646\n",
      "Train Epoch: 48 [48/54 (89%)]\tTrain Loss: 0.063928\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.97447824 0.97573435 0.97186476 0.92530757 0.92581767 0.94419777\n",
      " 0.92770761 0.9729495  0.83416116 0.34230128 0.57875234 0.19514397\n",
      " 0.87330627 0.93758744 0.95583254 0.75890267 0.72228938 0.77913719\n",
      " 0.53412038 0.57947445 0.59687221 0.80127883 0.94382215 0.93048173\n",
      " 0.59209025 0.95595276 0.97996521 0.87582564 0.89056665 0.89553702\n",
      " 0.99167097 0.98863214 0.96471655 0.4877159  0.49345985 0.55369806\n",
      " 0.47180557 0.51247543 0.88698035 0.8705169  0.94647127 0.78439438\n",
      " 0.55789798 0.86143214 0.9307366  0.98221177 0.99868721 0.99612659\n",
      " 0.99824071 0.92868263 0.99844331 0.47100055 0.87992674 0.98072088\n",
      " 0.72534078 0.65243679 0.98836106 0.71145993 0.3220897  0.97952932\n",
      " 0.99887818 0.98964244 0.9992674  0.9992829  0.97094721 0.95508248\n",
      " 0.94353229 0.9896543  0.98547006 0.89044917 0.99377269 0.98958522\n",
      " 0.99471879 0.98381269 0.97307837 0.99427807 0.99893636 0.99590331\n",
      " 0.99612421 0.99949515 0.99969769 0.99885213 0.99883062 0.92751706\n",
      " 0.96842903 0.98730868 0.98977309 0.99690729 0.69857484 0.58241791\n",
      " 0.84493649 0.91617095 0.95227838 0.87966162 0.83996546 0.74686205\n",
      " 0.54414898 0.95090681 0.76039839 0.87621158 0.82873303 0.99956793\n",
      " 0.70774001 0.78665155 0.77143419 0.92814541 0.95532894 0.52553213\n",
      " 0.63507187 0.55759764 0.5726403  0.58638304 0.75469321 0.65522587\n",
      " 0.74382412 0.82075012 0.59393477 0.68635112]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49 [0/54 (0%)]\tTrain Loss: 0.068907\n",
      "Train Epoch: 49 [8/54 (15%)]\tTrain Loss: 0.047720\n",
      "Train Epoch: 49 [16/54 (30%)]\tTrain Loss: 0.055095\n",
      "Train Epoch: 49 [24/54 (44%)]\tTrain Loss: 0.063583\n",
      "Train Epoch: 49 [32/54 (59%)]\tTrain Loss: 0.104476\n",
      "Train Epoch: 49 [40/54 (74%)]\tTrain Loss: 0.099044\n",
      "Train Epoch: 49 [48/54 (89%)]\tTrain Loss: 0.050558\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.56427199 0.87250495 0.82882106 0.61920005 0.57278496 0.36432153\n",
      " 0.71620768 0.83393484 0.29476991 0.21295866 0.31403312 0.30878031\n",
      " 0.34903729 0.82238418 0.82764745 0.32176238 0.35720021 0.22943513\n",
      " 0.34148291 0.42933935 0.54026788 0.3696098  0.74045956 0.86825353\n",
      " 0.36804351 0.8350774  0.95066756 0.57116228 0.47640574 0.46422219\n",
      " 0.82134783 0.78407913 0.64581627 0.39015362 0.37079009 0.35450566\n",
      " 0.29010656 0.29025093 0.44475663 0.42950946 0.50040787 0.38362479\n",
      " 0.20032771 0.45374048 0.56321752 0.72879392 0.94334257 0.9052164\n",
      " 0.95744723 0.83926249 0.94547659 0.10308319 0.52867502 0.61565703\n",
      " 0.45757979 0.30181339 0.76430035 0.32723054 0.18443878 0.67054987\n",
      " 0.89264339 0.74824578 0.90441167 0.93584484 0.83646375 0.77180713\n",
      " 0.7139197  0.86259758 0.83170676 0.60714287 0.85424    0.80860668\n",
      " 0.91053444 0.87199032 0.79027689 0.83603197 0.97837478 0.96326822\n",
      " 0.93938464 0.96162158 0.97937578 0.9741441  0.98282802 0.5339883\n",
      " 0.45830455 0.68883777 0.84827656 0.9107424  0.58272058 0.38700387\n",
      " 0.81449497 0.56358516 0.6716705  0.78997576 0.75618547 0.63443685\n",
      " 0.43177816 0.54117477 0.33299807 0.66449052 0.32401049 0.95152074\n",
      " 0.37865713 0.42152232 0.45772889 0.61840981 0.5896405  0.32687452\n",
      " 0.44111708 0.29022756 0.33368886 0.59145355 0.68959439 0.60978937\n",
      " 0.50187802 0.36008036 0.50372338 0.528557  ]\n",
      "predict [1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 50 [0/54 (0%)]\tTrain Loss: 0.056480\n",
      "Train Epoch: 50 [8/54 (15%)]\tTrain Loss: 0.093712\n",
      "Train Epoch: 50 [16/54 (30%)]\tTrain Loss: 0.053599\n",
      "Train Epoch: 50 [24/54 (44%)]\tTrain Loss: 0.048007\n",
      "Train Epoch: 50 [32/54 (59%)]\tTrain Loss: 0.101874\n",
      "Train Epoch: 50 [40/54 (74%)]\tTrain Loss: 0.062159\n",
      "Train Epoch: 50 [48/54 (89%)]\tTrain Loss: 0.079103\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.26786545 0.64329827 0.65863162 0.57576978 0.59177667 0.38835549\n",
      " 0.51639861 0.75466794 0.50947082 0.0385843  0.2447319  0.06056083\n",
      " 0.19401731 0.72363043 0.58497018 0.26079985 0.31619191 0.27649856\n",
      " 0.26039597 0.35389975 0.27313524 0.29733741 0.58530754 0.74735063\n",
      " 0.30929634 0.68667197 0.84656751 0.44889277 0.43150949 0.37685865\n",
      " 0.29445362 0.38249415 0.26379389 0.29762131 0.31356043 0.27970806\n",
      " 0.17075209 0.18218204 0.47894865 0.37003765 0.46338645 0.32146284\n",
      " 0.20230775 0.3887524  0.51939374 0.41819909 0.44992286 0.59600705\n",
      " 0.77017814 0.74701297 0.71930939 0.06734011 0.37513259 0.30273709\n",
      " 0.2858775  0.2676138  0.71461332 0.24432509 0.12310596 0.48985371\n",
      " 0.8855716  0.76480848 0.82769102 0.87682706 0.57262802 0.56996381\n",
      " 0.51827329 0.76646131 0.72904056 0.34188935 0.88172221 0.85366726\n",
      " 0.79002875 0.79855198 0.50881219 0.72341549 0.85140437 0.81338161\n",
      " 0.80398512 0.85212076 0.919447   0.87522876 0.94330633 0.61828148\n",
      " 0.4286086  0.58203918 0.84827471 0.89454299 0.53167546 0.39465883\n",
      " 0.7710768  0.53435749 0.63854975 0.73030853 0.68616241 0.48132843\n",
      " 0.172895   0.55153584 0.30263117 0.63701791 0.32875496 0.79893559\n",
      " 0.35565069 0.39051303 0.29880756 0.6493144  0.64357585 0.29008186\n",
      " 0.41849744 0.22967368 0.27428213 0.5986858  0.60065556 0.54872346\n",
      " 0.53366464 0.37530583 0.50925761 0.56703401]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
      "vote_pred [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 42 TN= 28 FN= 16 FP= 32\n",
      "TP+FP 74\n",
      "precision 0.5675675675675675\n",
      "recall 0.7241379310344828\n",
      "F1 0.6363636363636365\n",
      "acc 0.5932203389830508\n",
      "AUCp 0.5954022988505747\n",
      "AUC 0.6247126436781609\n",
      "\n",
      " The epoch is 50, average recall: 0.7241, average precision: 0.5676,average F1: 0.6364, average accuracy: 0.5932, average AUC: 0.6247\n",
      "Train Epoch: 51 [0/54 (0%)]\tTrain Loss: 0.081599\n",
      "Train Epoch: 51 [8/54 (15%)]\tTrain Loss: 0.030217\n",
      "Train Epoch: 51 [16/54 (30%)]\tTrain Loss: 0.061641\n",
      "Train Epoch: 51 [24/54 (44%)]\tTrain Loss: 0.077124\n",
      "Train Epoch: 51 [32/54 (59%)]\tTrain Loss: 0.034442\n",
      "Train Epoch: 51 [40/54 (74%)]\tTrain Loss: 0.099714\n",
      "Train Epoch: 51 [48/54 (89%)]\tTrain Loss: 0.097939\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.50862771 0.71104914 0.70380855 0.60879636 0.46903929 0.19762994\n",
      " 0.65115255 0.74181259 0.16618426 0.01315434 0.04512873 0.01773723\n",
      " 0.10494079 0.70392656 0.64955384 0.20821105 0.20862405 0.08673385\n",
      " 0.24114029 0.4901948  0.47002727 0.28225148 0.65804732 0.69204366\n",
      " 0.32449803 0.67841673 0.6650604  0.49750549 0.40667769 0.40590775\n",
      " 0.5313313  0.42553794 0.53880984 0.11503578 0.14403528 0.14858925\n",
      " 0.13645881 0.29838735 0.26080209 0.3401269  0.35999548 0.27000993\n",
      " 0.2037144  0.2775555  0.39039022 0.60365409 0.34155399 0.56948674\n",
      " 0.91947401 0.30698365 0.89233238 0.03770544 0.24045554 0.15068986\n",
      " 0.24000396 0.23908611 0.41909266 0.26340753 0.18736903 0.41834301\n",
      " 0.50383466 0.36806566 0.56001186 0.57252139 0.66219282 0.5825572\n",
      " 0.36970735 0.7304737  0.66915882 0.38112733 0.44729328 0.41395652\n",
      " 0.67093354 0.61347061 0.55984998 0.5272181  0.81573021 0.76009643\n",
      " 0.84287083 0.68480194 0.75955337 0.77444106 0.81526822 0.34812424\n",
      " 0.38087803 0.42137203 0.60283649 0.70065135 0.55424118 0.30828881\n",
      " 0.67602968 0.23831268 0.32110599 0.82300454 0.55787843 0.5675931\n",
      " 0.2194422  0.24262586 0.19077396 0.31863728 0.24213916 0.47392416\n",
      " 0.16232683 0.09528627 0.16092455 0.1347117  0.52979195 0.20006344\n",
      " 0.31523755 0.21748999 0.19734907 0.22895472 0.32836333 0.32704669\n",
      " 0.18496452 0.1174633  0.49050748 0.53111774]\n",
      "predict [1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 52 [0/54 (0%)]\tTrain Loss: 0.087250\n",
      "Train Epoch: 52 [8/54 (15%)]\tTrain Loss: 0.084150\n",
      "Train Epoch: 52 [16/54 (30%)]\tTrain Loss: 0.046921\n",
      "Train Epoch: 52 [24/54 (44%)]\tTrain Loss: 0.065812\n",
      "Train Epoch: 52 [32/54 (59%)]\tTrain Loss: 0.083577\n",
      "Train Epoch: 52 [40/54 (74%)]\tTrain Loss: 0.082566\n",
      "Train Epoch: 52 [48/54 (89%)]\tTrain Loss: 0.053067\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.91379768 0.90204334 0.89843857 0.94668531 0.94154018 0.9715324\n",
      " 0.83088875 0.98838705 0.97992718 0.63376743 0.83781016 0.5220992\n",
      " 0.80651563 0.94645131 0.91706085 0.72413057 0.97349495 0.8374126\n",
      " 0.75106895 0.81534976 0.69469094 0.81867832 0.90995485 0.96928126\n",
      " 0.77819675 0.9578023  0.98439837 0.76788139 0.84929216 0.68571281\n",
      " 0.93834364 0.89352793 0.79553008 0.78168529 0.72999358 0.70861918\n",
      " 0.75308156 0.77879232 0.84056127 0.8967855  0.86097193 0.89897192\n",
      " 0.64648235 0.88638598 0.86232919 0.93879515 0.96866983 0.95774108\n",
      " 0.97795594 0.96425468 0.98552334 0.93759584 0.93889141 0.99175805\n",
      " 0.86045009 0.86580658 0.9657864  0.87250775 0.42939505 0.95120919\n",
      " 0.98116493 0.96217263 0.98185456 0.96919864 0.97436154 0.9817031\n",
      " 0.98521751 0.97751254 0.95568126 0.93573707 0.98892224 0.98854208\n",
      " 0.97051448 0.96567363 0.96631438 0.96358413 0.95301306 0.92410719\n",
      " 0.96904445 0.97780401 0.98897129 0.98593545 0.98743057 0.96644419\n",
      " 0.95980233 0.95809734 0.9778496  0.98879761 0.75432914 0.73535597\n",
      " 0.88530058 0.96693277 0.97222579 0.9576208  0.96894431 0.83043855\n",
      " 0.62286061 0.82423282 0.84983444 0.82250881 0.7825765  0.98359495\n",
      " 0.91248465 0.93971854 0.91497648 0.97131497 0.97188479 0.68403614\n",
      " 0.76479894 0.78989929 0.71703148 0.82705176 0.92883265 0.74754739\n",
      " 0.7513873  0.76473933 0.70600182 0.72210443]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 53 [0/54 (0%)]\tTrain Loss: 0.082637\n",
      "Train Epoch: 53 [8/54 (15%)]\tTrain Loss: 0.097963\n",
      "Train Epoch: 53 [16/54 (30%)]\tTrain Loss: 0.042153\n",
      "Train Epoch: 53 [24/54 (44%)]\tTrain Loss: 0.051223\n",
      "Train Epoch: 53 [32/54 (59%)]\tTrain Loss: 0.049843\n",
      "Train Epoch: 53 [40/54 (74%)]\tTrain Loss: 0.048224\n",
      "Train Epoch: 53 [48/54 (89%)]\tTrain Loss: 0.061334\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.98353541 0.99819297 0.9977023  0.89583874 0.86506462 0.85980743\n",
      " 0.98764825 0.8727169  0.73292398 0.3635954  0.32759336 0.39761445\n",
      " 0.59386957 0.70369071 0.78584206 0.61643368 0.28457132 0.48204657\n",
      " 0.44507596 0.47999617 0.47487119 0.56295633 0.71920502 0.69945037\n",
      " 0.3851997  0.81533206 0.95451987 0.56285238 0.69445378 0.61244184\n",
      " 0.92074668 0.98909503 0.98902994 0.39779744 0.474397   0.43989304\n",
      " 0.38783917 0.31164807 0.47502837 0.48162895 0.61471504 0.36931595\n",
      " 0.38401547 0.78222102 0.89678276 0.98835468 0.99976522 0.9999218\n",
      " 0.99976903 0.90046126 0.99988329 0.16926549 0.46244478 0.54943252\n",
      " 0.56218195 0.22848389 0.89125669 0.28872424 0.35047847 0.92130464\n",
      " 0.97959501 0.89409208 0.96583742 0.98599768 0.75375533 0.80552107\n",
      " 0.72659314 0.8600359  0.88168544 0.80148715 0.98959434 0.98903745\n",
      " 0.98321736 0.98062474 0.69469857 0.97678316 0.99988592 0.99991918\n",
      " 0.99712831 0.99965084 0.99986422 0.99991715 0.99994099 0.59480923\n",
      " 0.46572632 0.96981847 0.99040902 0.99375206 0.52472836 0.42198929\n",
      " 0.80805004 0.9576723  0.97491664 0.83691198 0.84350556 0.63799089\n",
      " 0.30937538 0.77477717 0.46436939 0.63210344 0.63020509 0.99819344\n",
      " 0.38370055 0.89961946 0.91676503 0.97056997 0.96794003 0.31967676\n",
      " 0.4295983  0.28950703 0.3926875  0.56498224 0.52346802 0.51036227\n",
      " 0.56307989 0.47801352 0.64406133 0.6786924 ]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 54 [0/54 (0%)]\tTrain Loss: 0.081429\n",
      "Train Epoch: 54 [8/54 (15%)]\tTrain Loss: 0.068270\n",
      "Train Epoch: 54 [16/54 (30%)]\tTrain Loss: 0.088075\n",
      "Train Epoch: 54 [24/54 (44%)]\tTrain Loss: 0.096423\n",
      "Train Epoch: 54 [32/54 (59%)]\tTrain Loss: 0.067703\n",
      "Train Epoch: 54 [40/54 (74%)]\tTrain Loss: 0.059586\n",
      "Train Epoch: 54 [48/54 (89%)]\tTrain Loss: 0.073379\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.99895966 0.99962723 0.99967074 0.99245048 0.99440211 0.99688065\n",
      " 0.99857581 0.99616688 0.99231499 0.64986414 0.72495848 0.5796262\n",
      " 0.86399001 0.97626698 0.98964792 0.91432959 0.68411696 0.87792492\n",
      " 0.56983519 0.53998661 0.65292799 0.61395276 0.91701698 0.96583909\n",
      " 0.49413115 0.93228185 0.99366474 0.8570413  0.80647993 0.71894652\n",
      " 0.99268615 0.97980314 0.92475563 0.58763075 0.59046775 0.62296695\n",
      " 0.58247012 0.77468616 0.85354912 0.88298035 0.93622261 0.81976718\n",
      " 0.80760682 0.93554705 0.96294284 0.99806315 0.99999774 0.99999166\n",
      " 0.99999404 0.99847442 0.99999499 0.89237499 0.89953321 0.9976936\n",
      " 0.83976626 0.67719775 0.99020749 0.72105235 0.50238568 0.99782914\n",
      " 0.99965644 0.9939878  0.99931085 0.99972242 0.99674451 0.99221647\n",
      " 0.99587619 0.99740738 0.99825674 0.98264992 0.99992013 0.99981028\n",
      " 0.99606162 0.98153436 0.96817446 0.99737012 0.9999795  0.9999491\n",
      " 0.99997926 0.99991357 0.99997723 0.99995828 0.9999305  0.91627872\n",
      " 0.93176341 0.99767584 0.99838305 0.99928087 0.70006883 0.56123519\n",
      " 0.94721127 0.97651833 0.94683152 0.95549762 0.96879745 0.86120546\n",
      " 0.44919717 0.95614082 0.85242945 0.91194689 0.90985745 0.99997211\n",
      " 0.80316758 0.95560127 0.9531827  0.9837721  0.98806375 0.53825241\n",
      " 0.67138124 0.69513923 0.56083322 0.75843072 0.86528128 0.83105206\n",
      " 0.90130442 0.88934237 0.70168418 0.71006805]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 55 [0/54 (0%)]\tTrain Loss: 0.062052\n",
      "Train Epoch: 55 [8/54 (15%)]\tTrain Loss: 0.050251\n",
      "Train Epoch: 55 [16/54 (30%)]\tTrain Loss: 0.089219\n",
      "Train Epoch: 55 [24/54 (44%)]\tTrain Loss: 0.052641\n",
      "Train Epoch: 55 [32/54 (59%)]\tTrain Loss: 0.072716\n",
      "Train Epoch: 55 [40/54 (74%)]\tTrain Loss: 0.072739\n",
      "Train Epoch: 55 [48/54 (89%)]\tTrain Loss: 0.101024\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.6533066  0.77566147 0.64979815 0.59448123 0.55353069 0.70273083\n",
      " 0.71535802 0.89682406 0.68088502 0.28475231 0.46668223 0.39215186\n",
      " 0.61683357 0.80402476 0.53099686 0.45786339 0.63445377 0.29672334\n",
      " 0.29644024 0.43492734 0.52463955 0.2496139  0.68757069 0.84740496\n",
      " 0.25766754 0.88718814 0.93492657 0.57917446 0.404515   0.25916946\n",
      " 0.62394464 0.38622713 0.44138661 0.33693171 0.45854017 0.42692766\n",
      " 0.45473072 0.29255977 0.40962324 0.5551005  0.52726769 0.55338365\n",
      " 0.21432132 0.55095243 0.50721151 0.73483056 0.84682482 0.79720014\n",
      " 0.92536759 0.89119101 0.95225734 0.3240563  0.66966969 0.91498721\n",
      " 0.61340177 0.47359025 0.80514753 0.65287733 0.32166156 0.70026445\n",
      " 0.89209998 0.74119592 0.87003112 0.78129667 0.8160907  0.78763664\n",
      " 0.70156711 0.86473191 0.8444643  0.61636353 0.88321602 0.87089384\n",
      " 0.60519314 0.67385221 0.68536103 0.76835608 0.80289459 0.71558523\n",
      " 0.9172141  0.80906856 0.85842174 0.93949372 0.95893103 0.65425408\n",
      " 0.76738054 0.52631629 0.8759684  0.92004311 0.557491   0.44138369\n",
      " 0.81158614 0.75324953 0.61112654 0.92910147 0.81509835 0.76278961\n",
      " 0.37157127 0.39376637 0.4672679  0.5398038  0.6405825  0.75607711\n",
      " 0.47178146 0.81108886 0.54662693 0.77390939 0.77013308 0.38255343\n",
      " 0.48917174 0.50031132 0.45985618 0.30880865 0.74733776 0.61008358\n",
      " 0.39789587 0.36864522 0.24724367 0.37655067]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 56 [0/54 (0%)]\tTrain Loss: 0.074844\n",
      "Train Epoch: 56 [8/54 (15%)]\tTrain Loss: 0.078175\n",
      "Train Epoch: 56 [16/54 (30%)]\tTrain Loss: 0.029946\n",
      "Train Epoch: 56 [24/54 (44%)]\tTrain Loss: 0.099228\n",
      "Train Epoch: 56 [32/54 (59%)]\tTrain Loss: 0.099373\n",
      "Train Epoch: 56 [40/54 (74%)]\tTrain Loss: 0.077927\n",
      "Train Epoch: 56 [48/54 (89%)]\tTrain Loss: 0.047038\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.99710757 0.99999952 0.99999607 0.98536927 0.94751906 0.93881238\n",
      " 0.99999833 0.99253309 0.86109072 0.14311214 0.23129186 0.19605529\n",
      " 0.39918038 0.89985353 0.97691458 0.62622356 0.40288466 0.57278115\n",
      " 0.51682848 0.56118292 0.68384248 0.63812989 0.93198025 0.93174422\n",
      " 0.49842662 0.89214754 0.98459744 0.69193989 0.55332625 0.54466933\n",
      " 0.95359862 0.97683299 0.99830627 0.28895864 0.38745624 0.43038145\n",
      " 0.3715969  0.39462337 0.46265799 0.45811296 0.58214295 0.38153282\n",
      " 0.31749898 0.78547782 0.9339028  0.96328515 0.99994385 0.99996746\n",
      " 0.99996245 0.95181    0.99996805 0.13949203 0.45918176 0.66533625\n",
      " 0.4295072  0.31775954 0.8983804  0.40264812 0.32044047 0.9808411\n",
      " 0.99000031 0.97399569 0.99420542 0.9922809  0.92542422 0.92715907\n",
      " 0.88930547 0.95041186 0.98998308 0.60997427 0.99513006 0.99356234\n",
      " 0.97321767 0.98901409 0.83372205 0.99794012 1.         1.\n",
      " 0.99999082 0.99944288 0.99974912 0.99990165 0.99995673 0.66438693\n",
      " 0.72826213 0.83503658 0.99047041 0.99418384 0.60254329 0.38253483\n",
      " 0.85189229 0.96252227 0.94338268 0.88934094 0.84875309 0.70006287\n",
      " 0.30177364 0.75516689 0.48647836 0.67826658 0.75892657 0.99733019\n",
      " 0.55331039 0.74323219 0.73886126 0.8856827  0.91945559 0.30733109\n",
      " 0.45313254 0.29739118 0.32610399 0.37539119 0.64440215 0.52632594\n",
      " 0.60924572 0.42893311 0.99727279 0.99720508]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 57 [0/54 (0%)]\tTrain Loss: 0.044661\n",
      "Train Epoch: 57 [8/54 (15%)]\tTrain Loss: 0.105411\n",
      "Train Epoch: 57 [16/54 (30%)]\tTrain Loss: 0.044900\n",
      "Train Epoch: 57 [24/54 (44%)]\tTrain Loss: 0.043982\n",
      "Train Epoch: 57 [32/54 (59%)]\tTrain Loss: 0.052844\n",
      "Train Epoch: 57 [40/54 (74%)]\tTrain Loss: 0.086408\n",
      "Train Epoch: 57 [48/54 (89%)]\tTrain Loss: 0.075187\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.20453554 0.58806288 0.57465678 0.16638719 0.11206256 0.09372883\n",
      " 0.51205361 0.21498965 0.05759363 0.10855343 0.12273726 0.18415135\n",
      " 0.13917109 0.44471911 0.44745305 0.09551604 0.10012529 0.12036388\n",
      " 0.22099675 0.36838022 0.51686794 0.23397087 0.62917554 0.78219396\n",
      " 0.30739361 0.75094151 0.81669897 0.43380755 0.25918183 0.29534781\n",
      " 0.53578138 0.39799625 0.39582115 0.10714186 0.14616534 0.16624294\n",
      " 0.20668079 0.15040801 0.18161707 0.22778523 0.26179132 0.19263981\n",
      " 0.10732549 0.1773905  0.2108722  0.35078609 0.36545494 0.40575099\n",
      " 0.78868848 0.30909061 0.75104338 0.03824554 0.16153067 0.14158408\n",
      " 0.23255962 0.15612222 0.51285893 0.26873338 0.25246769 0.23455074\n",
      " 0.46575087 0.38051271 0.52338469 0.4948746  0.4581331  0.44142845\n",
      " 0.28784502 0.68501145 0.50071174 0.1075303  0.20566845 0.18873926\n",
      " 0.52130204 0.38920876 0.36888608 0.35037479 0.57951403 0.55072433\n",
      " 0.45471686 0.55732036 0.65035176 0.6794675  0.83556873 0.53591168\n",
      " 0.62463701 0.54059917 0.68798673 0.71814811 0.51589859 0.37698314\n",
      " 0.71649927 0.17915519 0.21313839 0.84020305 0.62772077 0.66227061\n",
      " 0.24447501 0.16199926 0.14711729 0.27457219 0.21306363 0.37620768\n",
      " 0.11575404 0.13905562 0.10898918 0.2063572  0.30552173 0.168025\n",
      " 0.20670554 0.12189245 0.12240992 0.09043083 0.19275627 0.3913911\n",
      " 0.34720227 0.17243446 0.3061125  0.34830663]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 58 [0/54 (0%)]\tTrain Loss: 0.043107\n",
      "Train Epoch: 58 [8/54 (15%)]\tTrain Loss: 0.049321\n",
      "Train Epoch: 58 [16/54 (30%)]\tTrain Loss: 0.115298\n",
      "Train Epoch: 58 [24/54 (44%)]\tTrain Loss: 0.045463\n",
      "Train Epoch: 58 [32/54 (59%)]\tTrain Loss: 0.106719\n",
      "Train Epoch: 58 [40/54 (74%)]\tTrain Loss: 0.075465\n",
      "Train Epoch: 58 [48/54 (89%)]\tTrain Loss: 0.053024\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.48498848 0.95675075 0.89858538 0.90428454 0.84341574 0.35547706\n",
      " 0.96353102 0.98199272 0.63091248 0.06553812 0.31065273 0.08346005\n",
      " 0.58797914 0.89427567 0.851942   0.82797199 0.93274814 0.58392161\n",
      " 0.71322411 0.52803785 0.54217881 0.81200826 0.89299375 0.97069329\n",
      " 0.58984464 0.9377436  0.92553204 0.73831838 0.73883301 0.57151097\n",
      " 0.67400217 0.73551661 0.81510943 0.41518414 0.3745926  0.55348039\n",
      " 0.54002047 0.86553425 0.603903   0.75815755 0.73843235 0.74070889\n",
      " 0.23416229 0.95492345 0.908907   0.58938003 0.93668145 0.9535324\n",
      " 0.8934533  0.54800409 0.92600429 0.34679562 0.48990077 0.83561337\n",
      " 0.1543978  0.38028294 0.88213181 0.3690753  0.22663935 0.75273699\n",
      " 0.88238275 0.80548179 0.87553519 0.78848511 0.85578072 0.94939852\n",
      " 0.88782114 0.9737466  0.96019882 0.45931822 0.85417593 0.85553533\n",
      " 0.67216396 0.56092113 0.68043631 0.93862993 0.87565309 0.84029096\n",
      " 0.95932859 0.88741612 0.85041326 0.74811351 0.91030973 0.82211065\n",
      " 0.88044435 0.6891849  0.95140606 0.94333339 0.82992917 0.75940382\n",
      " 0.90755272 0.78462011 0.77859533 0.97128987 0.89464152 0.64065754\n",
      " 0.26992264 0.69215029 0.47593257 0.79865348 0.72471815 0.96357387\n",
      " 0.74807423 0.61454511 0.19209677 0.28578064 0.39274901 0.42433074\n",
      " 0.45708305 0.39831838 0.30092576 0.53526872 0.46475625 0.66379493\n",
      " 0.64805478 0.66098809 0.80446488 0.95814931]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 59 [0/54 (0%)]\tTrain Loss: 0.063658\n",
      "Train Epoch: 59 [8/54 (15%)]\tTrain Loss: 0.051493\n",
      "Train Epoch: 59 [16/54 (30%)]\tTrain Loss: 0.055331\n",
      "Train Epoch: 59 [24/54 (44%)]\tTrain Loss: 0.027510\n",
      "Train Epoch: 59 [32/54 (59%)]\tTrain Loss: 0.053437\n",
      "Train Epoch: 59 [40/54 (74%)]\tTrain Loss: 0.066316\n",
      "Train Epoch: 59 [48/54 (89%)]\tTrain Loss: 0.052650\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.74970627 0.95550156 0.96128827 0.78511578 0.79257447 0.39516538\n",
      " 0.93476373 0.95547891 0.44228375 0.0346236  0.3838824  0.15463045\n",
      " 0.5563212  0.85354972 0.93213964 0.80066645 0.54936391 0.27458122\n",
      " 0.18603155 0.38750383 0.31793898 0.37313902 0.69806445 0.86306202\n",
      " 0.34292725 0.83578771 0.91388184 0.89747673 0.52826798 0.67861259\n",
      " 0.99071395 0.96779591 0.89403689 0.23660342 0.59944797 0.5371995\n",
      " 0.43667996 0.50125754 0.79862118 0.64504302 0.68977022 0.52693343\n",
      " 0.24197459 0.63770235 0.78245372 0.88021839 0.96752155 0.95804876\n",
      " 0.96441668 0.89836258 0.98325664 0.18071984 0.84198701 0.96623021\n",
      " 0.10507748 0.50608575 0.94872206 0.53432667 0.57692194 0.92994326\n",
      " 0.89348125 0.64131981 0.96329391 0.96156228 0.96757388 0.94608134\n",
      " 0.91701168 0.97496164 0.94359058 0.82720238 0.81240344 0.82265097\n",
      " 0.96509016 0.96029937 0.93638295 0.95950592 0.98329759 0.98115873\n",
      " 0.98238534 0.98510891 0.98510426 0.98993474 0.98862004 0.84559393\n",
      " 0.64472151 0.86146528 0.78652364 0.92443031 0.67945868 0.451314\n",
      " 0.48699632 0.85689455 0.85955495 0.77903605 0.89498782 0.52986819\n",
      " 0.24715354 0.68869537 0.25088418 0.57648039 0.54500407 0.98290986\n",
      " 0.61544037 0.63480377 0.65541279 0.53759319 0.84568542 0.46331567\n",
      " 0.42524377 0.12476072 0.33265689 0.45450464 0.57915395 0.60066473\n",
      " 0.48530054 0.61096132 0.51308805 0.52895695]\n",
      "predict [1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 60 [0/54 (0%)]\tTrain Loss: 0.034931\n",
      "Train Epoch: 60 [8/54 (15%)]\tTrain Loss: 0.091091\n",
      "Train Epoch: 60 [16/54 (30%)]\tTrain Loss: 0.063056\n",
      "Train Epoch: 60 [48/54 (89%)]\tTrain Loss: 0.060611\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.35726297 0.92816573 0.90571839 0.66526514 0.68053168 0.35048354\n",
      " 0.93375796 0.8559854  0.24902336 0.11171889 0.50750339 0.17693636\n",
      " 0.70562768 0.81782269 0.86945385 0.41358575 0.30350149 0.19064078\n",
      " 0.28749248 0.42535499 0.50422704 0.35599574 0.68068832 0.81816089\n",
      " 0.35360676 0.83088195 0.87508738 0.73079747 0.450836   0.41095862\n",
      " 0.86330438 0.76433665 0.79391468 0.25854731 0.42680696 0.38273975\n",
      " 0.41890454 0.37642068 0.48350111 0.48049486 0.49000773 0.38246202\n",
      " 0.15241742 0.45701027 0.54981512 0.87528527 0.88369626 0.9424749\n",
      " 0.93862468 0.7992568  0.96270877 0.14267954 0.67622924 0.94200552\n",
      " 0.32628739 0.50306368 0.84746683 0.56875283 0.43412119 0.75611848\n",
      " 0.97701776 0.80145419 0.96592772 0.97802347 0.92991132 0.83167207\n",
      " 0.73389637 0.94439721 0.8673771  0.48942342 0.72431576 0.6659776\n",
      " 0.81557655 0.80309373 0.8408094  0.84990543 0.97681254 0.95655471\n",
      " 0.99113923 0.93978149 0.92248225 0.97379881 0.95937204 0.71250224\n",
      " 0.57851666 0.6490972  0.82168132 0.91600871 0.58865249 0.43484375\n",
      " 0.7052142  0.60590822 0.59350902 0.82635391 0.74796462 0.52900583\n",
      " 0.64216739 0.44068074 0.50934327 0.54400223 0.71866149 0.91254705\n",
      " 0.39390013 0.54831105 0.49536747 0.47637469 0.58764273 0.41420791\n",
      " 0.40045288 0.32389984 0.40063989 0.22702649 0.43598643 0.49304891\n",
      " 0.32945558 0.32347956 0.38418123 0.43845645]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "vote_pred [1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 47 TN= 25 FN= 11 FP= 35\n",
      "TP+FP 82\n",
      "precision 0.573170731707317\n",
      "recall 0.8103448275862069\n",
      "F1 0.6714285714285714\n",
      "acc 0.6101694915254238\n",
      "AUCp 0.6135057471264367\n",
      "AUC 0.632183908045977\n",
      "\n",
      " The epoch is 60, average recall: 0.8103, average precision: 0.5732,average F1: 0.6714, average accuracy: 0.6102, average AUC: 0.6322\n",
      "Train Epoch: 61 [0/54 (0%)]\tTrain Loss: 0.035011\n",
      "Train Epoch: 61 [8/54 (15%)]\tTrain Loss: 0.081929\n",
      "Train Epoch: 61 [16/54 (30%)]\tTrain Loss: 0.075745\n",
      "Train Epoch: 61 [24/54 (44%)]\tTrain Loss: 0.072142\n",
      "Train Epoch: 61 [32/54 (59%)]\tTrain Loss: 0.073795\n",
      "Train Epoch: 61 [40/54 (74%)]\tTrain Loss: 0.057295\n",
      "Train Epoch: 61 [48/54 (89%)]\tTrain Loss: 0.096254\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.1241462  0.74537176 0.85880941 0.1494845  0.08107749 0.06291456\n",
      " 0.73839819 0.4240495  0.06480282 0.01038029 0.05706878 0.03129333\n",
      " 0.07900317 0.72961515 0.66032118 0.0867122  0.1781626  0.07120597\n",
      " 0.13277371 0.41021883 0.63477117 0.25978446 0.75867939 0.90404058\n",
      " 0.32718071 0.91413319 0.95758903 0.72671402 0.31808308 0.24435137\n",
      " 0.57088399 0.4101212  0.57242596 0.05015459 0.0806052  0.18582445\n",
      " 0.12704682 0.11056716 0.15626726 0.4407562  0.43855754 0.35707638\n",
      " 0.02907513 0.12827031 0.21152432 0.32045266 0.14667107 0.45219243\n",
      " 0.8844496  0.60931194 0.85931683 0.00668062 0.23985486 0.04931274\n",
      " 0.26623255 0.14666907 0.44945669 0.29802325 0.12764342 0.51266414\n",
      " 0.43329254 0.5996381  0.56427795 0.57004225 0.80938226 0.71509123\n",
      " 0.30644566 0.9079932  0.6684199  0.1012251  0.25406316 0.2286436\n",
      " 0.69195992 0.68407077 0.51630586 0.37483937 0.77213806 0.81978834\n",
      " 0.8157022  0.63099283 0.69033426 0.79805982 0.92712021 0.43447205\n",
      " 0.55519521 0.2858986  0.71039647 0.82285738 0.86874753 0.50411773\n",
      " 0.94284701 0.09953142 0.15331839 0.95460099 0.82389742 0.59469908\n",
      " 0.23192428 0.20034868 0.11388651 0.5338642  0.17936146 0.28207305\n",
      " 0.21764082 0.03541769 0.03454534 0.11792473 0.25303558 0.2611106\n",
      " 0.28623027 0.08590207 0.24974783 0.36699194 0.57947785 0.49480599\n",
      " 0.54222667 0.19441147 0.53141207 0.55524927]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.]\n",
      "Train Epoch: 62 [0/54 (0%)]\tTrain Loss: 0.052961\n",
      "Train Epoch: 62 [8/54 (15%)]\tTrain Loss: 0.150761\n",
      "Train Epoch: 62 [16/54 (30%)]\tTrain Loss: 0.043691\n",
      "Train Epoch: 62 [24/54 (44%)]\tTrain Loss: 0.082588\n",
      "Train Epoch: 62 [32/54 (59%)]\tTrain Loss: 0.040241\n",
      "Train Epoch: 62 [40/54 (74%)]\tTrain Loss: 0.065634\n",
      "Train Epoch: 62 [48/54 (89%)]\tTrain Loss: 0.041937\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.52516109 0.9996959  0.9994992  0.8456924  0.70179772 0.28323933\n",
      " 0.99916077 0.89673519 0.25727352 0.0107644  0.44918099 0.01909337\n",
      " 0.58351696 0.85360658 0.94612175 0.46118733 0.24256708 0.46643758\n",
      " 0.28889322 0.42109221 0.60471636 0.56044191 0.87442386 0.91369468\n",
      " 0.49622884 0.885629   0.93470395 0.73312306 0.54020149 0.54472774\n",
      " 0.93716711 0.94555891 0.96723783 0.11963837 0.31806073 0.37329912\n",
      " 0.41783538 0.34294066 0.51926929 0.51090491 0.66923982 0.4209857\n",
      " 0.20198195 0.64249921 0.75784028 0.97396129 0.99504542 0.9991824\n",
      " 0.99835628 0.63861382 0.99085265 0.05826719 0.57598615 0.77353269\n",
      " 0.29309401 0.26879016 0.95206094 0.44557735 0.17886171 0.95808804\n",
      " 0.99634188 0.98295903 0.99624866 0.99778277 0.9743672  0.87713248\n",
      " 0.67906636 0.97152442 0.97922224 0.46659052 0.83950233 0.79179776\n",
      " 0.96997899 0.97619265 0.94001204 0.98586386 0.9999733  0.99995232\n",
      " 0.99969149 0.997733   0.99906963 0.99931443 0.99819845 0.77740818\n",
      " 0.76775229 0.90216744 0.98110664 0.99087369 0.76601303 0.53266031\n",
      " 0.8306002  0.74004257 0.84924823 0.92647076 0.71261579 0.61113316\n",
      " 0.20952886 0.79898506 0.62832999 0.69588113 0.71264803 0.98162615\n",
      " 0.49684957 0.75123179 0.58308083 0.5979017  0.64726144 0.53896052\n",
      " 0.58801466 0.37870353 0.47262979 0.36047891 0.49576139 0.64144075\n",
      " 0.5454846  0.5451256  0.90657073 0.88431132]\n",
      "predict [1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 63 [0/54 (0%)]\tTrain Loss: 0.049315\n",
      "Train Epoch: 63 [8/54 (15%)]\tTrain Loss: 0.062969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 63 [16/54 (30%)]\tTrain Loss: 0.110058\n",
      "Train Epoch: 63 [24/54 (44%)]\tTrain Loss: 0.084084\n",
      "Train Epoch: 63 [32/54 (59%)]\tTrain Loss: 0.082354\n",
      "Train Epoch: 63 [40/54 (74%)]\tTrain Loss: 0.032230\n",
      "Train Epoch: 63 [48/54 (89%)]\tTrain Loss: 0.065754\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.15697229 0.81911218 0.83907223 0.27145067 0.17810576 0.03051991\n",
      " 0.74608237 0.48189902 0.01382873 0.01978544 0.12338626 0.06424188\n",
      " 0.20366824 0.63381195 0.51683015 0.04234572 0.10628668 0.09094077\n",
      " 0.30046192 0.60335141 0.82451385 0.42113626 0.90777844 0.95925343\n",
      " 0.53990752 0.96799904 0.96015906 0.84172875 0.3763822  0.36337242\n",
      " 0.8177405  0.65137231 0.79438037 0.05628183 0.07591473 0.10345229\n",
      " 0.18174028 0.20870709 0.19652374 0.28330567 0.3062149  0.22571029\n",
      " 0.03831265 0.21610993 0.45576528 0.48924789 0.13048677 0.40794474\n",
      " 0.91880262 0.49160355 0.83254683 0.00705167 0.38817248 0.15428576\n",
      " 0.26930368 0.31528106 0.74551284 0.3711763  0.17665368 0.62200624\n",
      " 0.64115793 0.55856967 0.7706337  0.74776173 0.904652   0.76763904\n",
      " 0.30936936 0.94742149 0.77250582 0.06956005 0.18090567 0.12092999\n",
      " 0.80889881 0.71714866 0.8161009  0.64160025 0.83530092 0.78107297\n",
      " 0.85330981 0.83472347 0.88535756 0.93088883 0.96769005 0.83068305\n",
      " 0.8666811  0.55741978 0.89143896 0.93352664 0.85289884 0.36764407\n",
      " 0.94138706 0.35947061 0.3948971  0.98365384 0.75469804 0.84009153\n",
      " 0.31709504 0.1882451  0.10435359 0.44555059 0.27000234 0.3162857\n",
      " 0.14583628 0.04734267 0.05441015 0.06938628 0.24934919 0.43178123\n",
      " 0.46908852 0.09679127 0.21872747 0.47262645 0.36098933 0.6747601\n",
      " 0.10503131 0.10247413 0.68887365 0.51816624]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 64 [0/54 (0%)]\tTrain Loss: 0.062633\n",
      "Train Epoch: 64 [8/54 (15%)]\tTrain Loss: 0.031046\n",
      "Train Epoch: 64 [16/54 (30%)]\tTrain Loss: 0.136508\n",
      "Train Epoch: 64 [24/54 (44%)]\tTrain Loss: 0.043801\n",
      "Train Epoch: 64 [32/54 (59%)]\tTrain Loss: 0.093321\n",
      "Train Epoch: 64 [40/54 (74%)]\tTrain Loss: 0.034185\n",
      "Train Epoch: 64 [48/54 (89%)]\tTrain Loss: 0.057025\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.89568388 0.94619679 0.90176815 0.79577821 0.64579767 0.71601951\n",
      " 0.88431799 0.96196663 0.49532133 0.07067841 0.30999181 0.10691161\n",
      " 0.25293881 0.85259473 0.7712788  0.44328836 0.79211015 0.37346593\n",
      " 0.37835339 0.44370157 0.5763247  0.47110978 0.55540228 0.96450549\n",
      " 0.38209519 0.7498709  0.96941739 0.43136069 0.44021574 0.24964616\n",
      " 0.55775636 0.45498595 0.42633736 0.44689474 0.37948936 0.40889141\n",
      " 0.32074398 0.35892928 0.52077425 0.52045071 0.55336821 0.52218312\n",
      " 0.499268   0.47474432 0.4401823  0.62174761 0.74128538 0.85626471\n",
      " 0.95725274 0.9633885  0.97665262 0.23088507 0.66518748 0.82062858\n",
      " 0.5182097  0.32160446 0.86146468 0.33358708 0.28394753 0.55369282\n",
      " 0.94090813 0.87382609 0.94713479 0.87042278 0.89167392 0.67308849\n",
      " 0.66084021 0.78298014 0.91359574 0.8057059  0.95387948 0.92595154\n",
      " 0.73920345 0.7823121  0.76220626 0.94195926 0.95822465 0.93872195\n",
      " 0.94626951 0.84526199 0.87251502 0.92564207 0.98054928 0.86568391\n",
      " 0.69145453 0.81281245 0.83453679 0.90254599 0.51518202 0.47231743\n",
      " 0.87855679 0.84331453 0.65110475 0.8925482  0.89622271 0.59811956\n",
      " 0.47053519 0.65707076 0.61675751 0.70617551 0.77334732 0.85669184\n",
      " 0.42045167 0.83420974 0.63183641 0.76697183 0.89200884 0.45464262\n",
      " 0.39815888 0.23624849 0.33580744 0.56301588 0.58521926 0.5295679\n",
      " 0.45431298 0.41270569 0.46512297 0.64142209]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1.]\n",
      "Train Epoch: 65 [0/54 (0%)]\tTrain Loss: 0.050053\n",
      "Train Epoch: 65 [8/54 (15%)]\tTrain Loss: 0.065488\n",
      "Train Epoch: 65 [16/54 (30%)]\tTrain Loss: 0.074413\n",
      "Train Epoch: 65 [24/54 (44%)]\tTrain Loss: 0.075149\n",
      "Train Epoch: 65 [32/54 (59%)]\tTrain Loss: 0.043406\n",
      "Train Epoch: 65 [40/54 (74%)]\tTrain Loss: 0.100036\n",
      "Train Epoch: 65 [48/54 (89%)]\tTrain Loss: 0.067125\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.99985003 0.99962091 0.99920589 0.99306726 0.99253911 0.98136514\n",
      " 0.99918634 0.99270868 0.98708981 0.49904859 0.86974204 0.66434932\n",
      " 0.87003762 0.90301788 0.98713791 0.81638891 0.42140263 0.96839172\n",
      " 0.97443163 0.96831942 0.97757727 0.98890764 0.99857509 0.99849296\n",
      " 0.97905695 0.99244881 0.99922812 0.9743827  0.98117709 0.9813751\n",
      " 0.99969041 0.99986994 0.99998641 0.68276095 0.7268582  0.86519492\n",
      " 0.83035475 0.96787977 0.91166741 0.9033727  0.95726097 0.90401149\n",
      " 0.96326977 0.98919642 0.99242264 0.99929261 0.99999523 0.99998581\n",
      " 0.99999666 0.99608201 0.99999249 0.55458444 0.97944826 0.96933705\n",
      " 0.87511879 0.78824145 0.99847502 0.73062253 0.87538993 0.99646509\n",
      " 0.99952173 0.9979735  0.99975461 0.99990094 0.99794012 0.99399251\n",
      " 0.99445361 0.99757797 0.99783212 0.99138379 0.99916553 0.99873239\n",
      " 0.99991786 0.999668   0.9937616  0.99995792 0.99999821 0.9999969\n",
      " 0.99995112 0.99998832 0.99999356 0.99999464 0.99999189 0.99124968\n",
      " 0.99725002 0.99947971 0.99972552 0.99983406 0.88240272 0.784711\n",
      " 0.98293614 0.998173   0.99571627 0.96879119 0.98812985 0.88903576\n",
      " 0.87616628 0.98791319 0.94311488 0.94601893 0.98530614 0.99998271\n",
      " 0.82774985 0.9872629  0.9693675  0.98863363 0.99799287 0.85597473\n",
      " 0.80764991 0.8178404  0.80955178 0.71962184 0.79133856 0.79824847\n",
      " 0.74843419 0.93904561 0.98969787 0.99415511]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 66 [0/54 (0%)]\tTrain Loss: 0.077843\n",
      "Train Epoch: 66 [8/54 (15%)]\tTrain Loss: 0.046795\n",
      "Train Epoch: 66 [16/54 (30%)]\tTrain Loss: 0.041105\n",
      "Train Epoch: 66 [24/54 (44%)]\tTrain Loss: 0.061562\n",
      "Train Epoch: 66 [32/54 (59%)]\tTrain Loss: 0.067778\n",
      "Train Epoch: 66 [40/54 (74%)]\tTrain Loss: 0.055272\n",
      "Train Epoch: 66 [48/54 (89%)]\tTrain Loss: 0.062110\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.61800295 0.98303115 0.97271878 0.94643104 0.79633766 0.5357374\n",
      " 0.97391337 0.99623883 0.36037198 0.05767597 0.32593548 0.05121843\n",
      " 0.16156183 0.54296696 0.57395244 0.11662517 0.21415061 0.24564379\n",
      " 0.08769861 0.62413281 0.29129937 0.59867084 0.75343663 0.90844887\n",
      " 0.31995195 0.87938434 0.92288518 0.48011342 0.47182545 0.45523289\n",
      " 0.41252851 0.65401202 0.62025291 0.02222217 0.08208607 0.11205105\n",
      " 0.08058565 0.52187783 0.47657856 0.42478603 0.6043005  0.36639789\n",
      " 0.75324571 0.67225361 0.66990542 0.78561527 0.9249149  0.96708572\n",
      " 0.98515654 0.91567343 0.9905979  0.03077074 0.36981165 0.28451678\n",
      " 0.49102044 0.17224886 0.94320238 0.27331528 0.0496906  0.66215909\n",
      " 0.98144424 0.95145124 0.97984731 0.96337366 0.95293134 0.66892046\n",
      " 0.6226142  0.95323789 0.9706614  0.49522701 0.98222029 0.9715336\n",
      " 0.76825148 0.86664408 0.670241   0.96581119 0.99419737 0.99246538\n",
      " 0.9915114  0.96639055 0.97006446 0.97899878 0.99269682 0.82441044\n",
      " 0.7021637  0.79922116 0.96653384 0.97977084 0.19600619 0.16082151\n",
      " 0.75404501 0.70083088 0.64624685 0.97924155 0.61205137 0.35160601\n",
      " 0.17390858 0.52273661 0.31233579 0.70350021 0.53650081 0.91558027\n",
      " 0.20461464 0.82875592 0.37165594 0.82528615 0.94745058 0.21093751\n",
      " 0.27146342 0.18088771 0.11924056 0.11253375 0.34559104 0.32694617\n",
      " 0.26500845 0.14860855 0.53256363 0.61279374]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 67 [0/54 (0%)]\tTrain Loss: 0.037440\n",
      "Train Epoch: 67 [8/54 (15%)]\tTrain Loss: 0.075671\n",
      "Train Epoch: 67 [16/54 (30%)]\tTrain Loss: 0.096446\n",
      "Train Epoch: 67 [24/54 (44%)]\tTrain Loss: 0.074441\n",
      "Train Epoch: 67 [32/54 (59%)]\tTrain Loss: 0.051559\n",
      "Train Epoch: 67 [40/54 (74%)]\tTrain Loss: 0.112061\n",
      "Train Epoch: 67 [48/54 (89%)]\tTrain Loss: 0.052885\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.08166344 0.63951939 0.54161572 0.29413256 0.06498789 0.04012899\n",
      " 0.55733788 0.55844176 0.04393854 0.00602962 0.08469211 0.01839551\n",
      " 0.09754945 0.38229799 0.3625885  0.14647442 0.15375845 0.12105227\n",
      " 0.08300297 0.32820663 0.43912181 0.16571601 0.75431001 0.7598058\n",
      " 0.20125856 0.83069187 0.90716165 0.6497488  0.2836802  0.20326523\n",
      " 0.74959713 0.71263802 0.57057095 0.04004676 0.09753447 0.14889501\n",
      " 0.09476433 0.19557786 0.3093445  0.60830414 0.59465688 0.60345161\n",
      " 0.06141978 0.21166541 0.29990083 0.57769412 0.55009764 0.64308345\n",
      " 0.93385226 0.71206182 0.92199194 0.0254005  0.27391854 0.446816\n",
      " 0.49040493 0.09788083 0.89505941 0.22331768 0.17159516 0.24533714\n",
      " 0.84829247 0.79094577 0.87304771 0.82535583 0.85029566 0.65278506\n",
      " 0.47138536 0.84503675 0.8074314  0.19301203 0.73701322 0.63517159\n",
      " 0.8907097  0.75265145 0.69552475 0.69567198 0.70629054 0.66533911\n",
      " 0.88890904 0.93648946 0.95378309 0.96554226 0.98936886 0.46992671\n",
      " 0.5777629  0.7378968  0.84455812 0.90922034 0.5365293  0.27879456\n",
      " 0.85518456 0.142101   0.29879692 0.90090072 0.83081716 0.38312256\n",
      " 0.24671814 0.33675459 0.29915804 0.65402716 0.21832512 0.72827369\n",
      " 0.1863056  0.22968811 0.08650725 0.62326366 0.77311474 0.17068392\n",
      " 0.3976872  0.12561265 0.14776284 0.33090162 0.55112559 0.56383371\n",
      " 0.30981365 0.33864534 0.30791941 0.33605415]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      "Train Epoch: 68 [0/54 (0%)]\tTrain Loss: 0.056954\n",
      "Train Epoch: 68 [8/54 (15%)]\tTrain Loss: 0.037175\n",
      "Train Epoch: 68 [16/54 (30%)]\tTrain Loss: 0.036919\n",
      "Train Epoch: 68 [24/54 (44%)]\tTrain Loss: 0.040683\n",
      "Train Epoch: 68 [32/54 (59%)]\tTrain Loss: 0.085436\n",
      "Train Epoch: 68 [40/54 (74%)]\tTrain Loss: 0.052711\n",
      "Train Epoch: 68 [48/54 (89%)]\tTrain Loss: 0.079349\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.99626899 0.99943537 0.99828255 0.98886263 0.96757632 0.84810144\n",
      " 0.99766123 0.99725419 0.74410981 0.32320452 0.4009662  0.59377384\n",
      " 0.34305164 0.92647362 0.96100217 0.52392578 0.50188637 0.43837935\n",
      " 0.26820248 0.48403659 0.58876926 0.55891967 0.95534801 0.98668736\n",
      " 0.51615179 0.97147691 0.99585426 0.82890373 0.55053401 0.75486451\n",
      " 0.9924159  0.98613364 0.98845649 0.16924295 0.32568106 0.41256848\n",
      " 0.32277927 0.55486214 0.50208014 0.57642031 0.5790841  0.59186852\n",
      " 0.38213965 0.669016   0.84826028 0.98592573 0.99949956 0.99920291\n",
      " 0.9998436  0.98577648 0.99983227 0.23185334 0.57011616 0.88161349\n",
      " 0.56435961 0.43124107 0.9339838  0.43615726 0.59952444 0.97938287\n",
      " 0.99533421 0.97193009 0.99543142 0.99544549 0.99400628 0.97263247\n",
      " 0.95149779 0.99200904 0.9948349  0.71206635 0.99827576 0.9945122\n",
      " 0.99070287 0.9872247  0.89663321 0.99771047 0.9999826  0.999964\n",
      " 0.99955267 0.99924064 0.99946707 0.99971539 0.99985874 0.66290897\n",
      " 0.74920601 0.94199014 0.99379748 0.98315525 0.70297259 0.36885568\n",
      " 0.56243163 0.94793099 0.90849054 0.97645253 0.75907171 0.84189337\n",
      " 0.31514782 0.66238099 0.5494523  0.74624825 0.78466463 0.99826849\n",
      " 0.60888988 0.7021777  0.50297815 0.79005891 0.93514639 0.65289503\n",
      " 0.53481787 0.34338215 0.35704523 0.43429217 0.68383479 0.63402277\n",
      " 0.59903252 0.34486109 0.86718541 0.79722589]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 69 [0/54 (0%)]\tTrain Loss: 0.044781\n",
      "Train Epoch: 69 [8/54 (15%)]\tTrain Loss: 0.077769\n",
      "Train Epoch: 69 [16/54 (30%)]\tTrain Loss: 0.066103\n",
      "Train Epoch: 69 [24/54 (44%)]\tTrain Loss: 0.063793\n",
      "Train Epoch: 69 [32/54 (59%)]\tTrain Loss: 0.029573\n",
      "Train Epoch: 69 [40/54 (74%)]\tTrain Loss: 0.049854\n",
      "Train Epoch: 69 [48/54 (89%)]\tTrain Loss: 0.029494\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.7930941  0.98529202 0.98213792 0.83164006 0.84228128 0.82154226\n",
      " 0.97239417 0.9720329  0.43748221 0.06075687 0.15872203 0.08470038\n",
      " 0.4969506  0.45983753 0.36344308 0.25258139 0.43423495 0.58423764\n",
      " 0.24597307 0.48208532 0.67836887 0.74189812 0.98228526 0.97874188\n",
      " 0.41942865 0.95670134 0.98959035 0.91255409 0.44321656 0.16119865\n",
      " 0.95398927 0.98775989 0.95944136 0.15538131 0.25194752 0.16162856\n",
      " 0.1700208  0.39728123 0.3767108  0.5930813  0.56169212 0.64939517\n",
      " 0.3758381  0.39954376 0.40051538 0.88430339 0.99656749 0.99122745\n",
      " 0.99927694 0.96139532 0.99867672 0.11522051 0.25188294 0.90455997\n",
      " 0.64277732 0.22138476 0.96334112 0.35582271 0.14671965 0.47259039\n",
      " 0.99299169 0.97643787 0.99667907 0.99360162 0.98950887 0.96180177\n",
      " 0.8852151  0.9783749  0.996997   0.87234563 0.99787319 0.99435461\n",
      " 0.9938013  0.99735689 0.97403854 0.99722254 0.99544901 0.99200034\n",
      " 0.99762911 0.99850947 0.99953985 0.99976259 0.99989974 0.67460799\n",
      " 0.88888776 0.92846185 0.97460902 0.99209762 0.65480119 0.55113649\n",
      " 0.9133836  0.95370024 0.83996999 0.97865105 0.91035461 0.77992535\n",
      " 0.23819801 0.5790714  0.64683032 0.88605756 0.31584457 0.99697959\n",
      " 0.27423701 0.83448744 0.69486749 0.59163523 0.90112841 0.36636138\n",
      " 0.63050145 0.47961158 0.56057191 0.18721712 0.93553853 0.7172237\n",
      " 0.48656836 0.36829752 0.5124886  0.59156358]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 70 [0/54 (0%)]\tTrain Loss: 0.043339\n",
      "Train Epoch: 70 [8/54 (15%)]\tTrain Loss: 0.082825\n",
      "Train Epoch: 70 [16/54 (30%)]\tTrain Loss: 0.061743\n",
      "Train Epoch: 70 [24/54 (44%)]\tTrain Loss: 0.124547\n",
      "Train Epoch: 70 [32/54 (59%)]\tTrain Loss: 0.075968\n",
      "Train Epoch: 70 [40/54 (74%)]\tTrain Loss: 0.055615\n",
      "Train Epoch: 70 [48/54 (89%)]\tTrain Loss: 0.071865\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.14513537 0.98024851 0.98306483 0.45296651 0.33036155 0.0970033\n",
      " 0.97611225 0.84994787 0.02286641 0.01625819 0.07887176 0.02883149\n",
      " 0.06883125 0.74917495 0.64925838 0.05581896 0.06332449 0.13923855\n",
      " 0.26212382 0.41650802 0.63334608 0.50606537 0.86502236 0.79200828\n",
      " 0.49770632 0.85879087 0.82277215 0.67273259 0.31057286 0.27749756\n",
      " 0.6951347  0.70496875 0.93196017 0.06242507 0.1214748  0.11878856\n",
      " 0.11561944 0.13629656 0.14916492 0.26175177 0.38619673 0.18580686\n",
      " 0.07789715 0.17250767 0.22806044 0.7398293  0.89904225 0.94905478\n",
      " 0.99450928 0.43467712 0.98423612 0.01424901 0.28038535 0.24215974\n",
      " 0.29280186 0.08476389 0.6426037  0.08040431 0.08323474 0.5186038\n",
      " 0.87889594 0.80186838 0.90731323 0.8988018  0.88593656 0.46451789\n",
      " 0.19249362 0.98430091 0.91953111 0.03834184 0.86736768 0.82688022\n",
      " 0.91471279 0.85844105 0.85456079 0.84734946 0.99680281 0.99421185\n",
      " 0.99168128 0.96122831 0.99033469 0.99654227 0.99440503 0.43299216\n",
      " 0.68681085 0.57170546 0.88040549 0.91165143 0.50054044 0.32321075\n",
      " 0.69692433 0.41610071 0.35230544 0.91789192 0.56708401 0.69022584\n",
      " 0.25502279 0.23490262 0.12233597 0.73723567 0.1449917  0.73285866\n",
      " 0.13023654 0.11948036 0.07738354 0.22356862 0.1969887  0.23167007\n",
      " 0.46513882 0.06560516 0.14956909 0.3433556  0.44724563 0.70695382\n",
      " 0.12453624 0.12970966 0.88113827 0.77169126]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "vote_pred [1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 44 TN= 31 FN= 14 FP= 29\n",
      "TP+FP 73\n",
      "precision 0.6027397260273972\n",
      "recall 0.7586206896551724\n",
      "F1 0.6717557251908397\n",
      "acc 0.635593220338983\n",
      "AUCp 0.6376436781609195\n",
      "AUC 0.6764367816091954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The epoch is 70, average recall: 0.7586, average precision: 0.6027,average F1: 0.6718, average accuracy: 0.6356, average AUC: 0.6764\n",
      "Train Epoch: 71 [0/54 (0%)]\tTrain Loss: 0.052171\n",
      "Train Epoch: 71 [8/54 (15%)]\tTrain Loss: 0.030901\n",
      "Train Epoch: 71 [16/54 (30%)]\tTrain Loss: 0.075858\n",
      "Train Epoch: 71 [24/54 (44%)]\tTrain Loss: 0.076202\n",
      "Train Epoch: 71 [32/54 (59%)]\tTrain Loss: 0.092139\n",
      "Train Epoch: 71 [40/54 (74%)]\tTrain Loss: 0.023683\n",
      "Train Epoch: 71 [48/54 (89%)]\tTrain Loss: 0.033020\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.44998452 0.82947665 0.7667926  0.89168644 0.95302415 0.07387055\n",
      " 0.70009905 0.87950546 0.1259072  0.0572634  0.7399689  0.05813447\n",
      " 0.73216754 0.52628011 0.57443899 0.63326049 0.43694291 0.2018722\n",
      " 0.19600351 0.55426979 0.84245139 0.49900335 0.67542511 0.99686122\n",
      " 0.63805765 0.8896606  0.99709725 0.77666306 0.2990267  0.54037535\n",
      " 0.94632465 0.60336065 0.71418822 0.27612916 0.7877087  0.75559521\n",
      " 0.52230567 0.5864507  0.82047188 0.96121305 0.92159635 0.95810646\n",
      " 0.41901246 0.49486068 0.26661834 0.90681005 0.83479226 0.89287519\n",
      " 0.96679616 0.98248947 0.97770542 0.05393185 0.43094099 0.49104935\n",
      " 0.16559486 0.32801449 0.66814846 0.75096107 0.46882573 0.63798606\n",
      " 0.60428876 0.58589816 0.74682248 0.539226   0.92598361 0.82342529\n",
      " 0.60930026 0.91503596 0.96452391 0.88990611 0.75015444 0.43751609\n",
      " 0.92605662 0.8934176  0.94778371 0.52732265 0.91251767 0.88401496\n",
      " 0.73044074 0.9095149  0.95962191 0.97619694 0.99317718 0.39562181\n",
      " 0.63779372 0.81128085 0.75474912 0.89526033 0.85520977 0.93480885\n",
      " 0.99723935 0.93820804 0.62924218 0.99912542 0.93036705 0.95477432\n",
      " 0.59294969 0.77086776 0.33670279 0.99173248 0.92906934 0.77807641\n",
      " 0.65066999 0.78438187 0.45085526 0.48318714 0.73435414 0.90053105\n",
      " 0.88876033 0.23363364 0.92292649 0.74245292 0.95918137 0.93778437\n",
      " 0.15969574 0.07429156 0.97719723 0.68910569]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 72 [0/54 (0%)]\tTrain Loss: 0.075754\n",
      "Train Epoch: 72 [8/54 (15%)]\tTrain Loss: 0.049424\n",
      "Train Epoch: 72 [16/54 (30%)]\tTrain Loss: 0.096927\n",
      "Train Epoch: 72 [24/54 (44%)]\tTrain Loss: 0.053314\n",
      "Train Epoch: 72 [32/54 (59%)]\tTrain Loss: 0.066204\n",
      "Train Epoch: 72 [40/54 (74%)]\tTrain Loss: 0.054772\n",
      "Train Epoch: 72 [48/54 (89%)]\tTrain Loss: 0.083483\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.96969563 0.99934953 0.99857306 0.92597401 0.74341238 0.8841809\n",
      " 0.99824595 0.97648036 0.51239061 0.06166483 0.10687211 0.08856402\n",
      " 0.17907444 0.66583556 0.79409355 0.09789713 0.08344836 0.4861728\n",
      " 0.24323562 0.54713714 0.69482851 0.67106074 0.96872813 0.9774875\n",
      " 0.49761665 0.93902951 0.97667921 0.81228054 0.51505172 0.56211483\n",
      " 0.97699887 0.99109286 0.99740809 0.04740426 0.09538709 0.109519\n",
      " 0.13507991 0.34056935 0.24898542 0.17434867 0.30578297 0.1498141\n",
      " 0.29690573 0.53141528 0.69951701 0.98882151 0.99909854 0.99923432\n",
      " 0.99992347 0.96994108 0.99989045 0.08623576 0.41474414 0.81040144\n",
      " 0.77238643 0.10977895 0.96798599 0.160896   0.25791267 0.92363346\n",
      " 0.99699956 0.99190897 0.99734443 0.99720508 0.99372697 0.90135807\n",
      " 0.79875934 0.99103957 0.99908853 0.43906444 0.99820995 0.99696016\n",
      " 0.99815208 0.99705911 0.96336031 0.99915779 0.99999416 0.99999034\n",
      " 0.99989104 0.99980539 0.99995959 0.99998331 0.99997556 0.63880295\n",
      " 0.890571   0.9777984  0.99354517 0.99692035 0.53477055 0.17317802\n",
      " 0.60591227 0.94543946 0.86230189 0.92434698 0.80020016 0.74667072\n",
      " 0.317821   0.54490882 0.61694658 0.7772122  0.57154119 0.99837071\n",
      " 0.21898755 0.46704698 0.45769089 0.8561852  0.9653728  0.22641194\n",
      " 0.22918193 0.48350337 0.28446275 0.19317386 0.61612576 0.31619865\n",
      " 0.15433817 0.25993761 0.61955005 0.75449431]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      "Train Epoch: 73 [0/54 (0%)]\tTrain Loss: 0.085789\n",
      "Train Epoch: 73 [8/54 (15%)]\tTrain Loss: 0.062336\n",
      "Train Epoch: 73 [16/54 (30%)]\tTrain Loss: 0.062338\n",
      "Train Epoch: 73 [24/54 (44%)]\tTrain Loss: 0.194732\n",
      "Train Epoch: 73 [32/54 (59%)]\tTrain Loss: 0.072343\n",
      "Train Epoch: 73 [40/54 (74%)]\tTrain Loss: 0.042324\n",
      "Train Epoch: 73 [48/54 (89%)]\tTrain Loss: 0.071123\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.98178434 0.99980193 0.99969137 0.97146827 0.96970189 0.95443302\n",
      " 0.99936873 0.98619753 0.89036304 0.33493078 0.37936288 0.48135605\n",
      " 0.41583404 0.7238183  0.66808575 0.32510823 0.54086268 0.77348363\n",
      " 0.30968434 0.48330757 0.83232528 0.53989214 0.97649539 0.997823\n",
      " 0.50521702 0.97728401 0.99904031 0.94355768 0.78013253 0.68097752\n",
      " 0.98295861 0.98384917 0.98799801 0.35872075 0.72682506 0.49519914\n",
      " 0.32951865 0.68386185 0.77833986 0.6797173  0.75007874 0.66946304\n",
      " 0.62146771 0.80485529 0.9079591  0.992589   0.99987864 0.99995029\n",
      " 0.99998891 0.99929798 0.99998558 0.23444611 0.40791768 0.89100224\n",
      " 0.85892618 0.43502292 0.98059285 0.48956695 0.51909584 0.96256351\n",
      " 0.9947719  0.99125004 0.99683315 0.99713767 0.99784219 0.90926254\n",
      " 0.93461716 0.97424465 0.9993692  0.58267891 0.9998191  0.99924928\n",
      " 0.99212885 0.99607807 0.92742294 0.99777156 0.99999225 0.9999882\n",
      " 0.99998033 0.99982834 0.99993467 0.99996257 0.99999297 0.91227853\n",
      " 0.97758889 0.986072   0.99853015 0.99914169 0.87154913 0.73943067\n",
      " 0.99663073 0.92693734 0.74770021 0.99493009 0.93885988 0.94656545\n",
      " 0.50837243 0.81212854 0.49639064 0.8702231  0.68948311 0.99890637\n",
      " 0.37439165 0.82758039 0.86318845 0.94860041 0.99301142 0.5033164\n",
      " 0.47131264 0.5384798  0.81055909 0.79987925 0.92055571 0.86285657\n",
      " 0.86017704 0.84655625 0.7314657  0.65085322]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 74 [0/54 (0%)]\tTrain Loss: 0.056924\n",
      "Train Epoch: 74 [8/54 (15%)]\tTrain Loss: 0.077888\n",
      "Train Epoch: 74 [16/54 (30%)]\tTrain Loss: 0.043551\n",
      "Train Epoch: 74 [24/54 (44%)]\tTrain Loss: 0.074868\n",
      "Train Epoch: 74 [32/54 (59%)]\tTrain Loss: 0.054629\n",
      "Train Epoch: 74 [40/54 (74%)]\tTrain Loss: 0.051387\n",
      "Train Epoch: 74 [48/54 (89%)]\tTrain Loss: 0.045385\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.04576917 0.81688207 0.68305129 0.29220414 0.10843163 0.0182055\n",
      " 0.64210349 0.51899767 0.00989778 0.00810006 0.46064246 0.01685268\n",
      " 0.1731884  0.66986293 0.45806372 0.1464659  0.22770488 0.19431965\n",
      " 0.13478799 0.27902892 0.33414841 0.46889737 0.68108928 0.8868627\n",
      " 0.42613143 0.86985493 0.93355709 0.53533667 0.37990737 0.39577028\n",
      " 0.67084903 0.62636882 0.65164626 0.05365772 0.24612463 0.25180206\n",
      " 0.19814302 0.24074656 0.34993511 0.22281443 0.41443309 0.23078038\n",
      " 0.1206029  0.4259606  0.34008762 0.5184139  0.64636916 0.82359976\n",
      " 0.92813444 0.75222695 0.80794418 0.01835131 0.27619106 0.5171656\n",
      " 0.21714975 0.07260775 0.89649206 0.10292149 0.08286983 0.26288602\n",
      " 0.7797721  0.76830882 0.87156481 0.8358922  0.82694566 0.63388175\n",
      " 0.4436653  0.847027   0.82855374 0.41628602 0.4622297  0.34384921\n",
      " 0.78234804 0.80950898 0.66862375 0.91977358 0.86654204 0.79727238\n",
      " 0.87123775 0.88278735 0.91068727 0.94832385 0.98230952 0.92613167\n",
      " 0.73724335 0.65892851 0.80597943 0.87025696 0.34127313 0.33060133\n",
      " 0.90933865 0.67918229 0.62687486 0.94937325 0.77651477 0.5045799\n",
      " 0.15326311 0.51309282 0.12316777 0.79021662 0.12403499 0.83620429\n",
      " 0.12541577 0.73495346 0.23691104 0.50735986 0.66740268 0.34353223\n",
      " 0.40046242 0.09875436 0.48607063 0.16269371 0.72562051 0.61904436\n",
      " 0.06514547 0.09108323 0.44797134 0.55881357]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 75 [0/54 (0%)]\tTrain Loss: 0.037314\n",
      "Train Epoch: 75 [8/54 (15%)]\tTrain Loss: 0.049264\n",
      "Train Epoch: 75 [16/54 (30%)]\tTrain Loss: 0.110179\n",
      "Train Epoch: 75 [24/54 (44%)]\tTrain Loss: 0.061221\n",
      "Train Epoch: 75 [32/54 (59%)]\tTrain Loss: 0.039171\n",
      "Train Epoch: 75 [40/54 (74%)]\tTrain Loss: 0.067533\n",
      "Train Epoch: 75 [48/54 (89%)]\tTrain Loss: 0.033075\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.79363585 0.89760315 0.81133306 0.70208806 0.58886671 0.24854963\n",
      " 0.76760459 0.86007416 0.4760575  0.05731949 0.39999738 0.08321768\n",
      " 0.29468295 0.85839361 0.77972364 0.37124959 0.34495744 0.42946765\n",
      " 0.35976365 0.29009053 0.60607189 0.66135925 0.83489662 0.98762983\n",
      " 0.68504095 0.90758181 0.9884935  0.76980782 0.44748023 0.3438836\n",
      " 0.95654219 0.78064805 0.73740482 0.27262741 0.50973314 0.5541718\n",
      " 0.33764148 0.40300944 0.76590651 0.64315224 0.72037745 0.69820982\n",
      " 0.21983935 0.43286237 0.32085261 0.88487101 0.94092828 0.96511549\n",
      " 0.99614704 0.95931989 0.99740452 0.05833006 0.41115662 0.59884274\n",
      " 0.50629246 0.1576077  0.91923082 0.19440183 0.2767531  0.56037807\n",
      " 0.96725357 0.95558447 0.9859466  0.98270041 0.95270008 0.84812307\n",
      " 0.84972203 0.96385604 0.96740913 0.96098232 0.97178531 0.93485069\n",
      " 0.96871567 0.98602408 0.91960686 0.97328168 0.96983945 0.9557637\n",
      " 0.97685903 0.97263902 0.98803455 0.99351102 0.99737215 0.85752833\n",
      " 0.89482343 0.96150827 0.89516383 0.94205874 0.83347803 0.84149832\n",
      " 0.99175984 0.88072687 0.58338934 0.96058905 0.88558263 0.76863188\n",
      " 0.26406145 0.80269796 0.6071949  0.99204296 0.86865413 0.9665308\n",
      " 0.46754554 0.82189614 0.57236636 0.59386885 0.96376395 0.75010687\n",
      " 0.68347704 0.41905561 0.90443593 0.93922013 0.92820805 0.85208672\n",
      " 0.60818684 0.52485615 0.88061619 0.88628054]\n",
      "predict [1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 76 [0/54 (0%)]\tTrain Loss: 0.055567\n",
      "Train Epoch: 76 [8/54 (15%)]\tTrain Loss: 0.120230\n",
      "Train Epoch: 76 [16/54 (30%)]\tTrain Loss: 0.051381\n",
      "Train Epoch: 76 [24/54 (44%)]\tTrain Loss: 0.096231\n",
      "Train Epoch: 76 [32/54 (59%)]\tTrain Loss: 0.063608\n",
      "Train Epoch: 76 [40/54 (74%)]\tTrain Loss: 0.050920\n",
      "Train Epoch: 76 [48/54 (89%)]\tTrain Loss: 0.051395\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.9892174  0.9999845  0.99991834 0.97652382 0.98799676 0.84098995\n",
      " 0.99998796 0.99917608 0.92595857 0.06743765 0.50114524 0.03366835\n",
      " 0.91615397 0.9795686  0.94183588 0.68340695 0.99634969 0.76397735\n",
      " 0.53528064 0.80451244 0.74948329 0.95200765 0.99324316 0.99666876\n",
      " 0.69042599 0.99185443 0.99690455 0.92234874 0.79259753 0.69767046\n",
      " 0.99530518 0.9913407  0.99665087 0.46510026 0.64508724 0.54916471\n",
      " 0.53724802 0.77722758 0.93169487 0.86648887 0.8878662  0.9105047\n",
      " 0.81216055 0.89169282 0.8189683  0.9730252  0.99997497 0.99998379\n",
      " 0.99999857 0.99908602 0.99999678 0.80316043 0.40964249 0.99811149\n",
      " 0.72181064 0.46622041 0.98737842 0.42658699 0.36838126 0.96035403\n",
      " 0.99944097 0.99722975 0.99952745 0.99927384 0.99997985 0.97899055\n",
      " 0.98409581 0.99954563 0.99999666 0.96846741 0.99999392 0.99997187\n",
      " 0.99796778 0.99612457 0.99028742 0.99993908 0.99999988 0.99999952\n",
      " 0.99999869 0.99996841 0.99997211 0.99999094 0.99999738 0.9987464\n",
      " 0.9990164  0.99167365 0.99675202 0.99942565 0.85516548 0.81903565\n",
      " 0.98638576 0.96066815 0.88838261 0.99781394 0.96666992 0.90921831\n",
      " 0.28235105 0.88983917 0.94655007 0.97932899 0.92273664 0.99993229\n",
      " 0.9202674  0.91362035 0.69994199 0.80510008 0.99154752 0.82175875\n",
      " 0.71555513 0.58327359 0.9014411  0.69049156 0.8915571  0.88735396\n",
      " 0.8825525  0.87492657 0.87751269 0.93097883]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 77 [0/54 (0%)]\tTrain Loss: 0.076251\n",
      "Train Epoch: 77 [8/54 (15%)]\tTrain Loss: 0.069462\n",
      "Train Epoch: 77 [16/54 (30%)]\tTrain Loss: 0.029009\n",
      "Train Epoch: 77 [24/54 (44%)]\tTrain Loss: 0.023690\n",
      "Train Epoch: 77 [32/54 (59%)]\tTrain Loss: 0.103371\n",
      "Train Epoch: 77 [40/54 (74%)]\tTrain Loss: 0.075235\n",
      "Train Epoch: 77 [48/54 (89%)]\tTrain Loss: 0.134587\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.07411507 0.86688423 0.84418046 0.49691215 0.23412786 0.10131422\n",
      " 0.6496197  0.52242738 0.23174417 0.19376382 0.36923632 0.24037595\n",
      " 0.35999054 0.43673915 0.5798676  0.22117099 0.13017702 0.266653\n",
      " 0.24085008 0.40795341 0.65147454 0.39400929 0.85670531 0.97728252\n",
      " 0.39826712 0.96538848 0.99467975 0.80578047 0.32904381 0.22137681\n",
      " 0.80188739 0.33175284 0.29976305 0.28410888 0.35974547 0.2637082\n",
      " 0.31187969 0.24832287 0.60462761 0.55964661 0.55572665 0.50506765\n",
      " 0.13022159 0.3199552  0.46501866 0.63609147 0.500251   0.52347469\n",
      " 0.9860239  0.98773658 0.98038989 0.0391817  0.50226539 0.48559994\n",
      " 0.89044577 0.28133133 0.90941751 0.27194071 0.42328131 0.71859616\n",
      " 0.93683064 0.93738198 0.97395372 0.97408903 0.81620449 0.48925409\n",
      " 0.37963063 0.9433859  0.93971926 0.93250489 0.98411369 0.98216552\n",
      " 0.86750734 0.86502051 0.77570027 0.8242538  0.94114298 0.85133946\n",
      " 0.9395591  0.94637859 0.97390252 0.98886883 0.99660313 0.76308531\n",
      " 0.83867151 0.93481207 0.95801222 0.96405822 0.49495089 0.52551025\n",
      " 0.98875183 0.35639203 0.4929733  0.9827351  0.95143151 0.73280007\n",
      " 0.26293886 0.79578084 0.39404553 0.85469407 0.41854176 0.9071908\n",
      " 0.52831078 0.71543097 0.20145445 0.88218302 0.92916137 0.12727316\n",
      " 0.13076769 0.08806624 0.42788127 0.27254817 0.28313667 0.66690677\n",
      " 0.70473057 0.48261034 0.61994535 0.51733279]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 78 [0/54 (0%)]\tTrain Loss: 0.079979\n",
      "Train Epoch: 78 [8/54 (15%)]\tTrain Loss: 0.078470\n",
      "Train Epoch: 78 [16/54 (30%)]\tTrain Loss: 0.079199\n",
      "Train Epoch: 78 [24/54 (44%)]\tTrain Loss: 0.087969\n",
      "Train Epoch: 78 [32/54 (59%)]\tTrain Loss: 0.103431\n",
      "Train Epoch: 78 [40/54 (74%)]\tTrain Loss: 0.125274\n",
      "Train Epoch: 78 [48/54 (89%)]\tTrain Loss: 0.043612\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.15881747 0.97782439 0.9419871  0.65856117 0.50148571 0.12276123\n",
      " 0.94271106 0.87674594 0.03999956 0.03094221 0.24479376 0.05526664\n",
      " 0.19535816 0.61935031 0.29683784 0.25331953 0.36396849 0.29944575\n",
      " 0.17670923 0.31239703 0.33880267 0.70228297 0.79852128 0.92923063\n",
      " 0.61305732 0.91339535 0.93751377 0.5788523  0.28548294 0.1241971\n",
      " 0.54379761 0.47099409 0.69737375 0.08144426 0.2237173  0.39484814\n",
      " 0.19816308 0.23750617 0.47928002 0.46355483 0.54202676 0.49320808\n",
      " 0.22991373 0.27863041 0.24091966 0.55530769 0.95423228 0.97910291\n",
      " 0.99199635 0.72154564 0.97735935 0.05192738 0.41397262 0.76944774\n",
      " 0.35937959 0.16673489 0.79895675 0.12050653 0.33754513 0.3755255\n",
      " 0.93004453 0.84040773 0.95201641 0.94484842 0.93856221 0.72827953\n",
      " 0.68519068 0.97233349 0.97339463 0.65492117 0.93879938 0.93607366\n",
      " 0.88386846 0.88949966 0.78549367 0.96381098 0.9983688  0.99531037\n",
      " 0.99712014 0.97599435 0.98573804 0.99225253 0.98861665 0.88775951\n",
      " 0.85075617 0.86644018 0.92262501 0.95552289 0.53386271 0.48283216\n",
      " 0.95466936 0.49829361 0.36874312 0.97452289 0.69476116 0.76026005\n",
      " 0.15501055 0.70784134 0.51969069 0.82377684 0.77297384 0.93118322\n",
      " 0.32548547 0.37705645 0.15983351 0.2442407  0.64290601 0.54559904\n",
      " 0.51143491 0.16441967 0.57674551 0.20079359 0.73174763 0.74433786\n",
      " 0.22524893 0.5317204  0.76859409 0.71375918]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 79 [0/54 (0%)]\tTrain Loss: 0.041894\n",
      "Train Epoch: 79 [8/54 (15%)]\tTrain Loss: 0.045899\n",
      "Train Epoch: 79 [16/54 (30%)]\tTrain Loss: 0.045351\n",
      "Train Epoch: 79 [24/54 (44%)]\tTrain Loss: 0.062482\n",
      "Train Epoch: 79 [32/54 (59%)]\tTrain Loss: 0.028497\n",
      "Train Epoch: 79 [40/54 (74%)]\tTrain Loss: 0.067794\n",
      "Train Epoch: 79 [48/54 (89%)]\tTrain Loss: 0.077569\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.99829954 1.         0.99999976 0.99997604 0.99998856 0.99967706\n",
      " 1.         0.99998498 0.95059282 0.62500644 0.85907388 0.27325249\n",
      " 0.99749541 0.99993527 0.99915004 0.99804461 0.99992096 0.96127415\n",
      " 0.32772702 0.99773693 0.87265414 0.99945813 0.99978524 0.99782181\n",
      " 0.93441379 0.99982029 0.99724102 0.99841249 0.96888328 0.94333655\n",
      " 0.99853754 0.99533075 0.99706489 0.1304314  0.20280303 0.51240009\n",
      " 0.38288128 0.98829019 0.98893714 0.98899686 0.97315073 0.99482358\n",
      " 0.9984113  0.99740666 0.9983651  0.99981302 0.99999869 0.99999952\n",
      " 1.         0.99994099 0.99999976 0.99671388 0.99032015 0.99999666\n",
      " 0.97068965 0.99841738 0.99933165 0.99114943 0.50572258 0.99997783\n",
      " 0.999982   0.99989033 0.9999702  0.99998891 0.99999976 0.99991286\n",
      " 0.99959558 0.99999177 0.99999988 0.99738163 0.99999976 0.99999952\n",
      " 0.99873787 0.99793941 0.99713421 0.99999881 1.         1.\n",
      " 1.         0.99999976 0.99999964 0.99999988 1.         0.99996936\n",
      " 0.99994183 0.99957162 0.99998176 0.99999952 0.824678   0.37025481\n",
      " 0.94930667 0.99762458 0.98614967 0.99984896 0.89971298 0.88175863\n",
      " 0.71930486 0.9876231  0.99347961 0.97813708 0.98964429 0.99992394\n",
      " 0.99590921 0.93521923 0.97658658 0.99148881 0.99671328 0.56347436\n",
      " 0.31322542 0.34073848 0.42564964 0.12611803 0.74233824 0.66822934\n",
      " 0.98476255 0.99846917 0.95239031 0.91661543]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 80 [0/54 (0%)]\tTrain Loss: 0.046969\n",
      "Train Epoch: 80 [8/54 (15%)]\tTrain Loss: 0.047713\n",
      "Train Epoch: 80 [16/54 (30%)]\tTrain Loss: 0.073399\n",
      "Train Epoch: 80 [24/54 (44%)]\tTrain Loss: 0.048539\n",
      "Train Epoch: 80 [32/54 (59%)]\tTrain Loss: 0.090325\n",
      "Train Epoch: 80 [40/54 (74%)]\tTrain Loss: 0.056957\n",
      "Train Epoch: 80 [48/54 (89%)]\tTrain Loss: 0.084167\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.30108739e-02 6.39040887e-01 5.86421132e-01 1.72102116e-02\n",
      " 1.69113521e-02 1.44603960e-02 5.28848350e-01 2.65936702e-02\n",
      " 3.53098847e-03 4.07479592e-02 7.59233832e-02 8.61001015e-02\n",
      " 9.21325386e-03 6.71659857e-02 8.48001838e-02 3.84801137e-03\n",
      " 4.95674554e-03 7.32658952e-02 7.57010803e-02 1.66286334e-01\n",
      " 4.02291059e-01 1.52496248e-01 6.29935563e-01 6.99837685e-01\n",
      " 2.65575588e-01 7.70796895e-01 8.94904256e-01 4.31015283e-01\n",
      " 4.96839359e-02 4.10637520e-02 2.23373368e-01 7.16509297e-02\n",
      " 1.86550707e-01 2.81088464e-02 4.07942310e-02 5.03323302e-02\n",
      " 4.12029922e-02 6.18588850e-02 6.19334318e-02 1.64831027e-01\n",
      " 1.90472603e-01 1.78898141e-01 1.31067066e-02 2.69494634e-02\n",
      " 2.36916319e-02 4.64859642e-02 1.03824995e-01 1.45537108e-01\n",
      " 7.70229876e-01 3.49325925e-01 5.66165864e-01 7.76780478e-04\n",
      " 2.72833724e-02 9.20582749e-03 1.35338604e-01 1.40153524e-02\n",
      " 5.27748525e-01 1.36440741e-02 8.84675086e-02 5.86566422e-03\n",
      " 6.96253955e-01 5.76121628e-01 7.04248428e-01 6.58062100e-01\n",
      " 8.65398049e-02 1.10497335e-02 7.21437950e-03 7.50362635e-01\n",
      " 1.46510512e-01 2.25873627e-02 2.02166572e-01 2.09111825e-01\n",
      " 6.92201912e-01 5.07274330e-01 2.12404519e-01 3.78869474e-01\n",
      " 7.85162807e-01 7.19448805e-01 3.86651576e-01 6.23394251e-01\n",
      " 7.65815854e-01 9.11721587e-01 9.44254339e-01 5.43307245e-01\n",
      " 7.24032879e-01 5.83857179e-01 6.94632411e-01 7.48962939e-01\n",
      " 3.19664538e-01 1.95879027e-01 9.37684834e-01 1.35708705e-01\n",
      " 1.44748360e-01 9.74323511e-01 8.03970516e-01 7.96720505e-01\n",
      " 1.26740068e-01 7.97799155e-02 3.86699438e-02 7.01447129e-01\n",
      " 1.14051700e-01 2.68307537e-01 2.72812340e-02 1.23324983e-01\n",
      " 2.74248999e-02 1.45616770e-01 1.32157326e-01 3.47730145e-02\n",
      " 2.26758644e-02 9.05068405e-03 1.55823492e-02 6.23178966e-02\n",
      " 3.03158373e-01 7.18187809e-01 7.53401220e-02 2.78845262e-02\n",
      " 5.36347687e-01 3.93971354e-01]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "vote_pred [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 47 TN= 27 FN= 11 FP= 33\n",
      "TP+FP 80\n",
      "precision 0.5875\n",
      "recall 0.8103448275862069\n",
      "F1 0.6811594202898551\n",
      "acc 0.6271186440677966\n",
      "AUCp 0.6301724137931034\n",
      "AUC 0.7244252873563218\n",
      "\n",
      " The epoch is 80, average recall: 0.8103, average precision: 0.5875,average F1: 0.6812, average accuracy: 0.6271, average AUC: 0.7244\n",
      "Train Epoch: 81 [0/54 (0%)]\tTrain Loss: 0.051645\n",
      "Train Epoch: 81 [8/54 (15%)]\tTrain Loss: 0.041899\n",
      "Train Epoch: 81 [16/54 (30%)]\tTrain Loss: 0.082751\n",
      "Train Epoch: 81 [24/54 (44%)]\tTrain Loss: 0.049444\n",
      "Train Epoch: 81 [32/54 (59%)]\tTrain Loss: 0.029542\n",
      "Train Epoch: 81 [40/54 (74%)]\tTrain Loss: 0.034865\n",
      "Train Epoch: 81 [48/54 (89%)]\tTrain Loss: 0.069762\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.09627948 0.81699079 0.6672799  0.1926339  0.17711535 0.03404339\n",
      " 0.62042189 0.62277591 0.07137372 0.03362204 0.16993526 0.11254389\n",
      " 0.41830283 0.16422585 0.16142118 0.05390419 0.16510308 0.15163329\n",
      " 0.36296436 0.66406661 0.59705579 0.71339864 0.89299589 0.90762872\n",
      " 0.56269491 0.98068649 0.97249806 0.79877728 0.52111667 0.29078507\n",
      " 0.72441566 0.64621866 0.53548473 0.05000353 0.09689936 0.0951259\n",
      " 0.21083546 0.33808368 0.33330482 0.51154757 0.49925649 0.53632623\n",
      " 0.0823317  0.43540055 0.54947811 0.29522753 0.50521988 0.61193997\n",
      " 0.9286496  0.79715669 0.94255072 0.01366054 0.38472775 0.36654797\n",
      " 0.1893166  0.11422616 0.92788631 0.16026314 0.1528668  0.23745349\n",
      " 0.91978377 0.80029345 0.8998791  0.8773675  0.96881068 0.61018318\n",
      " 0.17850357 0.98847008 0.90308833 0.26551884 0.81411624 0.79017144\n",
      " 0.86519778 0.84063143 0.83048636 0.985686   0.90125251 0.80868185\n",
      " 0.90035272 0.96647328 0.94007617 0.96712577 0.98522031 0.94546056\n",
      " 0.66454554 0.88836062 0.9161827  0.94375902 0.75604296 0.64654273\n",
      " 0.96615881 0.60703921 0.62404937 0.99383116 0.94947267 0.82160938\n",
      " 0.539882   0.43920007 0.28146553 0.88132608 0.7406584  0.78172159\n",
      " 0.26240698 0.60514414 0.28784129 0.36510068 0.74722242 0.03480158\n",
      " 0.01263389 0.01716447 0.02461217 0.03586929 0.04055684 0.63733953\n",
      " 0.41879418 0.35860699 0.53842604 0.48582175]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 82 [0/54 (0%)]\tTrain Loss: 0.036196\n",
      "Train Epoch: 82 [8/54 (15%)]\tTrain Loss: 0.088806\n",
      "Train Epoch: 82 [16/54 (30%)]\tTrain Loss: 0.125458\n",
      "Train Epoch: 82 [24/54 (44%)]\tTrain Loss: 0.039582\n",
      "Train Epoch: 82 [32/54 (59%)]\tTrain Loss: 0.052449\n",
      "Train Epoch: 82 [40/54 (74%)]\tTrain Loss: 0.059673\n",
      "Train Epoch: 82 [48/54 (89%)]\tTrain Loss: 0.042498\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.80865955 0.95268416 0.93083572 0.89319634 0.91810876 0.90388024\n",
      " 0.89452296 0.9938426  0.98294073 0.09953229 0.26625079 0.09789711\n",
      " 0.43540904 0.82100934 0.67134482 0.45951086 0.85017079 0.74571359\n",
      " 0.62932259 0.59749854 0.70528281 0.79542011 0.91504675 0.97770643\n",
      " 0.67010075 0.95900166 0.97335035 0.70078629 0.70187342 0.43939921\n",
      " 0.69490021 0.67953265 0.49596614 0.49390948 0.46021035 0.52112305\n",
      " 0.47038117 0.52017742 0.64631683 0.68629974 0.60119814 0.69822955\n",
      " 0.46560884 0.74159861 0.55519408 0.80760306 0.98525572 0.99194032\n",
      " 0.99360025 0.95890164 0.99640548 0.42336687 0.58736402 0.93589914\n",
      " 0.54267538 0.39726231 0.90420735 0.31535619 0.35845014 0.73254001\n",
      " 0.94887328 0.90348929 0.92953199 0.92484307 0.97053581 0.85590309\n",
      " 0.87355393 0.98845625 0.97921008 0.8391853  0.99869835 0.99826837\n",
      " 0.86328924 0.88283724 0.7735666  0.9825021  0.97882068 0.95953357\n",
      " 0.99417096 0.95077145 0.98001957 0.97384036 0.98883003 0.90347934\n",
      " 0.91987199 0.9028039  0.93282056 0.9677425  0.67910326 0.68277097\n",
      " 0.87352568 0.87008393 0.77013427 0.96594888 0.90009594 0.74690056\n",
      " 0.26165822 0.72124285 0.64839566 0.83829194 0.67796129 0.97875917\n",
      " 0.61282259 0.83005285 0.68945843 0.79506934 0.93124622 0.46944946\n",
      " 0.62519705 0.47823745 0.66324162 0.44277436 0.76659298 0.84813136\n",
      " 0.54884076 0.68565321 0.75967866 0.74835795]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 83 [0/54 (0%)]\tTrain Loss: 0.037109\n",
      "Train Epoch: 83 [8/54 (15%)]\tTrain Loss: 0.065203\n",
      "Train Epoch: 83 [16/54 (30%)]\tTrain Loss: 0.089939\n",
      "Train Epoch: 83 [24/54 (44%)]\tTrain Loss: 0.049847\n",
      "Train Epoch: 83 [32/54 (59%)]\tTrain Loss: 0.018342\n",
      "Train Epoch: 83 [40/54 (74%)]\tTrain Loss: 0.037057\n",
      "Train Epoch: 83 [48/54 (89%)]\tTrain Loss: 0.050301\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.31192446 0.97789198 0.97633535 0.85290271 0.7370351  0.09334864\n",
      " 0.96655679 0.89381903 0.19128896 0.01613835 0.19627142 0.02697799\n",
      " 0.62639022 0.75767982 0.74362808 0.24633849 0.44015938 0.59266043\n",
      " 0.15357625 0.4322235  0.69168741 0.79902065 0.97706044 0.97028446\n",
      " 0.66282541 0.97473437 0.98217171 0.94426364 0.60358584 0.58300662\n",
      " 0.99837083 0.99453449 0.99848276 0.16776368 0.33020809 0.47056428\n",
      " 0.23497888 0.41546482 0.78730398 0.88029647 0.86929399 0.87619793\n",
      " 0.36338711 0.75019687 0.75907993 0.9358061  0.99276137 0.99060315\n",
      " 0.99792093 0.94195312 0.99731487 0.10099493 0.26270375 0.83969367\n",
      " 0.28545558 0.28220642 0.99452025 0.33837187 0.05035466 0.59548646\n",
      " 0.99618608 0.98889565 0.99814689 0.99807161 0.99098212 0.91459143\n",
      " 0.34255698 0.99918061 0.99309409 0.9321332  0.99298918 0.98784488\n",
      " 0.99946636 0.99943322 0.99328989 0.99929535 0.99508673 0.98779905\n",
      " 0.99236488 0.99985087 0.99995017 0.99996448 0.99991727 0.90832222\n",
      " 0.93360347 0.98346418 0.98517376 0.99617934 0.5894016  0.70801234\n",
      " 0.96991134 0.95229572 0.95307666 0.99389851 0.94447988 0.87953156\n",
      " 0.10093133 0.64702737 0.53293824 0.99299169 0.62851232 0.99530041\n",
      " 0.60416758 0.67500073 0.56066447 0.78335506 0.92670095 0.24597752\n",
      " 0.24761094 0.49960461 0.51075709 0.17301883 0.97846395 0.87530363\n",
      " 0.29675296 0.26517633 0.8577466  0.72814381]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 84 [0/54 (0%)]\tTrain Loss: 0.025513\n",
      "Train Epoch: 84 [8/54 (15%)]\tTrain Loss: 0.041728\n",
      "Train Epoch: 84 [16/54 (30%)]\tTrain Loss: 0.071166\n",
      "Train Epoch: 84 [24/54 (44%)]\tTrain Loss: 0.046165\n",
      "Train Epoch: 84 [32/54 (59%)]\tTrain Loss: 0.109449\n",
      "Train Epoch: 84 [40/54 (74%)]\tTrain Loss: 0.060948\n",
      "Train Epoch: 84 [48/54 (89%)]\tTrain Loss: 0.070717\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.97711051 0.99676239 0.99127555 0.97366875 0.97311056 0.96742827\n",
      " 0.98764545 0.99777114 0.98170608 0.02378114 0.7885341  0.01984471\n",
      " 0.67738467 0.3611007  0.61998671 0.62346721 0.7354039  0.93856925\n",
      " 0.80584842 0.46968353 0.7757194  0.89456332 0.93072265 0.99533528\n",
      " 0.82164383 0.96274155 0.9938671  0.88696903 0.84799856 0.76140177\n",
      " 0.94938344 0.95137519 0.52143818 0.90723848 0.87875807 0.90155095\n",
      " 0.79667944 0.5991286  0.93885034 0.93987012 0.89229196 0.96006125\n",
      " 0.76632106 0.9380393  0.71269637 0.95487422 0.9988966  0.99909961\n",
      " 0.99884498 0.99554718 0.99959403 0.68192911 0.38979515 0.9893862\n",
      " 0.88928288 0.43110517 0.99691713 0.48629805 0.36367151 0.44386241\n",
      " 0.99862742 0.99715495 0.99873632 0.99706537 0.98772579 0.92700976\n",
      " 0.90457869 0.99559742 0.99878162 0.99144381 0.99981922 0.99981409\n",
      " 0.98476875 0.97257608 0.90553969 0.99974078 0.99959034 0.99868172\n",
      " 0.99975377 0.99883169 0.9995715  0.99950826 0.99976665 0.98319733\n",
      " 0.9790557  0.9958021  0.98397106 0.99540752 0.63155872 0.88382745\n",
      " 0.97917181 0.94537807 0.78844953 0.97478533 0.95086443 0.69563055\n",
      " 0.16800459 0.94108707 0.97571588 0.98070002 0.98639548 0.99898428\n",
      " 0.96890086 0.98511666 0.97010839 0.87403303 0.99593216 0.88987511\n",
      " 0.35581267 0.84119326 0.85705078 0.70356488 0.94462383 0.89672983\n",
      " 0.95900393 0.8955605  0.85004395 0.84857863]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 85 [0/54 (0%)]\tTrain Loss: 0.070033\n",
      "Train Epoch: 85 [8/54 (15%)]\tTrain Loss: 0.116566\n",
      "Train Epoch: 85 [16/54 (30%)]\tTrain Loss: 0.067122\n",
      "Train Epoch: 85 [24/54 (44%)]\tTrain Loss: 0.075628\n",
      "Train Epoch: 85 [32/54 (59%)]\tTrain Loss: 0.032248\n",
      "Train Epoch: 85 [40/54 (74%)]\tTrain Loss: 0.078630\n",
      "Train Epoch: 85 [48/54 (89%)]\tTrain Loss: 0.070088\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.04487355 0.16309208 0.10657894 0.19225697 0.14757788 0.01336842\n",
      " 0.07800933 0.12900911 0.01625673 0.99935502 0.99667501 0.99931812\n",
      " 0.90496701 0.01508171 0.00769846 0.04181864 0.04930041 0.01887501\n",
      " 0.00858531 0.03599057 0.15861678 0.02210503 0.06906306 0.47909942\n",
      " 0.01034408 0.16840604 0.52435839 0.08403733 0.00755873 0.01740369\n",
      " 0.04049547 0.01812434 0.03542581 0.27663517 0.66570884 0.10047101\n",
      " 0.06698611 0.10125686 0.04998842 0.03317496 0.02950104 0.0509269\n",
      " 0.07557025 0.09171342 0.06791052 0.42756462 0.59134072 0.43563375\n",
      " 0.03786177 0.71457309 0.19439319 0.929793   0.79286617 0.76037949\n",
      " 0.20575231 0.45456421 0.1305428  0.77876961 0.93289536 0.13390292\n",
      " 0.0280924  0.07037233 0.0524415  0.03188821 0.07579763 0.03218481\n",
      " 0.03545769 0.04662862 0.55596864 0.42010146 0.25094607 0.10698759\n",
      " 0.24830434 0.37681621 0.169881   0.02060376 0.03750799 0.04733101\n",
      " 0.03640808 0.00875591 0.090159   0.21624745 0.63827342 0.03506983\n",
      " 0.02624244 0.0438724  0.03488071 0.07799175 0.18079183 0.07311977\n",
      " 0.93225783 0.87364042 0.20518079 0.97847682 0.20812678 0.50737309\n",
      " 0.60812855 0.01643974 0.08948973 0.61348748 0.12706727 0.01393316\n",
      " 0.02701659 0.35455802 0.28861955 0.07506174 0.56553066 0.1393128\n",
      " 0.0719104  0.0379796  0.15370114 0.07542745 0.65778959 0.28019658\n",
      " 0.010855   0.01104964 0.03592809 0.04208329]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 86 [0/54 (0%)]\tTrain Loss: 0.059385\n",
      "Train Epoch: 86 [8/54 (15%)]\tTrain Loss: 0.101623\n",
      "Train Epoch: 86 [16/54 (30%)]\tTrain Loss: 0.067356\n",
      "Train Epoch: 86 [24/54 (44%)]\tTrain Loss: 0.033632\n",
      "Train Epoch: 86 [32/54 (59%)]\tTrain Loss: 0.046457\n",
      "Train Epoch: 86 [40/54 (74%)]\tTrain Loss: 0.020930\n",
      "Train Epoch: 86 [48/54 (89%)]\tTrain Loss: 0.014028\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.26380453 0.95914757 0.94558233 0.77032655 0.7296769  0.07281903\n",
      " 0.90640444 0.91684568 0.18865022 0.01710651 0.75256044 0.0257134\n",
      " 0.71765727 0.24814315 0.26087543 0.07422557 0.46762812 0.43983498\n",
      " 0.46431774 0.70039999 0.70504957 0.77268404 0.95378613 0.99424571\n",
      " 0.75340593 0.9902727  0.99073011 0.91100818 0.57607883 0.42007843\n",
      " 0.91987383 0.73734248 0.73580575 0.18711083 0.30881095 0.51099503\n",
      " 0.34541783 0.2915341  0.68553621 0.81514472 0.76493466 0.90325826\n",
      " 0.12398535 0.6900965  0.63691044 0.5173825  0.93810582 0.98154736\n",
      " 0.9940778  0.96165335 0.9942438  0.0570536  0.49058318 0.71680731\n",
      " 0.52990007 0.35563552 0.98504072 0.56957936 0.57579798 0.59415311\n",
      " 0.96040946 0.95266879 0.97872853 0.97136295 0.98154348 0.63511688\n",
      " 0.27344206 0.99805546 0.9948377  0.84764093 0.99347115 0.98822623\n",
      " 0.96148628 0.95956671 0.86015689 0.99074662 0.97898495 0.97604626\n",
      " 0.97559649 0.99309784 0.99546933 0.99718732 0.99896479 0.97003508\n",
      " 0.96191329 0.97446197 0.97716671 0.98853099 0.55564708 0.85512555\n",
      " 0.98623729 0.86272407 0.55512446 0.99624497 0.96292478 0.89183897\n",
      " 0.27523503 0.61251056 0.61574191 0.97338349 0.92067593 0.97593689\n",
      " 0.82067996 0.70263082 0.19798686 0.50006008 0.89681101 0.52242583\n",
      " 0.07255847 0.31583363 0.66353267 0.31861171 0.96578115 0.9332692\n",
      " 0.31747958 0.43645832 0.88247055 0.71579307]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 87 [0/54 (0%)]\tTrain Loss: 0.052783\n",
      "Train Epoch: 87 [8/54 (15%)]\tTrain Loss: 0.024578\n",
      "Train Epoch: 87 [16/54 (30%)]\tTrain Loss: 0.036577\n",
      "Train Epoch: 87 [24/54 (44%)]\tTrain Loss: 0.079128\n",
      "Train Epoch: 87 [32/54 (59%)]\tTrain Loss: 0.121752\n",
      "Train Epoch: 87 [40/54 (74%)]\tTrain Loss: 0.065922\n",
      "Train Epoch: 87 [48/54 (89%)]\tTrain Loss: 0.020567\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.73639369 0.99482697 0.98738283 0.80889153 0.47853372 0.38770425\n",
      " 0.96897531 0.88435262 0.36453348 0.01942717 0.12481993 0.02771937\n",
      " 0.19208352 0.57120639 0.73528695 0.05934609 0.10685467 0.11039563\n",
      " 0.14757663 0.59257495 0.63550848 0.54207671 0.90894264 0.99325281\n",
      " 0.57966071 0.96968746 0.99681193 0.67295521 0.17293593 0.12159904\n",
      " 0.95473999 0.71949548 0.15513042 0.05878625 0.03863677 0.06330097\n",
      " 0.06810642 0.19559807 0.40896592 0.43535468 0.53251606 0.4400107\n",
      " 0.07693709 0.19894473 0.4331128  0.91551423 0.88164485 0.96353388\n",
      " 0.99842948 0.97206151 0.99801958 0.03191441 0.23619179 0.24312577\n",
      " 0.48549286 0.09719729 0.78821087 0.1253082  0.06313673 0.6202386\n",
      " 0.98412722 0.94954109 0.99232131 0.99373573 0.96508527 0.50744241\n",
      " 0.30123726 0.99414563 0.96852255 0.73668194 0.99029946 0.98841631\n",
      " 0.98519552 0.97296166 0.8776986  0.98936558 0.99976486 0.99944836\n",
      " 0.99917942 0.99907458 0.99961793 0.99963665 0.99983323 0.69614899\n",
      " 0.84745973 0.95148373 0.99211991 0.99652857 0.33698362 0.40591747\n",
      " 0.96918732 0.42195559 0.59748936 0.98439354 0.92761469 0.73891789\n",
      " 0.0439425  0.22052531 0.1998702  0.73523664 0.1836586  0.98020554\n",
      " 0.21504205 0.4223913  0.34884915 0.80215645 0.91464478 0.0456217\n",
      " 0.03511826 0.04158131 0.10509219 0.11847161 0.10663713 0.75060844\n",
      " 0.2380119  0.31478617 0.82353795 0.78458273]\n",
      "predict [1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 88 [0/54 (0%)]\tTrain Loss: 0.118268\n",
      "Train Epoch: 88 [8/54 (15%)]\tTrain Loss: 0.100138\n",
      "Train Epoch: 88 [16/54 (30%)]\tTrain Loss: 0.022651\n",
      "Train Epoch: 88 [24/54 (44%)]\tTrain Loss: 0.040707\n",
      "Train Epoch: 88 [32/54 (59%)]\tTrain Loss: 0.052533\n",
      "Train Epoch: 88 [40/54 (74%)]\tTrain Loss: 0.057040\n",
      "Train Epoch: 88 [48/54 (89%)]\tTrain Loss: 0.035432\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.09612698 0.8893019  0.70557481 0.54011971 0.35654345 0.05841301\n",
      " 0.72327262 0.64100969 0.08517016 0.00376074 0.14876676 0.01215146\n",
      " 0.25825182 0.40976876 0.39334658 0.1551495  0.49795008 0.31782261\n",
      " 0.08798614 0.24683455 0.39352074 0.49545392 0.70151311 0.95879292\n",
      " 0.5881018  0.9041537  0.92879748 0.4210487  0.0777077  0.08317529\n",
      " 0.54947639 0.45677814 0.14618911 0.03882139 0.10561467 0.06550196\n",
      " 0.14200996 0.23863645 0.58788723 0.47473675 0.51623595 0.50701886\n",
      " 0.18552114 0.44081742 0.2304216  0.28427774 0.24581818 0.67001563\n",
      " 0.88832229 0.63200372 0.81968671 0.04491163 0.33954036 0.43870381\n",
      " 0.11653008 0.14820121 0.9237389  0.25146893 0.08211496 0.42495543\n",
      " 0.95097399 0.86911386 0.94719255 0.91953957 0.92435241 0.7774179\n",
      " 0.42568439 0.98868728 0.96268046 0.80553097 0.97937208 0.97172844\n",
      " 0.8679868  0.89876693 0.85024858 0.98064846 0.99007893 0.97785187\n",
      " 0.97172582 0.96231043 0.98029619 0.9852519  0.9967258  0.96743357\n",
      " 0.75546819 0.8957147  0.77367014 0.91973603 0.18979652 0.53744262\n",
      " 0.83065623 0.66333485 0.67706108 0.96427274 0.76137882 0.44761404\n",
      " 0.26658705 0.21612245 0.34985635 0.81070101 0.41443971 0.96003741\n",
      " 0.36245394 0.68311948 0.34898251 0.81515646 0.9390769  0.04510836\n",
      " 0.08400947 0.02376912 0.12301236 0.21656071 0.60888779 0.68968183\n",
      " 0.25514901 0.31906891 0.89752364 0.89902008]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 89 [0/54 (0%)]\tTrain Loss: 0.017447\n",
      "Train Epoch: 89 [8/54 (15%)]\tTrain Loss: 0.057677\n",
      "Train Epoch: 89 [16/54 (30%)]\tTrain Loss: 0.018884\n",
      "Train Epoch: 89 [24/54 (44%)]\tTrain Loss: 0.044287\n",
      "Train Epoch: 89 [32/54 (59%)]\tTrain Loss: 0.063369\n",
      "Train Epoch: 89 [40/54 (74%)]\tTrain Loss: 0.073813\n",
      "Train Epoch: 89 [48/54 (89%)]\tTrain Loss: 0.034828\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.07062464 0.91775936 0.87665802 0.36666259 0.22689122 0.01787454\n",
      " 0.88053888 0.80614609 0.05999617 0.00116266 0.03114565 0.00179701\n",
      " 0.06196686 0.53167403 0.44739568 0.01694591 0.0881977  0.04979199\n",
      " 0.12205435 0.36210281 0.4998503  0.16855571 0.89732629 0.96611875\n",
      " 0.23292556 0.97378123 0.93087995 0.6973021  0.07955361 0.09954108\n",
      " 0.86991107 0.62104809 0.81829923 0.02976194 0.06769666 0.04689809\n",
      " 0.04558341 0.13268693 0.13151968 0.27544871 0.31424698 0.27227953\n",
      " 0.03213463 0.20932677 0.20339926 0.49512175 0.67897314 0.85083252\n",
      " 0.98267949 0.3667708  0.98389578 0.00468541 0.14521907 0.14827047\n",
      " 0.16220486 0.11215141 0.81443399 0.22323306 0.02537743 0.28289914\n",
      " 0.91526353 0.83605474 0.96254259 0.95893914 0.91753805 0.63690513\n",
      " 0.05303233 0.99445182 0.93857878 0.08949286 0.90192902 0.83762646\n",
      " 0.9246344  0.91539705 0.70447302 0.97826195 0.97127187 0.9585883\n",
      " 0.98643261 0.9768464  0.97653276 0.98900908 0.9949367  0.64080495\n",
      " 0.7402038  0.49745712 0.82781488 0.94612861 0.28371492 0.15954773\n",
      " 0.8089962  0.34727103 0.15632007 0.96344936 0.78169918 0.54295057\n",
      " 0.10694934 0.07894225 0.17393917 0.88709319 0.27412882 0.86785084\n",
      " 0.1762269  0.02759857 0.03063304 0.09628083 0.57669318 0.10982991\n",
      " 0.03287985 0.08548293 0.07259793 0.12847516 0.41561285 0.43545291\n",
      " 0.06374739 0.06827295 0.96782345 0.90319192]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 90 [0/54 (0%)]\tTrain Loss: 0.059775\n",
      "Train Epoch: 90 [8/54 (15%)]\tTrain Loss: 0.085274\n",
      "Train Epoch: 90 [16/54 (30%)]\tTrain Loss: 0.047481\n",
      "Train Epoch: 90 [24/54 (44%)]\tTrain Loss: 0.053304\n",
      "Train Epoch: 90 [32/54 (59%)]\tTrain Loss: 0.073308\n",
      "Train Epoch: 90 [40/54 (74%)]\tTrain Loss: 0.073148\n",
      "Train Epoch: 90 [48/54 (89%)]\tTrain Loss: 0.039761\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.99550444 1.         1.         0.99993646 0.99828845 0.97419864\n",
      " 1.         0.99966776 0.90243459 0.00762928 0.13183708 0.00171959\n",
      " 0.69616652 0.47629014 0.97586834 0.82142675 0.19343758 0.97815984\n",
      " 0.42999023 0.7683028  0.82201755 0.99492127 0.99997604 0.999268\n",
      " 0.90417874 0.99774015 0.99743658 0.99227262 0.91572285 0.86136723\n",
      " 0.99943191 0.99986112 0.99987626 0.04238045 0.06905332 0.16824494\n",
      " 0.09509693 0.89868069 0.92486483 0.84018964 0.9445799  0.81231916\n",
      " 0.87332618 0.98917937 0.99531084 0.99997246 0.99999976 1.\n",
      " 1.         0.99862409 1.         0.34283084 0.51652157 0.97823775\n",
      " 0.52333164 0.59330744 0.99982738 0.50517541 0.09399386 0.99990857\n",
      " 0.9999907  0.99994624 0.9999975  0.99999785 0.99996781 0.94474667\n",
      " 0.68476373 0.99999833 0.99999857 0.97528547 0.99999976 0.99999857\n",
      " 0.99996769 0.99999142 0.99299365 0.99999988 1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99422657\n",
      " 0.99872714 0.99899572 0.99999607 0.99999881 0.0973252  0.50324023\n",
      " 0.7846902  0.99105364 0.98892188 0.95515352 0.95270634 0.69112581\n",
      " 0.02855956 0.99089164 0.96332973 0.98829621 0.76067537 0.99999988\n",
      " 0.63013542 0.99378705 0.9946385  0.99393713 0.99967337 0.03049381\n",
      " 0.01131491 0.15963519 0.04753682 0.09831015 0.75906765 0.76536369\n",
      " 0.72466362 0.98893738 0.99990141 0.99967515]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 44 TN= 30 FN= 14 FP= 30\n",
      "TP+FP 74\n",
      "precision 0.5945945945945946\n",
      "recall 0.7586206896551724\n",
      "F1 0.6666666666666667\n",
      "acc 0.6271186440677966\n",
      "AUCp 0.6293103448275862\n",
      "AUC 0.7031609195402299\n",
      "\n",
      " The epoch is 90, average recall: 0.7586, average precision: 0.5946,average F1: 0.6667, average accuracy: 0.6271, average AUC: 0.7032\n",
      "Train Epoch: 91 [0/54 (0%)]\tTrain Loss: 0.021590\n",
      "Train Epoch: 91 [8/54 (15%)]\tTrain Loss: 0.054466\n",
      "Train Epoch: 91 [16/54 (30%)]\tTrain Loss: 0.077762\n",
      "Train Epoch: 91 [24/54 (44%)]\tTrain Loss: 0.021529\n",
      "Train Epoch: 91 [32/54 (59%)]\tTrain Loss: 0.083766\n",
      "Train Epoch: 91 [40/54 (74%)]\tTrain Loss: 0.074022\n",
      "Train Epoch: 91 [48/54 (89%)]\tTrain Loss: 0.051043\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.31339461e-01 9.06147808e-02 2.24390849e-02 1.19444998e-02\n",
      " 4.89371121e-02 1.11647733e-01 7.57045224e-02 8.25263001e-03\n",
      " 1.03415828e-02 1.28535358e-02 8.36483911e-02 4.21225727e-02\n",
      " 2.92595290e-03 7.66288373e-04 3.63349944e-04 1.09095694e-02\n",
      " 9.20465402e-03 1.05728917e-01 9.14062932e-02 2.61163563e-02\n",
      " 1.28322899e-01 3.78649049e-02 3.49503011e-01 9.51545238e-01\n",
      " 5.50416335e-02 5.84590733e-01 9.13218796e-01 5.23894988e-02\n",
      " 1.23077380e-02 4.72241081e-03 4.70344424e-02 5.20962924e-02\n",
      " 5.50419986e-02 5.52123860e-02 3.40692475e-02 2.94740219e-02\n",
      " 5.59064979e-03 8.46449751e-03 7.77218118e-02 5.19032143e-02\n",
      " 5.30984662e-02 6.92166984e-02 1.00369202e-02 8.25800840e-03\n",
      " 8.61161202e-03 5.55370934e-02 1.24079101e-01 4.87522006e-01\n",
      " 4.35657173e-01 9.09096122e-01 8.71925592e-01 2.01071729e-03\n",
      " 3.74232535e-03 5.84154436e-03 2.76278704e-01 8.84735782e-04\n",
      " 6.18628502e-01 1.25397230e-03 5.39305666e-03 5.22690476e-04\n",
      " 9.28708136e-01 8.28864694e-01 9.47843432e-01 9.36718702e-01\n",
      " 1.07032610e-02 4.80692787e-03 1.42639223e-02 1.43787324e-01\n",
      " 9.08697993e-02 9.27939475e-01 9.81254458e-01 9.65302050e-01\n",
      " 6.47464633e-01 6.68243647e-01 6.81544840e-02 8.75244737e-01\n",
      " 4.70804602e-01 3.23041677e-01 4.85984050e-02 3.21215302e-01\n",
      " 9.06155348e-01 9.06623304e-01 9.88849401e-01 6.13491297e-01\n",
      " 7.02001989e-01 7.13044941e-01 6.53067112e-01 7.72856891e-01\n",
      " 4.55286633e-03 9.25616547e-02 7.76077330e-01 5.38717747e-01\n",
      " 3.35068144e-02 8.69398952e-01 6.12747848e-01 3.72020990e-01\n",
      " 5.75780263e-03 6.32140577e-01 9.01150763e-01 8.77663612e-01\n",
      " 3.76312912e-01 8.24563920e-01 1.96487606e-02 4.81149614e-01\n",
      " 3.39666843e-01 4.68722403e-01 9.57747757e-01 4.28094761e-03\n",
      " 3.57415969e-03 1.13656810e-02 8.25404003e-03 1.44442931e-01\n",
      " 5.09943843e-01 5.20710111e-01 6.10873736e-02 4.12624329e-02\n",
      " 5.37330449e-01 2.91415691e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Train Epoch: 92 [0/54 (0%)]\tTrain Loss: 0.034475\n",
      "Train Epoch: 92 [8/54 (15%)]\tTrain Loss: 0.055291\n",
      "Train Epoch: 92 [16/54 (30%)]\tTrain Loss: 0.043977\n",
      "Train Epoch: 92 [24/54 (44%)]\tTrain Loss: 0.053284\n",
      "Train Epoch: 92 [32/54 (59%)]\tTrain Loss: 0.035664\n",
      "Train Epoch: 92 [40/54 (74%)]\tTrain Loss: 0.062455\n",
      "Train Epoch: 92 [48/54 (89%)]\tTrain Loss: 0.028464\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.1289981  0.8489899  0.81171143 0.32257259 0.19039191 0.05455672\n",
      " 0.673446   0.39964643 0.09793926 0.0076421  0.13203309 0.01319567\n",
      " 0.10154754 0.15136674 0.09387147 0.02475711 0.29388425 0.08184936\n",
      " 0.06640646 0.38399568 0.61097807 0.38638422 0.79357874 0.9792251\n",
      " 0.34973106 0.94015634 0.93704933 0.42215037 0.03849317 0.03266648\n",
      " 0.68331212 0.32784033 0.43861136 0.03389319 0.05717189 0.07204284\n",
      " 0.05109742 0.08584395 0.1685711  0.29574263 0.29975352 0.50188291\n",
      " 0.0380955  0.09466353 0.06697966 0.54577494 0.6075874  0.8668834\n",
      " 0.98175365 0.86513156 0.98332214 0.03261394 0.07406642 0.30883965\n",
      " 0.29048392 0.06162591 0.66978735 0.05139891 0.03281563 0.12859267\n",
      " 0.93516189 0.7611075  0.94456851 0.93961    0.71169829 0.25387084\n",
      " 0.1173251  0.97765213 0.88691473 0.85072124 0.96121258 0.9633407\n",
      " 0.83928096 0.91874373 0.64981884 0.89231765 0.97185206 0.96421003\n",
      " 0.93161148 0.94461244 0.98627704 0.9891634  0.98811299 0.80151683\n",
      " 0.95810205 0.6382125  0.88756591 0.92709184 0.13518585 0.32630086\n",
      " 0.87288123 0.22494517 0.07482024 0.99536091 0.90445262 0.76136953\n",
      " 0.10106751 0.39990106 0.29571754 0.87203473 0.12970783 0.73710638\n",
      " 0.13505724 0.29577377 0.1935426  0.52293611 0.76891541 0.00573804\n",
      " 0.00997239 0.01880252 0.01712114 0.01281001 0.212934   0.64844787\n",
      " 0.08296653 0.19113527 0.80547172 0.61422455]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 93 [0/54 (0%)]\tTrain Loss: 0.067380\n",
      "Train Epoch: 93 [8/54 (15%)]\tTrain Loss: 0.032637\n",
      "Train Epoch: 93 [16/54 (30%)]\tTrain Loss: 0.043811\n",
      "Train Epoch: 93 [24/54 (44%)]\tTrain Loss: 0.039441\n",
      "Train Epoch: 93 [32/54 (59%)]\tTrain Loss: 0.053629\n",
      "Train Epoch: 93 [40/54 (74%)]\tTrain Loss: 0.035022\n",
      "Train Epoch: 93 [48/54 (89%)]\tTrain Loss: 0.028818\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.86033255 0.99921072 0.99286014 0.77364576 0.77555197 0.16496828\n",
      " 0.99730849 0.9847514  0.22723618 0.01206015 0.19642708 0.03225439\n",
      " 0.30203509 0.71647334 0.87978745 0.12557    0.48293218 0.06721821\n",
      " 0.3120538  0.51485795 0.75783753 0.33931485 0.941737   0.99097908\n",
      " 0.46640456 0.98954988 0.98696321 0.83034629 0.32904589 0.15329562\n",
      " 0.84037727 0.72402942 0.81215638 0.1058756  0.104279   0.19007364\n",
      " 0.12622993 0.13253935 0.33085045 0.58689469 0.61472297 0.54812443\n",
      " 0.0578413  0.39493066 0.31716058 0.82860827 0.99621058 0.99744821\n",
      " 0.99974173 0.94924217 0.99983251 0.04358609 0.73131216 0.8950451\n",
      " 0.2684806  0.34576631 0.91595691 0.50275558 0.36046726 0.90869105\n",
      " 0.97718871 0.93957084 0.98226285 0.98451024 0.99048632 0.88867682\n",
      " 0.50276285 0.9969213  0.9951669  0.77099603 0.99666506 0.99235016\n",
      " 0.92700028 0.93493956 0.81585723 0.99668914 0.9999218  0.99992847\n",
      " 0.99996257 0.99648607 0.99707699 0.99895048 0.99926656 0.93847936\n",
      " 0.94628006 0.93222284 0.99117649 0.99761617 0.67280817 0.40586239\n",
      " 0.95947438 0.66670591 0.47316754 0.99653637 0.91480011 0.88209808\n",
      " 0.46772319 0.45012677 0.56394005 0.97158313 0.84024853 0.98287326\n",
      " 0.72389531 0.46136075 0.34405887 0.61024004 0.93046439 0.04690086\n",
      " 0.00513804 0.04213763 0.02765421 0.11661667 0.4364894  0.53707236\n",
      " 0.29618487 0.47273976 0.93919152 0.960572  ]\n",
      "predict [1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 94 [0/54 (0%)]\tTrain Loss: 0.041589\n",
      "Train Epoch: 94 [8/54 (15%)]\tTrain Loss: 0.075346\n",
      "Train Epoch: 94 [16/54 (30%)]\tTrain Loss: 0.046581\n",
      "Train Epoch: 94 [24/54 (44%)]\tTrain Loss: 0.034748\n",
      "Train Epoch: 94 [32/54 (59%)]\tTrain Loss: 0.043251\n",
      "Train Epoch: 94 [40/54 (74%)]\tTrain Loss: 0.150798\n",
      "Train Epoch: 94 [48/54 (89%)]\tTrain Loss: 0.044956\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.98369455 0.99997771 0.99991131 0.95507836 0.92191291 0.98255676\n",
      " 0.99988353 0.99425817 0.96551239 0.06812262 0.41637495 0.07154447\n",
      " 0.57794279 0.39231566 0.58152598 0.78739262 0.58416271 0.5193367\n",
      " 0.75506848 0.30889356 0.71128368 0.83732623 0.9155575  0.96952641\n",
      " 0.62297314 0.91527641 0.96561241 0.45213369 0.42422915 0.29226249\n",
      " 0.68857157 0.92055875 0.50729144 0.7802676  0.80261856 0.74376816\n",
      " 0.51667881 0.29362163 0.83433908 0.77799433 0.8817603  0.80861145\n",
      " 0.37270308 0.4728629  0.31356838 0.95005536 0.99992895 0.99992096\n",
      " 0.9999572  0.96716535 0.99994373 0.08711556 0.0810597  0.83920074\n",
      " 0.68745559 0.05056108 0.89645994 0.07203406 0.12281473 0.44859859\n",
      " 0.99231017 0.98211849 0.99614799 0.99728763 0.90828049 0.82692963\n",
      " 0.88237965 0.99566025 0.99757737 0.96830845 0.99973077 0.99955422\n",
      " 0.94883728 0.98664153 0.80582434 0.99980313 0.99999964 0.99999976\n",
      " 0.99999154 0.99950659 0.99994397 0.99993229 0.99980444 0.83923483\n",
      " 0.95940274 0.98180449 0.99240351 0.99840039 0.44476172 0.8796367\n",
      " 0.9242909  0.84962279 0.29476047 0.94959807 0.90836161 0.83178627\n",
      " 0.12109115 0.96650398 0.96457666 0.99319929 0.86716908 0.9994567\n",
      " 0.47390023 0.88816881 0.86177707 0.85376924 0.96939522 0.48758233\n",
      " 0.67042732 0.54658586 0.76259607 0.59510022 0.95617628 0.96267575\n",
      " 0.81576836 0.49382934 0.951675   0.98863763]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 95 [0/54 (0%)]\tTrain Loss: 0.044806\n",
      "Train Epoch: 95 [8/54 (15%)]\tTrain Loss: 0.019436\n",
      "Train Epoch: 95 [16/54 (30%)]\tTrain Loss: 0.056208\n",
      "Train Epoch: 95 [24/54 (44%)]\tTrain Loss: 0.030090\n",
      "Train Epoch: 95 [32/54 (59%)]\tTrain Loss: 0.036875\n",
      "Train Epoch: 95 [40/54 (74%)]\tTrain Loss: 0.037141\n",
      "Train Epoch: 95 [48/54 (89%)]\tTrain Loss: 0.071236\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.99827862 0.99888831 0.98473281 0.98653072 0.94167042 0.97242343\n",
      " 0.99214655 0.98845184 0.89303482 0.30384213 0.40168679 0.3257001\n",
      " 0.42132404 0.8866846  0.9215309  0.85023987 0.84696054 0.93180895\n",
      " 0.49420911 0.48832235 0.80648261 0.96942419 0.98668033 0.99854553\n",
      " 0.90246868 0.9954313  0.99858654 0.92811185 0.70768231 0.45660609\n",
      " 0.99174643 0.99171668 0.97391212 0.6588636  0.51671028 0.83786601\n",
      " 0.53687644 0.88556337 0.97344941 0.97406614 0.97314543 0.97006375\n",
      " 0.88841671 0.85617304 0.65187013 0.99315614 0.99704957 0.99904948\n",
      " 0.99984694 0.99420643 0.99984407 0.34113583 0.42259294 0.92753959\n",
      " 0.5842185  0.14044283 0.99636394 0.30759379 0.81462348 0.78452784\n",
      " 0.99630141 0.98502618 0.99880767 0.99898201 0.98603827 0.93562019\n",
      " 0.96615422 0.99548101 0.99782556 0.99820924 0.9971385  0.99468237\n",
      " 0.99883598 0.99875402 0.97644949 0.99966693 0.99981445 0.99983382\n",
      " 0.99881905 0.99978489 0.99992537 0.99996197 0.9999342  0.96500164\n",
      " 0.97406435 0.99188143 0.99636406 0.99872071 0.74668425 0.95056945\n",
      " 0.99614161 0.97195798 0.9445923  0.99951231 0.98558295 0.98220426\n",
      " 0.37218368 0.9805299  0.97795755 0.99623293 0.98947734 0.99885547\n",
      " 0.79749715 0.97926372 0.89444137 0.97530717 0.99683541 0.46926761\n",
      " 0.23546831 0.22561742 0.584683   0.63542163 0.97070664 0.96554762\n",
      " 0.8117426  0.88190252 0.99460536 0.99121124]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 96 [0/54 (0%)]\tTrain Loss: 0.106246\n",
      "Train Epoch: 96 [8/54 (15%)]\tTrain Loss: 0.042978\n",
      "Train Epoch: 96 [16/54 (30%)]\tTrain Loss: 0.072248\n",
      "Train Epoch: 96 [24/54 (44%)]\tTrain Loss: 0.049543\n",
      "Train Epoch: 96 [32/54 (59%)]\tTrain Loss: 0.022848\n",
      "Train Epoch: 96 [40/54 (74%)]\tTrain Loss: 0.014036\n",
      "Train Epoch: 96 [48/54 (89%)]\tTrain Loss: 0.018419\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.05082131 0.67258221 0.47359258 0.21237634 0.18967305 0.03963215\n",
      " 0.47770521 0.3411898  0.04591648 0.17345953 0.39833561 0.41611984\n",
      " 0.15964626 0.14258067 0.07625141 0.15137751 0.16339828 0.26604021\n",
      " 0.22001272 0.23876929 0.75179499 0.40331388 0.71670061 0.95984715\n",
      " 0.5360797  0.93091518 0.98151392 0.43828601 0.0869965  0.09078781\n",
      " 0.56375849 0.37718493 0.43951875 0.14908083 0.20649603 0.16050622\n",
      " 0.18591216 0.2676293  0.47341445 0.64976788 0.59620327 0.68527162\n",
      " 0.15254194 0.17981587 0.10750391 0.26455775 0.28707805 0.47195622\n",
      " 0.79003924 0.70462108 0.88954645 0.01966136 0.06685387 0.34015504\n",
      " 0.26406947 0.09441053 0.58819926 0.19637606 0.37222931 0.05897956\n",
      " 0.7344718  0.7118516  0.89836866 0.78851461 0.4862794  0.37302023\n",
      " 0.42300832 0.91080344 0.86903203 0.59523094 0.85651416 0.75781786\n",
      " 0.84872949 0.91341114 0.61059344 0.77696913 0.78403044 0.71409708\n",
      " 0.46364796 0.80256629 0.88436419 0.90282673 0.9883287  0.74616414\n",
      " 0.82270485 0.73085082 0.79083323 0.86321825 0.69241422 0.63605917\n",
      " 0.99125212 0.61041027 0.32354361 0.99738735 0.81662571 0.94342548\n",
      " 0.20960547 0.35467213 0.42306826 0.94547182 0.67876625 0.76445681\n",
      " 0.12904556 0.35190967 0.17268756 0.37462509 0.58737433 0.08955544\n",
      " 0.0917597  0.05137464 0.2378796  0.50929677 0.78800076 0.94005215\n",
      " 0.35821879 0.18130735 0.71444064 0.6141423 ]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 97 [0/54 (0%)]\tTrain Loss: 0.068136\n",
      "Train Epoch: 97 [8/54 (15%)]\tTrain Loss: 0.033477\n",
      "Train Epoch: 97 [16/54 (30%)]\tTrain Loss: 0.035098\n",
      "Train Epoch: 97 [24/54 (44%)]\tTrain Loss: 0.053468\n",
      "Train Epoch: 97 [32/54 (59%)]\tTrain Loss: 0.054079\n",
      "Train Epoch: 97 [40/54 (74%)]\tTrain Loss: 0.140766\n",
      "Train Epoch: 97 [48/54 (89%)]\tTrain Loss: 0.080342\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.48459387 0.91145271 0.67527229 0.14830138 0.07078399 0.08972067\n",
      " 0.72570795 0.27418897 0.21712203 0.01278926 0.0892473  0.02709699\n",
      " 0.02187285 0.06446215 0.11085252 0.03315507 0.02179378 0.35164261\n",
      " 0.22709852 0.25414419 0.52171433 0.56293052 0.82944    0.98563296\n",
      " 0.46694252 0.94910729 0.99156559 0.67026407 0.08405362 0.15646283\n",
      " 0.86194754 0.5920586  0.92296547 0.04791186 0.03270414 0.14211696\n",
      " 0.17858322 0.18874085 0.38237745 0.40946889 0.42083088 0.50222754\n",
      " 0.21094167 0.13212679 0.04390989 0.23917359 0.88327056 0.98438394\n",
      " 0.99149209 0.81327689 0.99764234 0.00442453 0.027626   0.03455388\n",
      " 0.46800962 0.01299091 0.93356532 0.02974741 0.02246239 0.03056639\n",
      " 0.89106649 0.81943172 0.95698446 0.90166247 0.58033252 0.10268066\n",
      " 0.13742821 0.9566983  0.80267006 0.95430154 0.84483635 0.74664575\n",
      " 0.96462095 0.97635251 0.69959736 0.98348075 0.99539018 0.99402326\n",
      " 0.87041831 0.9524368  0.99185044 0.99486995 0.99480861 0.76743186\n",
      " 0.91654193 0.88610154 0.69827026 0.80673367 0.062281   0.89814001\n",
      " 0.98177654 0.3763755  0.11493139 0.99906343 0.92832589 0.88675255\n",
      " 0.04093358 0.71304482 0.31443247 0.97918642 0.37708926 0.70454365\n",
      " 0.11516283 0.57912838 0.24975571 0.29432344 0.69580448 0.0063542\n",
      " 0.00431649 0.00897358 0.01662261 0.02510342 0.96456295 0.90139985\n",
      " 0.04570941 0.08601426 0.9288857  0.87717646]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 98 [0/54 (0%)]\tTrain Loss: 0.060676\n",
      "Train Epoch: 98 [8/54 (15%)]\tTrain Loss: 0.047518\n",
      "Train Epoch: 98 [16/54 (30%)]\tTrain Loss: 0.110436\n",
      "Train Epoch: 98 [24/54 (44%)]\tTrain Loss: 0.057434\n",
      "Train Epoch: 98 [32/54 (59%)]\tTrain Loss: 0.072903\n",
      "Train Epoch: 98 [40/54 (74%)]\tTrain Loss: 0.041251\n",
      "Train Epoch: 98 [48/54 (89%)]\tTrain Loss: 0.016840\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.71487969 0.97284961 0.94694108 0.9716357  0.93898356 0.20358913\n",
      " 0.95435566 0.99239218 0.36627826 0.14601265 0.72206122 0.06377719\n",
      " 0.96472877 0.96600723 0.85549235 0.54902369 0.87456036 0.27429819\n",
      " 0.52258617 0.67884737 0.8891449  0.92901379 0.97572529 0.9901439\n",
      " 0.9190014  0.99736303 0.99580127 0.98217303 0.71850055 0.57750696\n",
      " 0.99626213 0.93145746 0.87877899 0.33153477 0.25570819 0.53898579\n",
      " 0.61654228 0.48158148 0.90768254 0.98538554 0.97296822 0.98292911\n",
      " 0.39206234 0.87766123 0.86328584 0.95918214 0.99510455 0.99538112\n",
      " 0.99797827 0.96142298 0.99969935 0.07509594 0.9888553  0.99466914\n",
      " 0.70948702 0.81303877 0.99719954 0.97208762 0.77461588 0.99190372\n",
      " 0.99821776 0.99043208 0.99899501 0.99845612 0.99944109 0.99219346\n",
      " 0.99084455 0.9997142  0.99971443 0.97042465 0.99118912 0.98546958\n",
      " 0.99584913 0.99894828 0.99846989 0.99948812 0.99801767 0.99159122\n",
      " 0.99386805 0.99929309 0.99935216 0.9997924  0.99951255 0.58075988\n",
      " 0.91563767 0.9511711  0.98588598 0.9902935  0.94801927 0.94242775\n",
      " 0.997531   0.98057693 0.9395898  0.99979466 0.99258852 0.99354124\n",
      " 0.92445642 0.96025914 0.91859204 0.99893695 0.8829729  0.9971329\n",
      " 0.8516112  0.94150138 0.78938055 0.88057017 0.9710958  0.70454431\n",
      " 0.51920372 0.28749213 0.76625866 0.59891409 0.99609476 0.98987561\n",
      " 0.70199347 0.62176007 0.97617143 0.97030759]\n",
      "predict [1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 99 [0/54 (0%)]\tTrain Loss: 0.029935\n",
      "Train Epoch: 99 [8/54 (15%)]\tTrain Loss: 0.058019\n",
      "Train Epoch: 99 [16/54 (30%)]\tTrain Loss: 0.039075\n",
      "Train Epoch: 99 [24/54 (44%)]\tTrain Loss: 0.056006\n",
      "Train Epoch: 99 [32/54 (59%)]\tTrain Loss: 0.057674\n",
      "Train Epoch: 99 [40/54 (74%)]\tTrain Loss: 0.038861\n",
      "Train Epoch: 99 [48/54 (89%)]\tTrain Loss: 0.069489\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.16331223 0.9685601  0.95084137 0.85356349 0.51063335 0.12772582\n",
      " 0.8748762  0.90044433 0.10438389 0.09127371 0.27510485 0.09213746\n",
      " 0.39833981 0.28423727 0.39466414 0.0491143  0.36568761 0.16543932\n",
      " 0.24233407 0.67571634 0.76857787 0.59009755 0.94518644 0.99078077\n",
      " 0.58327442 0.97182095 0.991189   0.39802182 0.11769807 0.08034395\n",
      " 0.68910438 0.32012746 0.15366533 0.04767568 0.0475009  0.05221567\n",
      " 0.03777817 0.41699117 0.57693249 0.93468624 0.82901663 0.8715722\n",
      " 0.19186857 0.23076729 0.36931282 0.77156383 0.84862149 0.90295762\n",
      " 0.99454725 0.96034986 0.99681854 0.09118259 0.18963324 0.91508132\n",
      " 0.26144096 0.22616714 0.87560046 0.23794538 0.03190994 0.28718808\n",
      " 0.99861121 0.97946429 0.99669552 0.99721295 0.87469012 0.63741392\n",
      " 0.32106751 0.98788667 0.95986396 0.33719015 0.97282887 0.96208858\n",
      " 0.96618825 0.98873526 0.88544428 0.98366469 0.99880898 0.99779594\n",
      " 0.94818681 0.99652171 0.99696666 0.99880064 0.9987191  0.7961992\n",
      " 0.98044968 0.7647844  0.98954952 0.99404413 0.39649621 0.15795484\n",
      " 0.93953907 0.37740251 0.4108111  0.9962793  0.93449879 0.78331846\n",
      " 0.17867374 0.61902851 0.48165643 0.8842575  0.2449774  0.97518277\n",
      " 0.08672489 0.6438123  0.54956889 0.77895099 0.71852255 0.02167613\n",
      " 0.04013607 0.03463824 0.02570488 0.31494755 0.51847517 0.69427615\n",
      " 0.14766386 0.13290481 0.86676008 0.5584473 ]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 100 [0/54 (0%)]\tTrain Loss: 0.035067\n",
      "Train Epoch: 100 [8/54 (15%)]\tTrain Loss: 0.061574\n",
      "Train Epoch: 100 [16/54 (30%)]\tTrain Loss: 0.036094\n",
      "Train Epoch: 100 [24/54 (44%)]\tTrain Loss: 0.019909\n",
      "Train Epoch: 100 [32/54 (59%)]\tTrain Loss: 0.057386\n",
      "Train Epoch: 100 [40/54 (74%)]\tTrain Loss: 0.104621\n",
      "Train Epoch: 100 [48/54 (89%)]\tTrain Loss: 0.142178\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.99957985 0.99998653 0.99998009 0.99988949 0.99972993 0.99978894\n",
      " 0.99987864 0.9998343  0.99944824 0.79377049 0.89110333 0.51643848\n",
      " 0.9868421  0.66805702 0.78275985 0.97706407 0.99205971 0.97868502\n",
      " 0.91633552 0.83663201 0.95762801 0.98092908 0.99635702 0.99982542\n",
      " 0.88097906 0.99647647 0.99978596 0.92005581 0.92399329 0.78691721\n",
      " 0.97817141 0.99809486 0.66128206 0.9631601  0.88287783 0.93766522\n",
      " 0.90527052 0.93936497 0.99546766 0.9980191  0.99591053 0.99709845\n",
      " 0.96741688 0.98806727 0.99185354 0.99948871 0.99999261 0.99999762\n",
      " 0.9999938  0.99987161 0.99999821 0.93075603 0.41515109 0.99980158\n",
      " 0.8935256  0.8658126  0.99906915 0.94971645 0.46729976 0.95392883\n",
      " 0.99985111 0.99943441 0.99994969 0.99992442 0.99905771 0.99002403\n",
      " 0.99058598 0.99960691 0.99999356 0.99949312 0.99999344 0.99997962\n",
      " 0.99924517 0.99985266 0.97268564 0.99996328 1.         0.99999988\n",
      " 0.99999893 0.99999535 0.99999893 0.99999774 0.99999821 0.85451239\n",
      " 0.99499029 0.99908924 0.99913114 0.99969387 0.89845073 0.9825477\n",
      " 0.99815375 0.99940324 0.99226499 0.99741286 0.99868602 0.98205823\n",
      " 0.69548631 0.99585587 0.99814522 0.99951673 0.99223095 0.9999944\n",
      " 0.96609658 0.99794096 0.99784839 0.99696201 0.99932683 0.34027284\n",
      " 0.08225803 0.30982265 0.73553908 0.93752146 0.98247749 0.98514682\n",
      " 0.90122831 0.97728628 0.98179954 0.99357855]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 45 TN= 33 FN= 13 FP= 27\n",
      "TP+FP 72\n",
      "precision 0.625\n",
      "recall 0.7758620689655172\n",
      "F1 0.6923076923076923\n",
      "acc 0.6610169491525424\n",
      "AUCp 0.6629310344827587\n",
      "AUC 0.7367816091954024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The epoch is 100, average recall: 0.7759, average precision: 0.6250,average F1: 0.6923, average accuracy: 0.6610, average AUC: 0.7368\n",
      "Train Epoch: 101 [0/54 (0%)]\tTrain Loss: 0.061385\n",
      "Train Epoch: 101 [8/54 (15%)]\tTrain Loss: 0.033254\n",
      "Train Epoch: 101 [16/54 (30%)]\tTrain Loss: 0.045218\n",
      "Train Epoch: 101 [24/54 (44%)]\tTrain Loss: 0.063031\n",
      "Train Epoch: 101 [32/54 (59%)]\tTrain Loss: 0.059263\n",
      "Train Epoch: 101 [40/54 (74%)]\tTrain Loss: 0.066529\n",
      "Train Epoch: 101 [48/54 (89%)]\tTrain Loss: 0.075293\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02303731 0.86271977 0.77543801 0.24892153 0.10845032 0.005365\n",
      " 0.6736871  0.60726225 0.01193593 0.01038684 0.32940477 0.03325356\n",
      " 0.0606987  0.06526314 0.14157057 0.01949169 0.01733888 0.10136172\n",
      " 0.09444161 0.0993569  0.30385396 0.33690327 0.74081552 0.89105999\n",
      " 0.3568235  0.93132335 0.93587112 0.19904217 0.03218624 0.05706777\n",
      " 0.18881138 0.12174311 0.30031654 0.04841824 0.02000312 0.04000648\n",
      " 0.02233833 0.2871156  0.12207085 0.2485631  0.34731597 0.22267273\n",
      " 0.0307675  0.12473876 0.13080658 0.29267058 0.14113754 0.52075362\n",
      " 0.50311476 0.42647102 0.75083458 0.00447172 0.02996126 0.05643836\n",
      " 0.04897637 0.0111385  0.4942475  0.01745841 0.05282676 0.02489014\n",
      " 0.77543193 0.71107405 0.41349491 0.40014508 0.53605896 0.15112475\n",
      " 0.05460708 0.93544263 0.67518449 0.14584324 0.62653041 0.68007088\n",
      " 0.3547664  0.51236594 0.38708696 0.31533    0.93866974 0.92973632\n",
      " 0.52222329 0.68858218 0.8044073  0.86099988 0.92215949 0.06610209\n",
      " 0.23401333 0.21487151 0.55270535 0.35613891 0.0782072  0.15925682\n",
      " 0.69932431 0.16173851 0.21247767 0.95364559 0.58471322 0.2758179\n",
      " 0.03413421 0.10192404 0.14739728 0.78029054 0.12942357 0.19128902\n",
      " 0.04951899 0.44504109 0.11838088 0.13245197 0.62992376 0.00797639\n",
      " 0.00296335 0.00534833 0.00786878 0.09205086 0.01930413 0.31480271\n",
      " 0.04971648 0.0269968  0.71552509 0.6461165 ]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "Train Epoch: 102 [0/54 (0%)]\tTrain Loss: 0.046268\n",
      "Train Epoch: 102 [8/54 (15%)]\tTrain Loss: 0.055084\n",
      "Train Epoch: 102 [16/54 (30%)]\tTrain Loss: 0.063856\n",
      "Train Epoch: 102 [24/54 (44%)]\tTrain Loss: 0.116260\n",
      "Train Epoch: 102 [32/54 (59%)]\tTrain Loss: 0.034521\n",
      "Train Epoch: 102 [40/54 (74%)]\tTrain Loss: 0.036442\n",
      "Train Epoch: 102 [48/54 (89%)]\tTrain Loss: 0.068778\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.00871597 0.9135716  0.91308951 0.96935374 0.89750093 0.01738548\n",
      " 0.788504   0.92230272 0.06195113 0.0135254  0.85973763 0.0068147\n",
      " 0.79641825 0.11517558 0.11211269 0.13131268 0.49671224 0.39027506\n",
      " 0.20992625 0.31070521 0.49853176 0.84064794 0.74542212 0.97616428\n",
      " 0.76747525 0.93438172 0.93933249 0.72425562 0.08828598 0.13595532\n",
      " 0.81514323 0.65693611 0.44436532 0.11440183 0.49266055 0.74431485\n",
      " 0.52575177 0.70909733 0.80472201 0.92448533 0.8992855  0.95023489\n",
      " 0.32949412 0.6933825  0.65663302 0.87343866 0.93226832 0.99190485\n",
      " 0.97047365 0.98379284 0.97099555 0.1086056  0.51854497 0.91675669\n",
      " 0.04837039 0.20155396 0.89988554 0.58848959 0.12787028 0.14351186\n",
      " 0.96932405 0.95933223 0.9925068  0.96801949 0.96653336 0.7021594\n",
      " 0.33100253 0.99877375 0.99730873 0.87860703 0.99051791 0.98844612\n",
      " 0.93361259 0.9847247  0.94607985 0.9920609  0.99476612 0.98911369\n",
      " 0.97423232 0.98587173 0.99726921 0.99659282 0.99881244 0.55601567\n",
      " 0.7260595  0.88558358 0.77908111 0.87303394 0.28242242 0.80802459\n",
      " 0.89724308 0.92218226 0.92555696 0.99550372 0.92795539 0.79014242\n",
      " 0.18585861 0.64959759 0.78778464 0.98999679 0.25398538 0.97366375\n",
      " 0.67363203 0.78799427 0.51984769 0.80021709 0.97071338 0.00845925\n",
      " 0.00371677 0.01130989 0.03490657 0.17734371 0.76275325 0.93772078\n",
      " 0.27777991 0.14977518 0.94505441 0.74441439]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 103 [0/54 (0%)]\tTrain Loss: 0.027139\n",
      "Train Epoch: 103 [8/54 (15%)]\tTrain Loss: 0.051335\n",
      "Train Epoch: 103 [16/54 (30%)]\tTrain Loss: 0.020088\n",
      "Train Epoch: 103 [24/54 (44%)]\tTrain Loss: 0.065586\n",
      "Train Epoch: 103 [32/54 (59%)]\tTrain Loss: 0.025671\n",
      "Train Epoch: 103 [40/54 (74%)]\tTrain Loss: 0.028843\n",
      "Train Epoch: 103 [48/54 (89%)]\tTrain Loss: 0.031585\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.0101669  0.63915634 0.64348817 0.04199575 0.02784211 0.00796426\n",
      " 0.30664161 0.10985438 0.01388396 0.00612511 0.32732758 0.01076466\n",
      " 0.0322771  0.01342654 0.01487176 0.00455522 0.01561159 0.06971288\n",
      " 0.11392321 0.15091604 0.23160487 0.35397092 0.38209611 0.91975629\n",
      " 0.34067392 0.85461187 0.88160312 0.43389148 0.0220317  0.03618054\n",
      " 0.22683725 0.13546214 0.145624   0.02457001 0.06141867 0.01409987\n",
      " 0.0378641  0.03462687 0.0230765  0.02988997 0.02428843 0.04479648\n",
      " 0.00334507 0.02885454 0.07467409 0.19259726 0.69440269 0.83619761\n",
      " 0.92218113 0.80695313 0.9236564  0.00160655 0.0126186  0.01191181\n",
      " 0.12318254 0.0059672  0.43399575 0.01230532 0.00942924 0.01519464\n",
      " 0.85123825 0.88239193 0.94225353 0.79592419 0.47683924 0.07120085\n",
      " 0.02405575 0.99201888 0.55326706 0.53382218 0.90951383 0.87636417\n",
      " 0.71832681 0.92639136 0.74788713 0.54230744 0.81170714 0.8825869\n",
      " 0.32487693 0.73613256 0.90597594 0.9221034  0.98184401 0.66513067\n",
      " 0.60970986 0.49591985 0.67742246 0.92106563 0.04122895 0.14273529\n",
      " 0.87351823 0.29201394 0.10110977 0.9904021  0.76663721 0.48176733\n",
      " 0.01499516 0.03963792 0.45433852 0.85097426 0.02073047 0.32111147\n",
      " 0.0083427  0.10928575 0.06820029 0.19348115 0.55141014 0.02106588\n",
      " 0.06225773 0.0333861  0.11508942 0.11874687 0.6273362  0.89403921\n",
      " 0.03357686 0.01324548 0.50061834 0.23688839]\n",
      "predict [0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Train Epoch: 104 [0/54 (0%)]\tTrain Loss: 0.094222\n",
      "Train Epoch: 104 [8/54 (15%)]\tTrain Loss: 0.027098\n",
      "Train Epoch: 104 [16/54 (30%)]\tTrain Loss: 0.101576\n",
      "Train Epoch: 104 [24/54 (44%)]\tTrain Loss: 0.069415\n",
      "Train Epoch: 104 [32/54 (59%)]\tTrain Loss: 0.024132\n",
      "Train Epoch: 104 [40/54 (74%)]\tTrain Loss: 0.043979\n",
      "Train Epoch: 104 [48/54 (89%)]\tTrain Loss: 0.114810\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.38422322 0.99969399 0.99948198 0.98740876 0.97880083 0.3948828\n",
      " 0.99826968 0.99794787 0.13629171 0.00748147 0.05925125 0.00912715\n",
      " 0.16285178 0.29441896 0.30924433 0.16787653 0.32816979 0.18557832\n",
      " 0.11033657 0.23027974 0.35931259 0.4486194  0.84182388 0.97603863\n",
      " 0.33880842 0.91332114 0.9478122  0.3782714  0.06365371 0.11056837\n",
      " 0.39044216 0.43693087 0.16657449 0.03999309 0.03076011 0.14735425\n",
      " 0.05419004 0.45817143 0.70647317 0.79029453 0.69650996 0.68030775\n",
      " 0.29277304 0.38401362 0.59323317 0.9000631  0.99743217 0.99936241\n",
      " 0.99996543 0.99038637 0.99990022 0.08382001 0.09482682 0.9438647\n",
      " 0.0510489  0.09197152 0.4894608  0.14696094 0.0588528  0.70382285\n",
      " 0.99595344 0.96309066 0.99879944 0.99842513 0.99335164 0.8097235\n",
      " 0.48915222 0.99895549 0.99946612 0.85748607 0.9998982  0.99971133\n",
      " 0.97712213 0.98710525 0.77795744 0.99278486 0.99998581 0.99998367\n",
      " 0.9999361  0.99934107 0.99986053 0.99991202 0.99956065 0.79935414\n",
      " 0.92445356 0.93776941 0.9849872  0.99593437 0.26673523 0.19796683\n",
      " 0.92777824 0.74790531 0.62271714 0.99332196 0.78046662 0.55939782\n",
      " 0.05873388 0.37173969 0.3424359  0.89201528 0.53441298 0.99716729\n",
      " 0.20067634 0.26177132 0.26153547 0.79874265 0.97607839 0.06477731\n",
      " 0.0392422  0.03263395 0.09765752 0.09663945 0.3448112  0.42023644\n",
      " 0.34239221 0.21294111 0.94242132 0.85875148]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 105 [0/54 (0%)]\tTrain Loss: 0.044626\n",
      "Train Epoch: 105 [8/54 (15%)]\tTrain Loss: 0.074337\n",
      "Train Epoch: 105 [16/54 (30%)]\tTrain Loss: 0.026001\n",
      "Train Epoch: 105 [24/54 (44%)]\tTrain Loss: 0.016639\n",
      "Train Epoch: 105 [32/54 (59%)]\tTrain Loss: 0.022534\n",
      "Train Epoch: 105 [40/54 (74%)]\tTrain Loss: 0.069130\n",
      "Train Epoch: 105 [48/54 (89%)]\tTrain Loss: 0.097083\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.03811757 0.94734776 0.97045273 0.47035134 0.46612266 0.1888855\n",
      " 0.7567482  0.62919372 0.05721828 0.01943443 0.21673994 0.04790329\n",
      " 0.08946445 0.0062498  0.00573266 0.06031533 0.07469325 0.32953203\n",
      " 0.15162082 0.06266036 0.35871246 0.29794964 0.7053706  0.98737818\n",
      " 0.32982287 0.91104072 0.98627484 0.21221909 0.05955502 0.04558152\n",
      " 0.40248957 0.54412878 0.01526499 0.04133416 0.07590442 0.13945423\n",
      " 0.04043955 0.15983793 0.5445466  0.48209661 0.56923884 0.43145108\n",
      " 0.24265242 0.18238772 0.20402493 0.27852586 0.92583615 0.98083419\n",
      " 0.99554813 0.9944185  0.99807066 0.01826776 0.00751482 0.20010842\n",
      " 0.27831995 0.01946251 0.83434623 0.02234859 0.3547563  0.01283136\n",
      " 0.99089211 0.94135129 0.99665183 0.99754339 0.404111   0.14441337\n",
      " 0.13323669 0.95579004 0.9873153  0.81897771 0.99924493 0.99772388\n",
      " 0.97607964 0.98621756 0.80623007 0.99044001 0.99809009 0.99809331\n",
      " 0.98545837 0.99595898 0.99983156 0.99984503 0.99987185 0.58361322\n",
      " 0.80301571 0.96452487 0.96543646 0.98156768 0.15841092 0.53533775\n",
      " 0.97994572 0.67773819 0.23116875 0.99832124 0.89833909 0.83591187\n",
      " 0.0416797  0.6523928  0.61643457 0.93998146 0.63571978 0.99318379\n",
      " 0.1153988  0.76308113 0.74164039 0.89113146 0.92953449 0.03848146\n",
      " 0.15839505 0.03951321 0.20482136 0.15291552 0.85715294 0.84507227\n",
      " 0.32406509 0.3225598  0.77561402 0.35495216]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Train Epoch: 106 [0/54 (0%)]\tTrain Loss: 0.016794\n",
      "Train Epoch: 106 [8/54 (15%)]\tTrain Loss: 0.054875\n",
      "Train Epoch: 106 [16/54 (30%)]\tTrain Loss: 0.043012\n",
      "Train Epoch: 106 [24/54 (44%)]\tTrain Loss: 0.034226\n",
      "Train Epoch: 106 [32/54 (59%)]\tTrain Loss: 0.031292\n",
      "Train Epoch: 106 [40/54 (74%)]\tTrain Loss: 0.044665\n",
      "Train Epoch: 106 [48/54 (89%)]\tTrain Loss: 0.095422\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.09693363 0.99993217 0.99987805 0.9821164  0.82703489 0.02801788\n",
      " 0.99949789 0.9992938  0.11059885 0.0011724  0.37100768 0.00350727\n",
      " 0.29542252 0.12443952 0.7740764  0.06410584 0.24981095 0.19763686\n",
      " 0.21700427 0.43231016 0.37083614 0.75202179 0.96419704 0.95942676\n",
      " 0.73122334 0.98834038 0.96468759 0.78022349 0.22684366 0.135928\n",
      " 0.91498631 0.90568173 0.67564762 0.02074527 0.01847048 0.19495484\n",
      " 0.06534627 0.68711972 0.63772416 0.84679586 0.89274329 0.82091397\n",
      " 0.37432617 0.67563456 0.88934803 0.63285738 0.99927253 0.9997595\n",
      " 0.99910718 0.98579919 0.99997079 0.052118   0.79240614 0.98506641\n",
      " 0.06364825 0.32912821 0.88808089 0.23733652 0.05718429 0.61692125\n",
      " 0.99467373 0.9913578  0.99868554 0.99746382 0.99964058 0.90792269\n",
      " 0.17983483 0.99989009 0.99990582 0.47582293 0.99985933 0.99955374\n",
      " 0.9708581  0.97610801 0.93879169 0.99960738 0.9999994  0.9999994\n",
      " 0.9999994  0.99945563 0.99979562 0.99987352 0.99969828 0.89748019\n",
      " 0.94265676 0.95459872 0.96381068 0.99226016 0.28281504 0.22791351\n",
      " 0.87485075 0.52394813 0.54304719 0.99784756 0.92194104 0.82843804\n",
      " 0.26841143 0.14866753 0.33425853 0.88479996 0.95508987 0.99637383\n",
      " 0.58724338 0.31111991 0.15380366 0.49677876 0.93875712 0.00939578\n",
      " 0.00353234 0.00988947 0.01067838 0.04790752 0.07407727 0.6810776\n",
      " 0.13673438 0.22540638 0.98993647 0.98886043]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 107 [0/54 (0%)]\tTrain Loss: 0.034388\n",
      "Train Epoch: 107 [8/54 (15%)]\tTrain Loss: 0.039605\n",
      "Train Epoch: 107 [16/54 (30%)]\tTrain Loss: 0.048761\n",
      "Train Epoch: 107 [24/54 (44%)]\tTrain Loss: 0.101557\n",
      "Train Epoch: 107 [32/54 (59%)]\tTrain Loss: 0.057758\n",
      "Train Epoch: 107 [40/54 (74%)]\tTrain Loss: 0.028905\n",
      "Train Epoch: 107 [48/54 (89%)]\tTrain Loss: 0.060067\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.20003988 0.98455137 0.89205909 0.92967331 0.60948646 0.01945328\n",
      " 0.83805132 0.97518355 0.01017594 0.0115508  0.62245399 0.04608676\n",
      " 0.27463058 0.13398781 0.48333219 0.00517788 0.01292507 0.06891386\n",
      " 0.41369635 0.35162532 0.66504413 0.56875592 0.92095649 0.98611844\n",
      " 0.64589363 0.97360194 0.98814172 0.5185765  0.04037829 0.11858138\n",
      " 0.74270672 0.3568103  0.02122776 0.04377233 0.05836689 0.15504943\n",
      " 0.04034064 0.17325443 0.10970972 0.2898159  0.36688042 0.26306081\n",
      " 0.05038718 0.2249351  0.44846138 0.19290979 0.23103951 0.45745528\n",
      " 0.78485918 0.38968989 0.91950113 0.00136193 0.45060372 0.52639556\n",
      " 0.08338536 0.14727761 0.4742454  0.24433255 0.41549936 0.46413773\n",
      " 0.98429859 0.9337644  0.99267572 0.99165738 0.85893595 0.24891081\n",
      " 0.07087125 0.99687433 0.84903443 0.15929474 0.5619812  0.35124478\n",
      " 0.90723187 0.9565326  0.71721894 0.98530465 0.99887246 0.99773991\n",
      " 0.7222141  0.98957014 0.9979279  0.99921    0.99926776 0.48386085\n",
      " 0.58408111 0.84495991 0.9639625  0.97908151 0.45445105 0.07562833\n",
      " 0.94418991 0.91334957 0.60469216 0.99817753 0.96996653 0.96400869\n",
      " 0.68656439 0.06502119 0.47414216 0.99440151 0.56605899 0.96240604\n",
      " 0.05502371 0.20396525 0.06652917 0.48375937 0.97928381 0.00672344\n",
      " 0.00355723 0.00589884 0.00625432 0.10040226 0.29572943 0.69686097\n",
      " 0.01490881 0.02663902 0.89371878 0.91810805]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 108 [0/54 (0%)]\tTrain Loss: 0.059094\n",
      "Train Epoch: 108 [8/54 (15%)]\tTrain Loss: 0.035746\n",
      "Train Epoch: 108 [16/54 (30%)]\tTrain Loss: 0.064784\n",
      "Train Epoch: 108 [24/54 (44%)]\tTrain Loss: 0.027595\n",
      "Train Epoch: 108 [32/54 (59%)]\tTrain Loss: 0.081576\n",
      "Train Epoch: 108 [40/54 (74%)]\tTrain Loss: 0.061642\n",
      "Train Epoch: 108 [48/54 (89%)]\tTrain Loss: 0.042151\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.73189491 0.94628072 0.83533192 0.22709329 0.14669898 0.06807409\n",
      " 0.848086   0.34588033 0.07479985 0.00611972 0.05808625 0.01690968\n",
      " 0.02297982 0.14346626 0.1993902  0.04036009 0.10202683 0.06697431\n",
      " 0.09381475 0.20058291 0.2686359  0.33835644 0.7425862  0.96644866\n",
      " 0.17981584 0.83997315 0.94560504 0.19122501 0.03741385 0.11100478\n",
      " 0.67792106 0.36591095 0.69305909 0.0369531  0.02488978 0.03987584\n",
      " 0.02281318 0.04278413 0.12560682 0.18010069 0.13145371 0.11033127\n",
      " 0.08039076 0.0526774  0.05725931 0.68382525 0.93701845 0.97988659\n",
      " 0.99919134 0.97734571 0.99979073 0.03045095 0.07347491 0.61828381\n",
      " 0.16293567 0.04090672 0.80103183 0.05806373 0.11047323 0.12948087\n",
      " 0.98808181 0.9767493  0.99768937 0.99744546 0.86038011 0.31439716\n",
      " 0.18153818 0.83237624 0.90635538 0.8860358  0.99778545 0.99368322\n",
      " 0.9163301  0.93007088 0.62548375 0.99321449 0.9965418  0.99565166\n",
      " 0.91905445 0.98273534 0.99357301 0.99553043 0.99880314 0.79480916\n",
      " 0.89018059 0.96162921 0.66450655 0.84695214 0.22667742 0.19232084\n",
      " 0.8405301  0.39032507 0.15826227 0.96658003 0.70585996 0.35735455\n",
      " 0.04273025 0.13837089 0.35282934 0.64180905 0.7369343  0.98175883\n",
      " 0.09117296 0.34327644 0.18912998 0.31497186 0.95976752 0.032591\n",
      " 0.02626313 0.04630749 0.04729141 0.085439   0.04255968 0.13257591\n",
      " 0.21970423 0.07710832 0.96320552 0.93701732]\n",
      "predict [1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 109 [0/54 (0%)]\tTrain Loss: 0.093960\n",
      "Train Epoch: 109 [8/54 (15%)]\tTrain Loss: 0.039847\n",
      "Train Epoch: 109 [16/54 (30%)]\tTrain Loss: 0.075978\n",
      "Train Epoch: 109 [24/54 (44%)]\tTrain Loss: 0.023522\n",
      "Train Epoch: 109 [32/54 (59%)]\tTrain Loss: 0.031758\n",
      "Train Epoch: 109 [40/54 (74%)]\tTrain Loss: 0.089032\n",
      "Train Epoch: 109 [48/54 (89%)]\tTrain Loss: 0.023236\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.0010185  0.55299252 0.53148806 0.04871677 0.04572125 0.01366527\n",
      " 0.4805266  0.08638909 0.02241361 0.01281803 0.06749029 0.04678656\n",
      " 0.00917896 0.00457888 0.00318991 0.00731977 0.02124696 0.35894778\n",
      " 0.17818893 0.15296283 0.58412415 0.4052909  0.7996496  0.99147499\n",
      " 0.34718516 0.91881579 0.98026901 0.21754834 0.01348404 0.01770334\n",
      " 0.017331   0.03839128 0.00946355 0.02965517 0.01364977 0.01848386\n",
      " 0.01333295 0.03952529 0.42370602 0.30474246 0.22191101 0.38132963\n",
      " 0.05368257 0.02869019 0.02699315 0.00630238 0.04015638 0.41186967\n",
      " 0.12041622 0.98189551 0.33072758 0.00170407 0.01138697 0.01486652\n",
      " 0.21492174 0.00927421 0.65767139 0.01153096 0.12584339 0.00236118\n",
      " 0.93685979 0.76901734 0.94696283 0.9108516  0.09726249 0.04527752\n",
      " 0.01311722 0.85947752 0.77656269 0.94041902 0.96404469 0.86764199\n",
      " 0.49451831 0.77230805 0.3165797  0.85397911 0.82361114 0.80087429\n",
      " 0.19505678 0.54745847 0.88207895 0.92918831 0.99139613 0.54515392\n",
      " 0.70578933 0.65324205 0.64747041 0.72219247 0.17131491 0.7964043\n",
      " 0.97168791 0.38040149 0.06937527 0.99808681 0.8497656  0.83814991\n",
      " 0.01637959 0.08623449 0.61644012 0.97020143 0.23365459 0.92699093\n",
      " 0.0258379  0.23895755 0.04919608 0.20798416 0.87070382 0.01404749\n",
      " 0.02950711 0.01572626 0.05480771 0.08018094 0.24702418 0.62166727\n",
      " 0.11647445 0.09592848 0.84335113 0.84440094]\n",
      "predict [0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 110 [0/54 (0%)]\tTrain Loss: 0.067440\n",
      "Train Epoch: 110 [8/54 (15%)]\tTrain Loss: 0.057202\n",
      "Train Epoch: 110 [16/54 (30%)]\tTrain Loss: 0.041836\n",
      "Train Epoch: 110 [24/54 (44%)]\tTrain Loss: 0.041343\n",
      "Train Epoch: 110 [32/54 (59%)]\tTrain Loss: 0.070641\n",
      "Train Epoch: 110 [40/54 (74%)]\tTrain Loss: 0.045951\n",
      "Train Epoch: 110 [48/54 (89%)]\tTrain Loss: 0.018604\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.51896542 0.97548521 0.95004278 0.7980786  0.83028799 0.2122076\n",
      " 0.89693606 0.90307319 0.50647223 0.10223383 0.35028279 0.32124981\n",
      " 0.04911004 0.1706394  0.18510234 0.10382519 0.21350877 0.6560986\n",
      " 0.21830858 0.12325443 0.30358136 0.75777274 0.91917419 0.97918874\n",
      " 0.72049862 0.91697598 0.97628784 0.59174228 0.1125994  0.09038163\n",
      " 0.96261013 0.90854895 0.90951484 0.22328047 0.25882202 0.35361388\n",
      " 0.13768709 0.3557072  0.75313276 0.74534637 0.69812441 0.84912443\n",
      " 0.31231654 0.31287488 0.25453198 0.96548903 0.93766731 0.99236578\n",
      " 0.95884931 0.95856518 0.9863621  0.0921015  0.15267058 0.34037963\n",
      " 0.36140114 0.04234833 0.89523202 0.03136886 0.04878462 0.08331315\n",
      " 0.9455173  0.87622368 0.98941153 0.98297244 0.97224581 0.66895586\n",
      " 0.72394478 0.99207413 0.99732322 0.99716538 0.98600024 0.97588617\n",
      " 0.99183851 0.99630487 0.95224524 0.98681539 0.99800771 0.99742031\n",
      " 0.9479779  0.994528   0.99694115 0.99670714 0.99684912 0.83071154\n",
      " 0.92650044 0.97769988 0.71471471 0.74943441 0.4197385  0.852714\n",
      " 0.9768694  0.94606    0.87939817 0.98474693 0.9399997  0.85388637\n",
      " 0.0285521  0.62809575 0.88007617 0.9766717  0.69951057 0.99278682\n",
      " 0.34169289 0.92893869 0.70828301 0.71268976 0.9636665  0.13757107\n",
      " 0.13216762 0.07701873 0.25383559 0.43526483 0.78334075 0.87683982\n",
      " 0.56480074 0.38403913 0.94651622 0.95058817]\n",
      "predict [1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
      "vote_pred [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 38 TN= 46 FN= 20 FP= 14\n",
      "TP+FP 52\n",
      "precision 0.7307692307692307\n",
      "recall 0.6551724137931034\n",
      "F1 0.6909090909090909\n",
      "acc 0.711864406779661\n",
      "AUCp 0.710919540229885\n",
      "AUC 0.7183908045977011\n",
      "\n",
      " The epoch is 110, average recall: 0.6552, average precision: 0.7308,average F1: 0.6909, average accuracy: 0.7119, average AUC: 0.7184\n",
      "Train Epoch: 111 [0/54 (0%)]\tTrain Loss: 0.025413\n",
      "Train Epoch: 111 [8/54 (15%)]\tTrain Loss: 0.020583\n",
      "Train Epoch: 111 [16/54 (30%)]\tTrain Loss: 0.052692\n",
      "Train Epoch: 111 [24/54 (44%)]\tTrain Loss: 0.041205\n",
      "Train Epoch: 111 [32/54 (59%)]\tTrain Loss: 0.040984\n",
      "Train Epoch: 111 [40/54 (74%)]\tTrain Loss: 0.054645\n",
      "Train Epoch: 111 [48/54 (89%)]\tTrain Loss: 0.019267\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.8439768  0.91410238 0.91893065 0.25199461 0.65489346 0.16353208\n",
      " 0.72841573 0.45915157 0.5105859  0.05094926 0.33960527 0.22014146\n",
      " 0.03641628 0.20029858 0.0945268  0.05927382 0.05407398 0.30020085\n",
      " 0.15407602 0.15930696 0.60654837 0.2247621  0.77422851 0.99071795\n",
      " 0.20902695 0.93573886 0.99244821 0.67840594 0.16202487 0.11320034\n",
      " 0.72563791 0.61898828 0.15269087 0.27872378 0.18181339 0.05021706\n",
      " 0.03902579 0.05879269 0.49161559 0.4163073  0.395206   0.65086555\n",
      " 0.06810066 0.09847727 0.13707951 0.54959005 0.88488889 0.94733524\n",
      " 0.97339588 0.98795182 0.99320257 0.01009825 0.04191875 0.21671671\n",
      " 0.71411192 0.05243442 0.71263981 0.09026913 0.2809512  0.04722598\n",
      " 0.92134351 0.89991516 0.96407938 0.95106971 0.8271628  0.55378139\n",
      " 0.32429749 0.94402212 0.99614018 0.98320675 0.99213517 0.98698294\n",
      " 0.94250381 0.97487336 0.84259748 0.84466422 0.98563093 0.98070621\n",
      " 0.90195471 0.97429174 0.99456882 0.99525672 0.99923337 0.49634784\n",
      " 0.53530777 0.89187825 0.7185241  0.82187259 0.60657781 0.74837601\n",
      " 0.98802978 0.84856385 0.5331313  0.99916899 0.97964293 0.93905646\n",
      " 0.17779692 0.12991501 0.95515919 0.98513526 0.56432933 0.94986624\n",
      " 0.09243222 0.64850289 0.41587129 0.44349423 0.96480978 0.13804361\n",
      " 0.37779543 0.14246124 0.41260287 0.51493257 0.76648647 0.83358186\n",
      " 0.39230075 0.08251224 0.70888591 0.80610734]\n",
      "predict [1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 112 [0/54 (0%)]\tTrain Loss: 0.040748\n",
      "Train Epoch: 112 [8/54 (15%)]\tTrain Loss: 0.033083\n",
      "Train Epoch: 112 [16/54 (30%)]\tTrain Loss: 0.023351\n",
      "Train Epoch: 112 [24/54 (44%)]\tTrain Loss: 0.112352\n",
      "Train Epoch: 112 [32/54 (59%)]\tTrain Loss: 0.048868\n",
      "Train Epoch: 112 [40/54 (74%)]\tTrain Loss: 0.020447\n",
      "Train Epoch: 112 [48/54 (89%)]\tTrain Loss: 0.066261\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.09644005 0.92127907 0.9581039  0.4968015  0.27927351 0.08028138\n",
      " 0.73975378 0.79184031 0.23038478 0.01162172 0.49354172 0.04244799\n",
      " 0.15516736 0.2076032  0.11515155 0.02376936 0.06062468 0.12661639\n",
      " 0.1160489  0.1919305  0.51952034 0.41227925 0.77075368 0.94539225\n",
      " 0.34291965 0.90351051 0.97064275 0.83103067 0.05649854 0.03741335\n",
      " 0.87645555 0.36466661 0.13418171 0.04217814 0.03011317 0.07041831\n",
      " 0.02882546 0.10113553 0.22042881 0.19206913 0.29266348 0.28728995\n",
      " 0.02980442 0.26266655 0.28511572 0.75808036 0.99100798 0.9919194\n",
      " 0.9919613  0.95932388 0.99866319 0.02702269 0.15612182 0.27077746\n",
      " 0.11977674 0.16181497 0.953749   0.19820234 0.25656146 0.07771687\n",
      " 0.93049037 0.78612483 0.91697174 0.9072274  0.86266184 0.7047348\n",
      " 0.40672743 0.93819886 0.99857366 0.57198787 0.99126679 0.98287213\n",
      " 0.88090545 0.91331583 0.68251425 0.95629865 0.99799585 0.99589622\n",
      " 0.99357611 0.99447107 0.99638003 0.998779   0.99963582 0.35517055\n",
      " 0.61560297 0.52764273 0.72478735 0.88239694 0.36954615 0.57237899\n",
      " 0.97365731 0.68035996 0.50903958 0.99968994 0.98580533 0.93188047\n",
      " 0.18369882 0.21543971 0.53577292 0.90436339 0.09515671 0.94298345\n",
      " 0.09660641 0.97101587 0.74292028 0.81734133 0.81499642 0.01619056\n",
      " 0.02945526 0.03001978 0.02362966 0.05628563 0.20677808 0.52242154\n",
      " 0.19452061 0.09879753 0.79005527 0.77944446]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 113 [0/54 (0%)]\tTrain Loss: 0.071456\n",
      "Train Epoch: 113 [8/54 (15%)]\tTrain Loss: 0.017519\n",
      "Train Epoch: 113 [16/54 (30%)]\tTrain Loss: 0.022004\n",
      "Train Epoch: 113 [24/54 (44%)]\tTrain Loss: 0.094872\n",
      "Train Epoch: 113 [32/54 (59%)]\tTrain Loss: 0.040555\n",
      "Train Epoch: 113 [40/54 (74%)]\tTrain Loss: 0.034792\n",
      "Train Epoch: 113 [48/54 (89%)]\tTrain Loss: 0.024207\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.08531915 0.9750582  0.96473199 0.85184914 0.75229293 0.03269115\n",
      " 0.8852573  0.9420293  0.05514411 0.0148949  0.33319595 0.04072771\n",
      " 0.40828815 0.47311112 0.23305151 0.10180774 0.59698534 0.15367381\n",
      " 0.30292609 0.29452443 0.56735009 0.50054616 0.9239592  0.96935999\n",
      " 0.4509238  0.97449297 0.99054998 0.91617805 0.1255796  0.11818688\n",
      " 0.89696223 0.37014812 0.22129928 0.07374652 0.0575517  0.19023858\n",
      " 0.11454174 0.20666845 0.63248056 0.73452663 0.7996068  0.75478804\n",
      " 0.04790213 0.26467517 0.47584593 0.51816452 0.98497307 0.98513871\n",
      " 0.99141991 0.96848375 0.9922685  0.03141657 0.32571751 0.70061243\n",
      " 0.18807346 0.3677775  0.85756129 0.59953541 0.29469287 0.42730147\n",
      " 0.91967726 0.83408207 0.87646306 0.88446611 0.95952564 0.92547315\n",
      " 0.63185143 0.98834455 0.99916923 0.51135105 0.97060758 0.96817768\n",
      " 0.92665148 0.96010429 0.8953988  0.76117265 0.99813062 0.99413621\n",
      " 0.98603475 0.99045193 0.99210125 0.99758995 0.99936467 0.71131164\n",
      " 0.89280838 0.519086   0.86682653 0.9137345  0.66230237 0.68267906\n",
      " 0.98937315 0.88774407 0.74804324 0.9998877  0.98653597 0.98456615\n",
      " 0.20336099 0.32414052 0.35017529 0.97302169 0.5062356  0.92982399\n",
      " 0.14492565 0.71065336 0.26088154 0.32233334 0.78367621 0.07814595\n",
      " 0.04691793 0.03849489 0.06960148 0.12678452 0.89939839 0.77252614\n",
      " 0.12015035 0.06771564 0.92316628 0.78547698]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 114 [0/54 (0%)]\tTrain Loss: 0.030455\n",
      "Train Epoch: 114 [8/54 (15%)]\tTrain Loss: 0.020218\n",
      "Train Epoch: 114 [16/54 (30%)]\tTrain Loss: 0.047024\n",
      "Train Epoch: 114 [24/54 (44%)]\tTrain Loss: 0.055041\n",
      "Train Epoch: 114 [32/54 (59%)]\tTrain Loss: 0.132446\n",
      "Train Epoch: 114 [40/54 (74%)]\tTrain Loss: 0.047742\n",
      "Train Epoch: 114 [48/54 (89%)]\tTrain Loss: 0.028522\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.22827384 0.87751186 0.64148831 0.22121181 0.45200661 0.13916156\n",
      " 0.58891499 0.66486168 0.21993242 0.00805879 0.03472098 0.02151579\n",
      " 0.02912169 0.24064982 0.12098792 0.0281026  0.0111267  0.19461745\n",
      " 0.17060064 0.06694148 0.34100115 0.37777257 0.68663758 0.97812927\n",
      " 0.26772922 0.86406416 0.97433531 0.17844827 0.04023772 0.02348028\n",
      " 0.36045942 0.16648585 0.0146822  0.03126691 0.01719603 0.02340815\n",
      " 0.01656197 0.06721481 0.4447231  0.41789389 0.55045706 0.57099253\n",
      " 0.04177671 0.03491234 0.07110838 0.28773132 0.70203841 0.90261084\n",
      " 0.96656203 0.88198084 0.98734957 0.00272738 0.01577746 0.03441945\n",
      " 0.1665944  0.00951675 0.60502517 0.01295605 0.02219686 0.046049\n",
      " 0.96709967 0.87791568 0.97467178 0.97566301 0.62708044 0.41329995\n",
      " 0.30870765 0.9576779  0.93769497 0.9068293  0.9771378  0.96600038\n",
      " 0.73068714 0.94510615 0.53136539 0.96986234 0.99762315 0.99563426\n",
      " 0.89337492 0.97655118 0.98865128 0.99362493 0.99863476 0.09837693\n",
      " 0.34100845 0.77256739 0.86008239 0.91326666 0.11801386 0.16372064\n",
      " 0.93867487 0.63516867 0.26709282 0.99642295 0.8461169  0.81020343\n",
      " 0.03825639 0.24086718 0.17273191 0.88510513 0.21463117 0.94262022\n",
      " 0.08583399 0.5360679  0.23149297 0.71252167 0.95941287 0.02014736\n",
      " 0.03876974 0.03471207 0.0704907  0.38914305 0.35142574 0.72500473\n",
      " 0.2013548  0.0996263  0.9281894  0.68408316]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 115 [0/54 (0%)]\tTrain Loss: 0.056948\n",
      "Train Epoch: 115 [8/54 (15%)]\tTrain Loss: 0.037175\n",
      "Train Epoch: 115 [16/54 (30%)]\tTrain Loss: 0.039859\n",
      "Train Epoch: 115 [24/54 (44%)]\tTrain Loss: 0.049156\n",
      "Train Epoch: 115 [32/54 (59%)]\tTrain Loss: 0.030869\n",
      "Train Epoch: 115 [40/54 (74%)]\tTrain Loss: 0.072822\n",
      "Train Epoch: 115 [48/54 (89%)]\tTrain Loss: 0.118044\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01953571 0.30600172 0.24510272 0.13890648 0.07773897 0.00541611\n",
      " 0.0939065  0.23775281 0.00755958 0.00319923 0.37191337 0.00342589\n",
      " 0.07191922 0.060775   0.01344094 0.02052062 0.06852325 0.03360872\n",
      " 0.04970479 0.04857476 0.11051647 0.1064661  0.53483176 0.61072135\n",
      " 0.11771971 0.9211452  0.96202147 0.67279387 0.06315735 0.01916606\n",
      " 0.69486821 0.30539903 0.17389004 0.04928801 0.06729596 0.0980607\n",
      " 0.05745206 0.02861406 0.34126446 0.37624159 0.35977766 0.43097943\n",
      " 0.00536032 0.02680567 0.07334469 0.07102489 0.42313051 0.54142255\n",
      " 0.64345181 0.60964078 0.87365943 0.00138857 0.03627287 0.33121437\n",
      " 0.17798783 0.0150334  0.56417209 0.09902262 0.00676548 0.03554284\n",
      " 0.91249859 0.79283917 0.94879961 0.84891719 0.88023674 0.67086369\n",
      " 0.20801494 0.97690409 0.98183894 0.68388724 0.89544469 0.9186756\n",
      " 0.93319124 0.97878224 0.92837775 0.75415707 0.87340111 0.74847335\n",
      " 0.40941691 0.94887292 0.97354615 0.99656028 0.9980197  0.599563\n",
      " 0.40811408 0.46129221 0.76431376 0.85889196 0.18079543 0.53923219\n",
      " 0.96021748 0.55117601 0.3458074  0.99908555 0.90046203 0.84199071\n",
      " 0.03906094 0.10661524 0.21055795 0.98658329 0.16769207 0.60381997\n",
      " 0.02547651 0.76476663 0.16846736 0.44044983 0.51388019 0.11333261\n",
      " 0.13973664 0.02649455 0.20931362 0.11909372 0.92326373 0.97033036\n",
      " 0.05955935 0.01929694 0.33260599 0.30994937]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 116 [0/54 (0%)]\tTrain Loss: 0.088405\n",
      "Train Epoch: 116 [8/54 (15%)]\tTrain Loss: 0.049130\n",
      "Train Epoch: 116 [16/54 (30%)]\tTrain Loss: 0.017708\n",
      "Train Epoch: 116 [24/54 (44%)]\tTrain Loss: 0.097328\n",
      "Train Epoch: 116 [32/54 (59%)]\tTrain Loss: 0.040620\n",
      "Train Epoch: 116 [40/54 (74%)]\tTrain Loss: 0.093058\n",
      "Train Epoch: 116 [48/54 (89%)]\tTrain Loss: 0.022424\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.5855732  0.93471444 0.78113872 0.28054243 0.58727741 0.87894857\n",
      " 0.73558164 0.2934311  0.65700614 0.12980142 0.4601396  0.16899733\n",
      " 0.11435688 0.08508692 0.09619285 0.11626104 0.24897848 0.43404779\n",
      " 0.18264766 0.09482799 0.33159903 0.59335297 0.90563774 0.99066764\n",
      " 0.39989808 0.97684389 0.99942005 0.32738432 0.06125557 0.04185955\n",
      " 0.85390359 0.55349046 0.04634699 0.16191149 0.06726351 0.0626566\n",
      " 0.04434326 0.19821146 0.79591644 0.79218256 0.84100848 0.73326415\n",
      " 0.26690817 0.15753449 0.16415484 0.82810986 0.99598557 0.99783868\n",
      " 0.99859113 0.99725974 0.99987912 0.0520666  0.03859914 0.84212017\n",
      " 0.83697724 0.01791329 0.96381396 0.01897901 0.10457703 0.12173948\n",
      " 0.99924767 0.95361418 0.99967611 0.99948609 0.70083243 0.28946277\n",
      " 0.50279289 0.77463943 0.9938525  0.93869138 0.99870539 0.99834359\n",
      " 0.99551117 0.99652356 0.97803235 0.99772376 0.99985921 0.99974865\n",
      " 0.99922371 0.99967003 0.99980408 0.99995959 0.99986386 0.87773609\n",
      " 0.96034962 0.9711203  0.9729355  0.98998427 0.32237005 0.89264518\n",
      " 0.9980697  0.71434677 0.49047643 0.99984157 0.99573922 0.9699564\n",
      " 0.08223442 0.94171035 0.81331939 0.96153164 0.94367552 0.99884939\n",
      " 0.2861928  0.97928518 0.90720236 0.98332107 0.99212885 0.01394448\n",
      " 0.05255583 0.03009936 0.0742991  0.43571937 0.23773225 0.93862331\n",
      " 0.40688983 0.16687602 0.88821095 0.89014828]\n",
      "predict [1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 117 [0/54 (0%)]\tTrain Loss: 0.050016\n",
      "Train Epoch: 117 [8/54 (15%)]\tTrain Loss: 0.048200\n",
      "Train Epoch: 117 [16/54 (30%)]\tTrain Loss: 0.063144\n",
      "Train Epoch: 117 [24/54 (44%)]\tTrain Loss: 0.020301\n",
      "Train Epoch: 117 [32/54 (59%)]\tTrain Loss: 0.043808\n",
      "Train Epoch: 117 [40/54 (74%)]\tTrain Loss: 0.039841\n",
      "Train Epoch: 117 [48/54 (89%)]\tTrain Loss: 0.026192\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.99207741 0.99942786 0.99979657 0.99906224 0.99775982 0.9945088\n",
      " 0.9984737  0.99924809 0.99634522 0.57050908 0.14261246 0.37724\n",
      " 0.59324068 0.75964296 0.75869179 0.67414558 0.57347262 0.98486531\n",
      " 0.44728839 0.57650137 0.85053259 0.96980178 0.99915385 0.99998426\n",
      " 0.77736372 0.99897122 0.99998963 0.98300409 0.50070441 0.37144908\n",
      " 0.99902403 0.99466622 0.90886742 0.17885071 0.07623208 0.2371899\n",
      " 0.06078198 0.7229563  0.98790282 0.9887166  0.97806019 0.97704256\n",
      " 0.84655112 0.79731691 0.84550577 0.99864119 0.99999011 0.99998772\n",
      " 0.99999571 0.99997318 0.99999893 0.54663628 0.23619856 0.98914796\n",
      " 0.86197108 0.18021394 0.99947113 0.2315812  0.68362492 0.84499121\n",
      " 0.99995124 0.99960345 0.9999665  0.99994016 0.99958318 0.97687006\n",
      " 0.96245384 0.99994051 0.99999046 0.99803072 0.9999975  0.99999082\n",
      " 0.99983466 0.99991536 0.9982363  0.99998951 0.99999964 0.99999893\n",
      " 0.99999785 0.99999762 0.99999905 0.99999893 0.9999994  0.98600626\n",
      " 0.9992488  0.99926871 0.99990237 0.99997115 0.88563591 0.87252706\n",
      " 0.99984562 0.99595815 0.96298993 0.99999011 0.99663711 0.98779804\n",
      " 0.07094255 0.96847767 0.97564566 0.99895644 0.99537498 0.99996519\n",
      " 0.62094057 0.99729508 0.99362093 0.99761707 0.99971324 0.19108739\n",
      " 0.20741139 0.45667809 0.70037115 0.87169039 0.98579848 0.94096118\n",
      " 0.95441031 0.93005139 0.99407494 0.91663069]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 118 [0/54 (0%)]\tTrain Loss: 0.055243\n",
      "Train Epoch: 118 [8/54 (15%)]\tTrain Loss: 0.027970\n",
      "Train Epoch: 118 [16/54 (30%)]\tTrain Loss: 0.040638\n",
      "Train Epoch: 118 [24/54 (44%)]\tTrain Loss: 0.052335\n",
      "Train Epoch: 118 [32/54 (59%)]\tTrain Loss: 0.029349\n",
      "Train Epoch: 118 [40/54 (74%)]\tTrain Loss: 0.015598\n",
      "Train Epoch: 118 [48/54 (89%)]\tTrain Loss: 0.170418\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.17974256 0.66522157 0.43899709 0.24686567 0.050274   0.02067898\n",
      " 0.5412004  0.35977066 0.01540002 0.05814691 0.21184281 0.22041656\n",
      " 0.12856518 0.12527511 0.53714597 0.02781997 0.38519597 0.11175321\n",
      " 0.69318062 0.67575157 0.80500162 0.90629405 0.98638201 0.99626273\n",
      " 0.59278393 0.9968279  0.99848551 0.90946758 0.19663216 0.16675177\n",
      " 0.19402319 0.09104444 0.02099161 0.01964707 0.01821211 0.15700629\n",
      " 0.07531837 0.10867842 0.24954885 0.25721323 0.46309888 0.25076845\n",
      " 0.03810339 0.2846989  0.37018737 0.01858311 0.0718871  0.41063789\n",
      " 0.66929013 0.87364382 0.96441066 0.01906607 0.88855362 0.54471958\n",
      " 0.12882096 0.30693865 0.70197254 0.67105061 0.874129   0.29552564\n",
      " 0.93658525 0.92274863 0.96630245 0.91484267 0.83274674 0.33869386\n",
      " 0.11456895 0.99079061 0.95860076 0.0700138  0.64949244 0.37961006\n",
      " 0.36494899 0.83330649 0.26772121 0.96924609 0.78122312 0.58788127\n",
      " 0.97576922 0.83039576 0.8219617  0.9371385  0.99249762 0.9841525\n",
      " 0.97238016 0.34530255 0.98430139 0.98725498 0.85810709 0.47114533\n",
      " 0.99421388 0.37608349 0.31114116 0.99988568 0.93003803 0.98684525\n",
      " 0.82564169 0.49012995 0.39071959 0.85250634 0.7977227  0.8906616\n",
      " 0.36895755 0.32777226 0.07525953 0.19429943 0.62235403 0.00517494\n",
      " 0.00399447 0.00279702 0.00440452 0.01825423 0.36469817 0.50136131\n",
      " 0.05519591 0.33928826 0.85654652 0.9799065 ]\n",
      "predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 119 [0/54 (0%)]\tTrain Loss: 0.082788\n",
      "Train Epoch: 119 [8/54 (15%)]\tTrain Loss: 0.053270\n",
      "Train Epoch: 119 [16/54 (30%)]\tTrain Loss: 0.071817\n",
      "Train Epoch: 119 [24/54 (44%)]\tTrain Loss: 0.023406\n",
      "Train Epoch: 119 [32/54 (59%)]\tTrain Loss: 0.077315\n",
      "Train Epoch: 119 [40/54 (74%)]\tTrain Loss: 0.029127\n",
      "Train Epoch: 119 [48/54 (89%)]\tTrain Loss: 0.022289\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.96247458 0.99967337 0.9991166  0.93505871 0.90778941 0.45111302\n",
      " 0.99790406 0.99300545 0.49153385 0.0643189  0.04541339 0.06100475\n",
      " 0.08834888 0.2291307  0.60294598 0.02948639 0.16795635 0.76760072\n",
      " 0.30012888 0.53471357 0.61722481 0.8393243  0.98799932 0.99806625\n",
      " 0.44163767 0.97979331 0.99718875 0.82619375 0.15242986 0.13739151\n",
      " 0.9705537  0.95565712 0.54800826 0.01766253 0.00846701 0.02473896\n",
      " 0.00981242 0.11587378 0.79738683 0.75491339 0.63856786 0.63635093\n",
      " 0.33569866 0.22888131 0.68466467 0.89385998 0.98232794 0.99875808\n",
      " 0.99897987 0.99511802 0.99993813 0.13860178 0.38974407 0.81787455\n",
      " 0.26439455 0.14028575 0.95708477 0.18487082 0.13363257 0.3230367\n",
      " 0.99837863 0.99879467 0.99897468 0.99875939 0.98955297 0.63448495\n",
      " 0.35685453 0.99913871 0.99903071 0.93711287 0.99955672 0.99869043\n",
      " 0.9970746  0.99683821 0.95427424 0.99974126 0.99999404 0.99999845\n",
      " 0.99958318 0.99980587 0.9999373  0.99995363 0.99997437 0.94564056\n",
      " 0.97745699 0.97890913 0.99737895 0.99785787 0.0688876  0.1624267\n",
      " 0.85273123 0.92691606 0.86558449 0.99742931 0.94995755 0.68829483\n",
      " 0.10839391 0.3245073  0.92151499 0.98723757 0.34058496 0.9864586\n",
      " 0.20577256 0.95765501 0.94518942 0.94998461 0.98754495 0.00572774\n",
      " 0.00282401 0.01376806 0.00556757 0.01183735 0.08344471 0.59089279\n",
      " 0.2282173  0.28718159 0.96492285 0.89413261]\n",
      "predict [1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 120 [0/54 (0%)]\tTrain Loss: 0.020609\n",
      "Train Epoch: 120 [8/54 (15%)]\tTrain Loss: 0.033934\n",
      "Train Epoch: 120 [16/54 (30%)]\tTrain Loss: 0.079441\n",
      "Train Epoch: 120 [24/54 (44%)]\tTrain Loss: 0.021436\n",
      "Train Epoch: 120 [32/54 (59%)]\tTrain Loss: 0.019463\n",
      "Train Epoch: 120 [40/54 (74%)]\tTrain Loss: 0.048912\n",
      "Train Epoch: 120 [48/54 (89%)]\tTrain Loss: 0.015256\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.78920767e-02 3.66608322e-01 2.07402200e-01 6.89993938e-03\n",
      " 1.04961423e-02 4.64367354e-03 1.71618253e-01 3.78381200e-02\n",
      " 3.67099931e-03 1.11154616e-02 2.88710892e-01 6.30136430e-02\n",
      " 3.20020923e-03 1.05279451e-03 1.54996582e-03 1.10961124e-03\n",
      " 3.83369974e-03 3.44866104e-02 7.55887255e-02 1.56347960e-01\n",
      " 3.18474174e-01 9.83940586e-02 4.06981915e-01 8.93344641e-01\n",
      " 1.60837039e-01 9.15498137e-01 9.63808537e-01 2.37596020e-01\n",
      " 7.07178190e-03 5.25547192e-03 3.66015472e-02 4.69518937e-02\n",
      " 1.18821589e-02 1.34893199e-02 1.22083602e-02 1.47684505e-02\n",
      " 1.32667040e-02 1.99884698e-02 4.10062462e-01 2.36227199e-01\n",
      " 3.73884201e-01 2.74376035e-01 4.45323158e-03 5.69903478e-03\n",
      " 1.19398087e-02 1.26454988e-02 2.35184357e-02 2.24271476e-01\n",
      " 8.76483858e-01 8.39732707e-01 9.80973423e-01 8.42002220e-04\n",
      " 3.54675250e-03 4.31084214e-03 5.55229783e-02 1.77902146e-03\n",
      " 3.05689842e-01 3.59950983e-03 4.85695638e-02 3.66769871e-03\n",
      " 8.67532134e-01 7.84054995e-01 8.23956251e-01 8.01393092e-01\n",
      " 2.22279504e-02 1.17954817e-02 5.97981038e-03 9.28129911e-01\n",
      " 3.40216756e-01 1.98224276e-01 9.19065237e-01 8.05262804e-01\n",
      " 5.59195399e-01 6.88668907e-01 1.77516609e-01 7.20448673e-01\n",
      " 8.06060016e-01 6.62346423e-01 1.53265744e-02 7.06572056e-01\n",
      " 8.29519749e-01 8.86458933e-01 9.83561337e-01 7.27774739e-01\n",
      " 7.07532406e-01 4.56171542e-01 7.99346089e-01 8.47811818e-01\n",
      " 1.64222568e-02 1.31677195e-01 8.72469306e-01 3.04246604e-01\n",
      " 1.41517401e-01 9.98967409e-01 9.31385458e-01 8.68475854e-01\n",
      " 1.84908547e-02 1.54098824e-01 6.25871480e-01 9.44552004e-01\n",
      " 7.33128190e-01 8.29446554e-01 6.10115044e-02 6.92185163e-01\n",
      " 9.61143374e-02 3.48769635e-01 9.19623315e-01 2.53212750e-02\n",
      " 1.19387824e-02 7.75645860e-03 8.10100976e-03 3.60015705e-02\n",
      " 8.30158684e-03 4.51029301e-01 1.42387319e-02 8.36472679e-03\n",
      " 8.15417409e-01 9.04284239e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "vote_pred [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 42 TN= 42 FN= 16 FP= 18\n",
      "TP+FP 60\n",
      "precision 0.7\n",
      "recall 0.7241379310344828\n",
      "F1 0.711864406779661\n",
      "acc 0.711864406779661\n",
      "AUCp 0.7120689655172412\n",
      "AUC 0.7471264367816092\n",
      "\n",
      " The epoch is 120, average recall: 0.7241, average precision: 0.7000,average F1: 0.7119, average accuracy: 0.7119, average AUC: 0.7471\n",
      "Train Epoch: 121 [0/54 (0%)]\tTrain Loss: 0.031973\n",
      "Train Epoch: 121 [8/54 (15%)]\tTrain Loss: 0.051159\n",
      "Train Epoch: 121 [16/54 (30%)]\tTrain Loss: 0.029050\n",
      "Train Epoch: 121 [24/54 (44%)]\tTrain Loss: 0.011682\n",
      "Train Epoch: 121 [32/54 (59%)]\tTrain Loss: 0.019123\n",
      "Train Epoch: 121 [40/54 (74%)]\tTrain Loss: 0.040448\n",
      "Train Epoch: 121 [48/54 (89%)]\tTrain Loss: 0.016108\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.42238304 0.89997339 0.71778262 0.37171179 0.14900175 0.46572843\n",
      " 0.83024591 0.40355584 0.33033299 0.14661367 0.23621619 0.05854419\n",
      " 0.24858069 0.02741015 0.06395011 0.0565964  0.2742056  0.63885945\n",
      " 0.38958511 0.42430052 0.43900189 0.9476856  0.89071387 0.99664813\n",
      " 0.87146556 0.98811138 0.98679906 0.18771464 0.0943925  0.50169545\n",
      " 0.49825674 0.85712254 0.05327107 0.05384164 0.02091469 0.08308282\n",
      " 0.07115804 0.160919   0.71527576 0.49520683 0.38692304 0.4957861\n",
      " 0.22555889 0.1917396  0.07099003 0.09234508 0.304304   0.84050834\n",
      " 0.86577612 0.89773649 0.99430895 0.17343074 0.03867745 0.99337572\n",
      " 0.11259639 0.25851688 0.90137666 0.1796644  0.15527016 0.03029866\n",
      " 0.99980098 0.99029458 0.99975425 0.99915266 0.83259237 0.43550044\n",
      " 0.60862499 0.99495727 0.99810177 0.95005786 0.99541676 0.99093986\n",
      " 0.88725376 0.96202081 0.72599274 0.99711835 0.98804796 0.9399997\n",
      " 0.9126389  0.99770629 0.99968958 0.99979395 0.99909341 0.98441309\n",
      " 0.9985128  0.97975129 0.84901595 0.97556484 0.36871758 0.97943753\n",
      " 0.99236602 0.80069113 0.47047099 0.99963307 0.986292   0.98700571\n",
      " 0.05704691 0.9160493  0.99801362 0.99552035 0.80003977 0.99987996\n",
      " 0.23767242 0.95945114 0.57355511 0.78455395 0.98426855 0.03702295\n",
      " 0.07893396 0.04041259 0.08804094 0.50667089 0.46657422 0.95021462\n",
      " 0.50719672 0.48834515 0.94990498 0.9681437 ]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 122 [0/54 (0%)]\tTrain Loss: 0.074682\n",
      "Train Epoch: 122 [8/54 (15%)]\tTrain Loss: 0.041656\n",
      "Train Epoch: 122 [16/54 (30%)]\tTrain Loss: 0.062833\n",
      "Train Epoch: 122 [24/54 (44%)]\tTrain Loss: 0.062624\n",
      "Train Epoch: 122 [32/54 (59%)]\tTrain Loss: 0.030312\n",
      "Train Epoch: 122 [40/54 (74%)]\tTrain Loss: 0.037763\n",
      "Train Epoch: 122 [48/54 (89%)]\tTrain Loss: 0.038400\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.51003104e-01 1.25351503e-01 3.65723148e-02 1.21461255e-02\n",
      " 1.82252731e-02 4.04793441e-01 4.78854328e-02 2.36097779e-02\n",
      " 1.30449206e-01 2.59176642e-02 1.11951753e-02 8.51919502e-02\n",
      " 3.12705594e-03 1.46423886e-03 8.93759716e-04 6.45113084e-03\n",
      " 1.15360040e-02 1.75263286e-01 1.76525354e-01 1.94704924e-02\n",
      " 1.38085037e-01 1.60840347e-01 5.44688702e-01 9.96436715e-01\n",
      " 4.28933538e-02 8.94495487e-01 9.91095781e-01 3.81128043e-02\n",
      " 2.13460978e-02 7.66078709e-03 7.88281858e-02 1.31626591e-01\n",
      " 6.28293492e-03 5.69025390e-02 8.50118976e-03 1.12139853e-02\n",
      " 5.02330996e-03 4.34692353e-02 6.67972490e-02 3.25728133e-02\n",
      " 5.51365800e-02 7.16574565e-02 2.20908560e-02 7.41199171e-03\n",
      " 9.12792888e-03 6.77530542e-02 6.49883449e-02 5.70595980e-01\n",
      " 9.58535373e-01 9.56784487e-01 9.96035516e-01 2.56454642e-03\n",
      " 1.73947762e-03 1.19271707e-02 2.84459323e-01 1.63904577e-03\n",
      " 3.39172632e-01 1.39208091e-03 5.26727587e-02 5.40485082e-04\n",
      " 8.61782670e-01 6.39335990e-01 8.71124923e-01 9.11159456e-01\n",
      " 6.13156594e-02 2.06936635e-02 5.17393276e-02 2.34677255e-01\n",
      " 2.41085365e-01 8.85776699e-01 9.94411886e-01 9.85953391e-01\n",
      " 8.97441447e-01 9.01242077e-01 1.99820489e-01 6.72720253e-01\n",
      " 7.14057088e-01 6.78377330e-01 6.55219331e-02 7.57545888e-01\n",
      " 9.79545116e-01 9.78785098e-01 9.98313189e-01 5.80218852e-01\n",
      " 4.98615265e-01 9.82503712e-01 5.12661994e-01 7.35145390e-01\n",
      " 7.11881220e-02 4.61760104e-01 9.82532382e-01 2.87320405e-01\n",
      " 2.67887618e-02 9.97969687e-01 7.26143837e-01 7.51135468e-01\n",
      " 2.05670353e-02 2.18477651e-01 5.72530627e-01 5.42799771e-01\n",
      " 6.68437108e-02 9.02587533e-01 8.04939680e-03 4.59478021e-01\n",
      " 1.93685532e-01 4.13706571e-01 9.88107383e-01 2.62298491e-02\n",
      " 9.67029780e-02 2.96402741e-02 1.31965056e-01 6.34279907e-01\n",
      " 2.16508463e-01 3.16334069e-01 2.49885887e-01 1.20809533e-01\n",
      " 7.52698660e-01 1.56752959e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 123 [0/54 (0%)]\tTrain Loss: 0.037849\n",
      "Train Epoch: 123 [8/54 (15%)]\tTrain Loss: 0.031790\n",
      "Train Epoch: 123 [16/54 (30%)]\tTrain Loss: 0.070055\n",
      "Train Epoch: 123 [24/54 (44%)]\tTrain Loss: 0.054294\n",
      "Train Epoch: 123 [32/54 (59%)]\tTrain Loss: 0.037843\n",
      "Train Epoch: 123 [40/54 (74%)]\tTrain Loss: 0.058401\n",
      "Train Epoch: 123 [48/54 (89%)]\tTrain Loss: 0.028889\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.00834035 0.87407082 0.780101   0.33602169 0.15265374 0.01003017\n",
      " 0.71951026 0.4339501  0.01584933 0.01674915 0.08710203 0.10858548\n",
      " 0.04873311 0.40599164 0.07206064 0.0438372  0.11897577 0.39345852\n",
      " 0.27516857 0.36474907 0.5997147  0.83920908 0.8188315  0.99139762\n",
      " 0.74842542 0.97303164 0.99470454 0.45619965 0.09342404 0.05324469\n",
      " 0.2872749  0.46840721 0.00533299 0.06016542 0.03375917 0.0914574\n",
      " 0.08864121 0.21276256 0.86554849 0.69048315 0.56464666 0.73823994\n",
      " 0.06519255 0.12216627 0.11597627 0.03009098 0.02585828 0.39295202\n",
      " 0.92167705 0.9567309  0.90039456 0.00211438 0.04760619 0.07994435\n",
      " 0.0921591  0.06453642 0.68649417 0.05524227 0.17211917 0.04069824\n",
      " 0.93023574 0.95979542 0.9317925  0.88738132 0.67015624 0.78217733\n",
      " 0.4779081  0.99525732 0.9955942  0.94008923 0.97709095 0.98161465\n",
      " 0.81744021 0.98423254 0.71981525 0.58813858 0.97758055 0.95889795\n",
      " 0.42261311 0.96705103 0.98697412 0.9922806  0.99938893 0.95591223\n",
      " 0.76083213 0.81163681 0.93079931 0.94364756 0.65454841 0.84979397\n",
      " 0.99716884 0.83139437 0.63651425 0.99980408 0.95278674 0.9360261\n",
      " 0.06429733 0.80112922 0.29070276 0.96178442 0.35267845 0.95028675\n",
      " 0.11526792 0.76501733 0.26004073 0.6009903  0.81598282 0.0696378\n",
      " 0.14628369 0.01854726 0.21531256 0.7086792  0.67219657 0.89529979\n",
      " 0.54443353 0.20151861 0.98432642 0.88115859]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 124 [0/54 (0%)]\tTrain Loss: 0.050092\n",
      "Train Epoch: 124 [8/54 (15%)]\tTrain Loss: 0.049671\n",
      "Train Epoch: 124 [16/54 (30%)]\tTrain Loss: 0.088160\n",
      "Train Epoch: 124 [24/54 (44%)]\tTrain Loss: 0.091762\n",
      "Train Epoch: 124 [32/54 (59%)]\tTrain Loss: 0.058916\n",
      "Train Epoch: 124 [40/54 (74%)]\tTrain Loss: 0.031981\n",
      "Train Epoch: 124 [48/54 (89%)]\tTrain Loss: 0.025927\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.14843426e-02 5.71760774e-01 2.38778800e-01 9.18830093e-03\n",
      " 9.81846731e-03 6.19622367e-03 2.08624467e-01 3.60439606e-02\n",
      " 6.56893011e-03 4.12624963e-02 5.58184564e-01 1.59267575e-01\n",
      " 1.82942096e-02 9.74488212e-04 2.63095833e-03 5.99726045e-04\n",
      " 1.92430057e-03 2.38534883e-01 1.38098568e-01 1.49915814e-01\n",
      " 5.67367673e-01 3.69393229e-01 6.00228429e-01 9.83800352e-01\n",
      " 4.91416693e-01 9.33675110e-01 9.87591624e-01 3.45214456e-01\n",
      " 1.11850435e-02 5.35136648e-03 1.85370460e-01 9.32608470e-02\n",
      " 1.20467069e-02 2.09026337e-02 7.37974327e-03 8.83960165e-03\n",
      " 1.20298313e-02 4.74463739e-02 1.06523611e-01 8.91376808e-02\n",
      " 8.28353018e-02 2.13403478e-01 1.28669115e-02 3.80430669e-02\n",
      " 2.97253802e-02 9.98301804e-02 9.76186171e-02 3.76301795e-01\n",
      " 9.40615952e-01 8.53002667e-01 9.45759714e-01 2.90735712e-04\n",
      " 4.20518918e-03 2.42220890e-02 2.63083994e-01 5.47218742e-03\n",
      " 8.53652000e-01 8.04419909e-03 2.92013228e-01 1.10694778e-03\n",
      " 9.29828227e-01 8.86043787e-01 8.97843957e-01 8.06482494e-01\n",
      " 1.42683446e-01 1.19457832e-02 7.06735672e-03 9.54074442e-01\n",
      " 9.43759680e-01 5.87431550e-01 9.55919683e-01 9.34529603e-01\n",
      " 6.43380940e-01 7.07748711e-01 6.27276182e-01 8.07175457e-01\n",
      " 7.80923665e-01 7.65797079e-01 2.28007399e-02 9.59770560e-01\n",
      " 9.74722922e-01 9.81928587e-01 9.98574495e-01 4.08425838e-01\n",
      " 4.97848034e-01 8.93766582e-01 6.35164142e-01 6.80685818e-01\n",
      " 1.39693528e-01 8.51997852e-01 9.93415952e-01 2.51152247e-01\n",
      " 9.89588574e-02 9.99860168e-01 9.81263757e-01 9.80565429e-01\n",
      " 3.68407890e-02 3.90160263e-01 5.52251220e-01 7.27400839e-01\n",
      " 5.42172790e-02 8.95531356e-01 2.03348510e-02 9.42134023e-01\n",
      " 7.02547908e-01 6.55644000e-01 8.26632619e-01 1.35575430e-02\n",
      " 6.77125994e-03 1.85829140e-02 1.70409642e-02 8.03440288e-02\n",
      " 3.61518711e-02 7.90666163e-01 3.93188857e-02 3.65230292e-02\n",
      " 6.83740258e-01 8.04672956e-01]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 125 [0/54 (0%)]\tTrain Loss: 0.129899\n",
      "Train Epoch: 125 [8/54 (15%)]\tTrain Loss: 0.068889\n",
      "Train Epoch: 125 [16/54 (30%)]\tTrain Loss: 0.111920\n",
      "Train Epoch: 125 [24/54 (44%)]\tTrain Loss: 0.023713\n",
      "Train Epoch: 125 [32/54 (59%)]\tTrain Loss: 0.074654\n",
      "Train Epoch: 125 [40/54 (74%)]\tTrain Loss: 0.050198\n",
      "Train Epoch: 125 [48/54 (89%)]\tTrain Loss: 0.014449\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.06744415e-03 7.16319680e-01 5.90194225e-01 4.15033214e-02\n",
      " 1.33708445e-02 1.82970404e-03 4.87512171e-01 1.54244918e-02\n",
      " 1.23339728e-03 1.18601859e-01 5.32247186e-01 4.92607877e-02\n",
      " 2.48878356e-02 3.01896011e-06 2.59196822e-05 2.04281037e-04\n",
      " 4.03070450e-03 3.99267703e-01 7.19118565e-02 2.28201523e-01\n",
      " 4.50566828e-01 7.49853015e-01 8.32151353e-01 9.77658033e-01\n",
      " 6.09772742e-01 9.34861839e-01 9.78480697e-01 2.04114571e-01\n",
      " 1.44891478e-02 6.58541080e-03 1.86962515e-01 2.65575111e-01\n",
      " 3.00679286e-03 1.10912332e-02 9.01688263e-03 1.69364251e-02\n",
      " 1.26389842e-02 7.53327608e-02 6.29650176e-01 7.85882771e-01\n",
      " 6.68807983e-01 6.33360744e-01 2.50904784e-02 5.22551462e-02\n",
      " 4.23269980e-02 4.25666086e-02 6.93923887e-03 5.24678677e-02\n",
      " 9.32192385e-01 8.85231972e-01 8.25809479e-01 1.46478205e-03\n",
      " 4.25875094e-03 8.50231290e-01 6.06256276e-02 2.80770790e-02\n",
      " 8.66600394e-01 7.95354974e-03 3.78768176e-01 8.79139916e-05\n",
      " 9.86017942e-01 9.55936015e-01 9.91386652e-01 9.87060666e-01\n",
      " 1.91166028e-01 1.08405377e-03 7.78786256e-04 9.85325754e-01\n",
      " 9.94363666e-01 7.74485290e-01 9.63620245e-01 9.34166729e-01\n",
      " 8.56210887e-01 9.45397556e-01 7.02057600e-01 9.86431897e-01\n",
      " 9.49399471e-01 8.90274107e-01 6.55693471e-01 9.86289501e-01\n",
      " 9.95265126e-01 9.98893201e-01 9.99600351e-01 9.40948665e-01\n",
      " 9.99713242e-01 8.59347105e-01 9.89342093e-01 9.85507250e-01\n",
      " 1.34839147e-01 5.33698559e-01 9.86807406e-01 6.19420528e-01\n",
      " 2.65029252e-01 9.98842299e-01 8.56605530e-01 9.04535711e-01\n",
      " 6.03348389e-02 6.61619842e-01 8.25450957e-01 9.71343756e-01\n",
      " 8.02164152e-02 9.87331808e-01 4.69246469e-02 9.58794177e-01\n",
      " 4.04190093e-01 4.58680123e-01 8.47620547e-01 1.43318521e-02\n",
      " 2.78306892e-03 1.28729520e-02 7.60394474e-03 7.31068701e-02\n",
      " 7.86655620e-02 6.52952254e-01 7.00410008e-02 1.34340927e-01\n",
      " 7.72822618e-01 9.44842458e-01]\n",
      "predict [0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 126 [0/54 (0%)]\tTrain Loss: 0.033088\n",
      "Train Epoch: 126 [8/54 (15%)]\tTrain Loss: 0.016132\n",
      "Train Epoch: 126 [16/54 (30%)]\tTrain Loss: 0.031267\n",
      "Train Epoch: 126 [24/54 (44%)]\tTrain Loss: 0.040906\n",
      "Train Epoch: 126 [32/54 (59%)]\tTrain Loss: 0.079245\n",
      "Train Epoch: 126 [40/54 (74%)]\tTrain Loss: 0.076789\n",
      "Train Epoch: 126 [48/54 (89%)]\tTrain Loss: 0.069526\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.76860176e-03 6.94005936e-02 9.17192474e-02 3.94354295e-03\n",
      " 4.40367172e-03 5.97809162e-03 3.90473008e-02 5.59636531e-03\n",
      " 5.34518855e-03 2.89592408e-02 2.90260557e-02 9.30870250e-02\n",
      " 3.02949920e-03 1.40380487e-03 8.46376584e-04 8.42627021e-04\n",
      " 1.13802275e-03 2.18892768e-02 1.21205887e-02 1.62892081e-02\n",
      " 1.57630816e-01 1.10380854e-02 2.06478834e-01 9.50790524e-01\n",
      " 5.91985974e-03 5.80622077e-01 9.69013929e-01 2.43439917e-02\n",
      " 2.08004634e-03 2.39126338e-03 4.30678651e-02 3.43361124e-02\n",
      " 1.81464967e-03 2.28150375e-02 6.60754414e-03 5.78558445e-03\n",
      " 4.50223079e-03 2.36356724e-03 3.19258757e-02 2.98951287e-02\n",
      " 1.74913257e-02 6.86322898e-02 2.60827201e-03 1.79725816e-03\n",
      " 1.45359477e-03 2.88817734e-02 7.22025055e-04 6.90270402e-03\n",
      " 6.21613741e-01 9.28591728e-01 5.34668446e-01 1.85921119e-04\n",
      " 1.46662199e-03 1.67673558e-03 9.22781751e-02 2.45871348e-03\n",
      " 6.09556176e-02 3.44235846e-03 6.85999319e-02 4.28308878e-04\n",
      " 3.01155567e-01 2.70917505e-01 4.92052734e-01 4.82406855e-01\n",
      " 1.06805945e-02 5.67398174e-03 3.80534399e-03 2.38198623e-01\n",
      " 6.65106416e-01 8.56696129e-01 7.64484346e-01 7.65873790e-01\n",
      " 6.60985172e-01 7.95814693e-01 1.86209947e-01 2.08991058e-02\n",
      " 6.27178401e-02 6.59665614e-02 2.37883022e-03 3.17416698e-01\n",
      " 7.83566594e-01 8.60819340e-01 9.93413508e-01 2.50808001e-01\n",
      " 3.74634236e-01 7.38259435e-01 1.57607466e-01 2.68353760e-01\n",
      " 6.49377629e-02 3.90432835e-01 9.89117920e-01 4.25296992e-01\n",
      " 4.98627238e-02 9.98329103e-01 8.78910899e-01 8.32612395e-01\n",
      " 1.27731869e-02 1.53183460e-01 6.73688576e-02 9.47535634e-01\n",
      " 2.76456829e-02 1.80467919e-01 3.16817453e-03 6.10364377e-01\n",
      " 3.15563530e-01 4.08785820e-01 8.74951959e-01 1.78997200e-02\n",
      " 6.43775472e-03 5.13824122e-03 3.71244550e-02 2.11961284e-01\n",
      " 1.76526323e-01 7.10076511e-01 3.82955857e-02 3.73952347e-03\n",
      " 5.61747789e-01 3.48969996e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "Train Epoch: 127 [0/54 (0%)]\tTrain Loss: 0.052942\n",
      "Train Epoch: 127 [8/54 (15%)]\tTrain Loss: 0.077827\n",
      "Train Epoch: 127 [16/54 (30%)]\tTrain Loss: 0.010253\n",
      "Train Epoch: 127 [24/54 (44%)]\tTrain Loss: 0.111322\n",
      "Train Epoch: 127 [32/54 (59%)]\tTrain Loss: 0.016455\n",
      "Train Epoch: 127 [40/54 (74%)]\tTrain Loss: 0.007514\n",
      "Train Epoch: 127 [48/54 (89%)]\tTrain Loss: 0.061204\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.68198615e-03 2.09946290e-01 7.22021237e-02 8.58614407e-03\n",
      " 8.40348005e-03 3.24754533e-03 4.85576242e-02 4.35528979e-02\n",
      " 1.78397365e-03 6.78318553e-03 1.65587410e-01 1.42661417e-02\n",
      " 6.64635189e-03 1.59057134e-04 2.56968720e-04 7.50380917e-04\n",
      " 1.51493296e-03 3.35926786e-02 3.12444754e-02 3.06369346e-02\n",
      " 7.38045499e-02 1.72994629e-01 2.24574447e-01 6.70043766e-01\n",
      " 1.07432678e-01 4.45697755e-01 8.64317298e-01 3.07027511e-02\n",
      " 4.23331792e-03 1.82056555e-03 1.97227281e-02 7.19955713e-02\n",
      " 8.57753970e-04 3.73991043e-03 1.95211789e-03 4.24076151e-03\n",
      " 5.35128871e-03 1.18897585e-02 1.82184085e-01 6.94855526e-02\n",
      " 4.93345894e-02 1.27631441e-01 2.93955090e-03 5.67967072e-03\n",
      " 6.57967711e-03 2.20988300e-02 3.37239206e-02 3.87186527e-01\n",
      " 3.99866700e-01 5.20147383e-01 4.36085701e-01 2.90261814e-04\n",
      " 1.03334989e-03 1.01866201e-02 9.59224999e-03 1.74576812e-03\n",
      " 4.27654624e-01 1.00813620e-03 2.10109111e-02 3.27306450e-04\n",
      " 9.30358589e-01 8.79564404e-01 9.74350333e-01 9.03568625e-01\n",
      " 3.47705036e-02 8.51023756e-03 4.14633425e-03 9.44091201e-01\n",
      " 6.74355388e-01 8.01190436e-01 9.50950384e-01 9.40761447e-01\n",
      " 3.17059845e-01 6.12531543e-01 2.57553011e-01 9.76822436e-01\n",
      " 8.12721193e-01 7.77397513e-01 4.42416444e-02 5.01725614e-01\n",
      " 8.41804087e-01 8.94430220e-01 9.91814911e-01 3.13015521e-01\n",
      " 3.53058457e-01 3.75368416e-01 2.67096192e-01 4.73960042e-01\n",
      " 4.54869354e-03 1.89878955e-01 7.04955637e-01 2.97166675e-01\n",
      " 2.06958782e-02 9.69781697e-01 5.54526508e-01 3.25978369e-01\n",
      " 5.41817816e-03 3.97561282e-01 2.00767517e-01 8.79765213e-01\n",
      " 7.12575436e-01 9.55888927e-01 1.92903858e-02 7.88724422e-01\n",
      " 1.76088482e-01 1.84892550e-01 6.78780854e-01 1.77230639e-03\n",
      " 1.62929366e-03 2.61899154e-03 5.58219012e-03 8.94625857e-03\n",
      " 5.51241683e-03 3.86193395e-01 1.31127769e-02 8.01184308e-03\n",
      " 7.04556286e-01 9.21142220e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "Train Epoch: 128 [0/54 (0%)]\tTrain Loss: 0.017065\n",
      "Train Epoch: 128 [8/54 (15%)]\tTrain Loss: 0.009923\n",
      "Train Epoch: 128 [16/54 (30%)]\tTrain Loss: 0.012900\n",
      "Train Epoch: 128 [24/54 (44%)]\tTrain Loss: 0.042578\n",
      "Train Epoch: 128 [32/54 (59%)]\tTrain Loss: 0.025299\n",
      "Train Epoch: 128 [40/54 (74%)]\tTrain Loss: 0.027665\n",
      "Train Epoch: 128 [48/54 (89%)]\tTrain Loss: 0.062822\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.04692318e-03 9.30471361e-01 7.82374084e-01 2.75319189e-01\n",
      " 7.43905827e-02 5.84955327e-03 8.50496769e-01 6.52009428e-01\n",
      " 1.83725508e-03 7.21631292e-03 7.00318038e-01 5.13759721e-03\n",
      " 5.88383377e-01 9.80546921e-02 1.38881236e-01 1.80341173e-02\n",
      " 7.90668428e-02 1.09064177e-01 3.92020494e-02 1.05228283e-01\n",
      " 1.95013255e-01 3.50618064e-01 3.26099068e-01 8.42782378e-01\n",
      " 2.04275444e-01 7.38848209e-01 9.25190389e-01 5.67875743e-01\n",
      " 3.52803990e-02 4.93057147e-02 4.65857476e-01 2.56624937e-01\n",
      " 1.07717164e-01 1.54986149e-02 3.53986472e-02 2.65619546e-01\n",
      " 8.67143795e-02 1.24241360e-01 4.38581854e-01 7.88737774e-01\n",
      " 6.38454437e-01 5.84424138e-01 2.78818626e-02 2.05128983e-01\n",
      " 7.90910646e-02 7.30575085e-01 2.57683784e-01 8.57924223e-01\n",
      " 9.76198912e-01 8.27956557e-01 9.83099043e-01 2.97450647e-03\n",
      " 1.35184124e-01 9.65694308e-01 9.99653488e-02 1.03057630e-01\n",
      " 7.21105456e-01 2.07486585e-01 1.21866204e-01 2.68018078e-02\n",
      " 9.87674952e-01 8.91738772e-01 9.85817373e-01 9.69361126e-01\n",
      " 7.89522529e-01 3.13923836e-01 9.18601081e-02 9.95764494e-01\n",
      " 9.99181926e-01 8.59959126e-01 9.63223219e-01 9.26264584e-01\n",
      " 7.91383386e-01 9.56653535e-01 8.82630885e-01 9.68851507e-01\n",
      " 9.99235392e-01 9.94588971e-01 9.70363140e-01 9.94042099e-01\n",
      " 9.97721493e-01 9.98198450e-01 9.99421835e-01 4.31984961e-01\n",
      " 6.02237463e-01 7.36110032e-01 3.67553979e-01 7.32243836e-01\n",
      " 3.49817164e-02 9.17119622e-01 9.68797147e-01 8.66631329e-01\n",
      " 2.54027784e-01 9.99416471e-01 9.81127858e-01 9.57133710e-01\n",
      " 6.55211285e-02 7.65859246e-01 9.59112406e-01 9.92336571e-01\n",
      " 8.72897267e-01 9.57102835e-01 2.43611351e-01 7.78466523e-01\n",
      " 4.21933681e-01 2.78728604e-01 8.87309313e-01 8.50980077e-03\n",
      " 4.31339460e-04 5.64434752e-03 5.01892390e-03 3.79640870e-02\n",
      " 4.44480032e-01 8.64037812e-01 3.73755917e-02 3.09191477e-02\n",
      " 9.78546202e-01 9.43133354e-01]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 129 [0/54 (0%)]\tTrain Loss: 0.019532\n",
      "Train Epoch: 129 [8/54 (15%)]\tTrain Loss: 0.058166\n",
      "Train Epoch: 129 [16/54 (30%)]\tTrain Loss: 0.075345\n",
      "Train Epoch: 129 [24/54 (44%)]\tTrain Loss: 0.065795\n",
      "Train Epoch: 129 [32/54 (59%)]\tTrain Loss: 0.010150\n",
      "Train Epoch: 129 [40/54 (74%)]\tTrain Loss: 0.090863\n",
      "Train Epoch: 129 [48/54 (89%)]\tTrain Loss: 0.011234\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [8.88675451e-03 1.92821577e-01 1.05342269e-01 1.38856908e-02\n",
      " 7.12420838e-03 7.90653378e-03 8.83670002e-02 2.91571338e-02\n",
      " 4.95949574e-03 1.08362024e-03 8.62731263e-02 2.44137389e-03\n",
      " 1.10711064e-02 1.81762257e-03 1.27500389e-03 3.81302962e-04\n",
      " 6.72776194e-04 1.87022295e-02 1.29572731e-02 3.38228866e-02\n",
      " 1.23533309e-01 8.85526463e-02 2.43647456e-01 8.75841737e-01\n",
      " 3.80317308e-02 5.74155152e-01 7.18710124e-01 1.98551998e-01\n",
      " 4.91346326e-03 4.07136092e-03 1.41351223e-01 8.84642005e-02\n",
      " 9.56733897e-03 2.57327128e-03 3.41891521e-03 8.86625890e-03\n",
      " 1.14222029e-02 9.63766687e-03 9.70107093e-02 7.66836852e-02\n",
      " 6.98726550e-02 7.87720755e-02 2.17575952e-03 2.84735626e-03\n",
      " 3.32645047e-03 5.32761030e-02 6.92951456e-02 2.07990587e-01\n",
      " 8.31424475e-01 5.73448122e-01 7.02045500e-01 1.19493117e-04\n",
      " 1.60346203e-03 1.55827040e-02 6.39335290e-02 3.11005325e-03\n",
      " 2.00624779e-01 3.47026694e-03 5.05823791e-02 1.08204328e-03\n",
      " 7.02062309e-01 3.82296771e-01 7.01994359e-01 5.78070045e-01\n",
      " 3.96837026e-01 1.84730832e-02 1.12782056e-02 6.50232136e-01\n",
      " 8.22655857e-01 7.09575713e-01 5.27179539e-01 3.62518996e-01\n",
      " 4.09886569e-01 6.40248895e-01 7.38539457e-01 4.53254223e-01\n",
      " 6.74316764e-01 6.47775829e-01 3.61115634e-02 5.78546703e-01\n",
      " 7.08636642e-01 9.43416655e-01 9.91800964e-01 2.55829304e-01\n",
      " 3.73503864e-01 1.97557628e-01 1.08571254e-01 2.41521448e-01\n",
      " 2.00671609e-02 1.20325103e-01 6.39658213e-01 2.83433557e-01\n",
      " 2.09887773e-02 9.95149553e-01 6.28449857e-01 4.22494143e-01\n",
      " 7.52175087e-03 1.00799322e-01 4.24504757e-01 8.14670682e-01\n",
      " 1.30443886e-01 5.00682712e-01 7.23463390e-03 2.73194462e-01\n",
      " 2.41806693e-02 5.01462854e-02 7.35645115e-01 5.05642733e-03\n",
      " 4.17406717e-03 2.67539523e-03 2.86485869e-02 1.31446227e-01\n",
      " 2.61818301e-02 4.14454877e-01 8.55497178e-03 2.15648720e-03\n",
      " 3.58045936e-01 5.33129752e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 130 [0/54 (0%)]\tTrain Loss: 0.044470\n",
      "Train Epoch: 130 [8/54 (15%)]\tTrain Loss: 0.010454\n",
      "Train Epoch: 130 [16/54 (30%)]\tTrain Loss: 0.062626\n",
      "Train Epoch: 130 [24/54 (44%)]\tTrain Loss: 0.039243\n",
      "Train Epoch: 130 [32/54 (59%)]\tTrain Loss: 0.025271\n",
      "Train Epoch: 130 [40/54 (74%)]\tTrain Loss: 0.075388\n",
      "Train Epoch: 130 [48/54 (89%)]\tTrain Loss: 0.035094\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.88346213 0.99999619 0.99999595 0.99998713 0.99698395 0.96430469\n",
      " 0.99997175 0.99998558 0.99989796 0.42540824 0.26644197 0.01924639\n",
      " 0.84341675 0.55714583 0.53466278 0.07256906 0.31802249 0.64272362\n",
      " 0.40205517 0.60589516 0.43482316 0.98645419 0.99577564 0.99927408\n",
      " 0.75682878 0.96045387 0.99151176 0.81325465 0.21319368 0.16534421\n",
      " 0.99215937 0.96991885 0.74676543 0.01907104 0.01413027 0.2555638\n",
      " 0.13744558 0.68119746 0.65578848 0.72209924 0.49135947 0.72325581\n",
      " 0.62456495 0.92190993 0.65325981 0.99972159 0.99999988 1.\n",
      " 0.99999785 0.99593508 0.99999964 0.78961289 0.0221914  0.99999166\n",
      " 0.13447267 0.12249301 0.99475151 0.07848276 0.25616744 0.64243698\n",
      " 0.99996126 0.99995625 0.99999154 0.99993896 0.99999619 0.99408299\n",
      " 0.98884588 1.         1.         0.98051608 1.         1.\n",
      " 0.99925011 0.9983663  0.99560505 0.99999976 1.         1.\n",
      " 1.         0.99999094 0.99999857 0.99999642 0.9999975  0.9623208\n",
      " 0.99978489 0.9932816  0.99949396 0.99989903 0.06689139 0.82867295\n",
      " 0.89163423 0.84604633 0.77211112 0.99355644 0.90253443 0.39761975\n",
      " 0.05605447 0.63819021 0.96503884 0.8891837  0.97737086 0.99984848\n",
      " 0.35111326 0.96278048 0.9369368  0.79511362 0.98534644 0.02672262\n",
      " 0.01234046 0.00697295 0.02937266 0.02112991 0.03348476 0.79976273\n",
      " 0.11871555 0.96328193 0.98373359 0.96512467]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.]\n",
      "vote_pred [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 35 TN= 51 FN= 23 FP= 9\n",
      "TP+FP 44\n",
      "precision 0.7954545454545454\n",
      "recall 0.603448275862069\n",
      "F1 0.6862745098039215\n",
      "acc 0.7288135593220338\n",
      "AUCp 0.7267241379310344\n",
      "AUC 0.7741379310344828\n",
      "\n",
      " The epoch is 130, average recall: 0.6034, average precision: 0.7955,average F1: 0.6863, average accuracy: 0.7288, average AUC: 0.7741\n",
      "Train Epoch: 131 [0/54 (0%)]\tTrain Loss: 0.022768\n",
      "Train Epoch: 131 [8/54 (15%)]\tTrain Loss: 0.114511\n",
      "Train Epoch: 131 [16/54 (30%)]\tTrain Loss: 0.050681\n",
      "Train Epoch: 131 [24/54 (44%)]\tTrain Loss: 0.069501\n",
      "Train Epoch: 131 [32/54 (59%)]\tTrain Loss: 0.013563\n",
      "Train Epoch: 131 [40/54 (74%)]\tTrain Loss: 0.033288\n",
      "Train Epoch: 131 [48/54 (89%)]\tTrain Loss: 0.048941\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [6.42883405e-02 1.61254153e-01 4.95544411e-02 7.92510435e-02\n",
      " 1.20169163e-01 2.45609414e-02 3.74621563e-02 6.71057701e-02\n",
      " 1.50148235e-02 5.23368046e-02 4.15055007e-01 1.13210768e-01\n",
      " 3.67436856e-02 8.09419551e-04 2.67890398e-04 1.60346460e-02\n",
      " 2.23840829e-02 1.80606335e-01 1.76378235e-01 3.09275705e-02\n",
      " 3.19839537e-01 3.57832611e-01 5.06504476e-01 9.41422582e-01\n",
      " 8.83265585e-02 7.82708287e-01 9.96104360e-01 9.25188437e-02\n",
      " 2.08551213e-02 1.58861261e-02 4.30948198e-01 5.57429612e-01\n",
      " 6.67637527e-01 6.73617944e-02 8.90706256e-02 1.97570950e-01\n",
      " 9.92087498e-02 1.26287743e-01 7.24373162e-01 5.03110647e-01\n",
      " 4.93243635e-01 4.63736773e-01 3.99281681e-02 4.06526849e-02\n",
      " 7.44476309e-03 4.48946565e-01 7.30815887e-01 9.60482240e-01\n",
      " 9.68054533e-01 9.85969067e-01 9.97088730e-01 2.60322122e-03\n",
      " 7.64921890e-04 7.95466676e-02 3.61723602e-01 2.07735505e-03\n",
      " 7.59497821e-01 4.06266470e-03 2.03806814e-03 4.25321836e-04\n",
      " 9.91893589e-01 9.51447189e-01 9.96085525e-01 9.95866895e-01\n",
      " 2.00979829e-01 1.32190362e-01 1.86000496e-01 9.44973469e-01\n",
      " 5.71914732e-01 9.95418191e-01 9.86198246e-01 9.72518921e-01\n",
      " 9.95906711e-01 9.97970045e-01 9.78094876e-01 9.72925544e-01\n",
      " 9.39447224e-01 8.79891098e-01 1.09402835e-02 9.80134785e-01\n",
      " 9.96463716e-01 9.97737885e-01 9.98723447e-01 9.66257274e-01\n",
      " 9.12751913e-01 9.87515211e-01 1.14181951e-01 3.21798295e-01\n",
      " 2.63777710e-02 9.34981704e-01 9.96184766e-01 9.33556676e-01\n",
      " 2.74689794e-01 9.99915600e-01 9.93737817e-01 9.58827496e-01\n",
      " 8.57332442e-03 3.60651612e-01 8.48858595e-01 9.97201204e-01\n",
      " 2.15002775e-01 9.52168047e-01 1.97123699e-02 9.80556488e-01\n",
      " 8.89113188e-01 6.01805925e-01 9.66683626e-01 5.99331707e-02\n",
      " 6.38372526e-02 3.36997770e-02 1.54172018e-01 8.75958949e-02\n",
      " 9.87867355e-01 9.92916107e-01 7.56740272e-02 8.07774253e-03\n",
      " 5.43288589e-01 6.13649189e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 132 [0/54 (0%)]\tTrain Loss: 0.033150\n",
      "Train Epoch: 132 [8/54 (15%)]\tTrain Loss: 0.015081\n",
      "Train Epoch: 132 [16/54 (30%)]\tTrain Loss: 0.084842\n",
      "Train Epoch: 132 [24/54 (44%)]\tTrain Loss: 0.028975\n",
      "Train Epoch: 132 [32/54 (59%)]\tTrain Loss: 0.042948\n",
      "Train Epoch: 132 [40/54 (74%)]\tTrain Loss: 0.010836\n",
      "Train Epoch: 132 [48/54 (89%)]\tTrain Loss: 0.058891\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.10659655 0.96263957 0.92933637 0.66145676 0.28091288 0.03032673\n",
      " 0.77503097 0.68210322 0.01117468 0.01299047 0.35951644 0.07850067\n",
      " 0.04245986 0.06478247 0.18560801 0.02130058 0.18750027 0.60274982\n",
      " 0.29064369 0.17724657 0.33584207 0.87074894 0.90203792 0.9965204\n",
      " 0.61713469 0.95974952 0.99434537 0.37724924 0.04766717 0.05858335\n",
      " 0.22574717 0.68783802 0.76122624 0.02280723 0.01808406 0.33063316\n",
      " 0.09248884 0.35823074 0.79602194 0.47186744 0.44442949 0.53417563\n",
      " 0.3770982  0.45777991 0.11390135 0.11829444 0.64654559 0.98397684\n",
      " 0.97998041 0.93985212 0.9981091  0.03066384 0.05864822 0.94335079\n",
      " 0.1287515  0.06399133 0.88743705 0.01867863 0.53331667 0.01403982\n",
      " 0.99098301 0.93850362 0.99368906 0.99282348 0.71241206 0.13472643\n",
      " 0.24693041 0.98136789 0.99010676 0.99679166 0.98607594 0.95565373\n",
      " 0.97591656 0.9578734  0.84974432 0.9982698  0.99949265 0.99875784\n",
      " 0.97696757 0.98764277 0.99513763 0.99561191 0.99880314 0.98672837\n",
      " 0.98909503 0.98858672 0.79549479 0.92962861 0.0495911  0.7163226\n",
      " 0.97159582 0.91147512 0.38572234 0.99944657 0.92509305 0.73122072\n",
      " 0.08079202 0.86288905 0.94742811 0.94809371 0.66485125 0.95100719\n",
      " 0.20760028 0.88989466 0.42231399 0.32484385 0.98077893 0.033863\n",
      " 0.00990679 0.05415841 0.07065235 0.05852805 0.1726878  0.9559747\n",
      " 0.06847444 0.1028375  0.99598724 0.955064  ]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 133 [0/54 (0%)]\tTrain Loss: 0.072757\n",
      "Train Epoch: 133 [8/54 (15%)]\tTrain Loss: 0.055272\n",
      "Train Epoch: 133 [16/54 (30%)]\tTrain Loss: 0.065622\n",
      "Train Epoch: 133 [24/54 (44%)]\tTrain Loss: 0.027568\n",
      "Train Epoch: 133 [32/54 (59%)]\tTrain Loss: 0.052632\n",
      "Train Epoch: 133 [40/54 (74%)]\tTrain Loss: 0.039471\n",
      "Train Epoch: 133 [48/54 (89%)]\tTrain Loss: 0.039902\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.36728123 0.28507251 0.17961133 0.39259019 0.17165123 0.04470851\n",
      " 0.1768726  0.65529764 0.08711956 0.01569939 0.17595381 0.03966792\n",
      " 0.26041746 0.27892271 0.30873135 0.01004027 0.99193758 0.15908797\n",
      " 0.08049963 0.30164498 0.2530264  0.79782271 0.81419182 0.94564694\n",
      " 0.53256148 0.92434698 0.95450306 0.61918968 0.15612242 0.08333109\n",
      " 0.38002524 0.54376525 0.42416298 0.02162787 0.05599547 0.02708529\n",
      " 0.04484775 0.14522758 0.76473582 0.67016542 0.65365297 0.70434636\n",
      " 0.03652156 0.09748653 0.08697513 0.50164449 0.73321337 0.6307655\n",
      " 0.99153686 0.82140321 0.99567634 0.02422681 0.52963883 0.97378767\n",
      " 0.06644133 0.10783789 0.83900779 0.05411676 0.39213786 0.66797227\n",
      " 0.99732012 0.97343308 0.99103254 0.99280894 0.97149336 0.46926135\n",
      " 0.64408475 0.997491   0.94371438 0.90918511 0.98992145 0.99026883\n",
      " 0.96077049 0.99044633 0.97461557 0.98180264 0.94470721 0.87412262\n",
      " 0.9998222  0.99503052 0.99521244 0.99750072 0.99796873 0.99996591\n",
      " 0.99943095 0.74584872 0.98741049 0.99957913 0.15705596 0.11497521\n",
      " 0.86780632 0.90205139 0.91491091 0.99697459 0.70535773 0.73324698\n",
      " 0.30950114 0.36433095 0.37862858 0.9905048  0.4984479  0.98535097\n",
      " 0.21673931 0.53455204 0.28391558 0.65389192 0.98808473 0.10711415\n",
      " 0.01802313 0.03218466 0.0485004  0.49083513 0.3437264  0.89372337\n",
      " 0.09006757 0.05970194 0.79310441 0.69784546]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 134 [0/54 (0%)]\tTrain Loss: 0.041372\n",
      "Train Epoch: 134 [8/54 (15%)]\tTrain Loss: 0.029400\n",
      "Train Epoch: 134 [16/54 (30%)]\tTrain Loss: 0.042056\n",
      "Train Epoch: 134 [24/54 (44%)]\tTrain Loss: 0.037633\n",
      "Train Epoch: 134 [32/54 (59%)]\tTrain Loss: 0.025059\n",
      "Train Epoch: 134 [40/54 (74%)]\tTrain Loss: 0.032864\n",
      "Train Epoch: 134 [48/54 (89%)]\tTrain Loss: 0.014827\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.85940266e-01 9.97032046e-01 9.78190780e-01 8.86533678e-01\n",
      " 2.04995990e-01 8.69487375e-02 9.85182941e-01 9.69298780e-01\n",
      " 1.19686183e-02 1.56970971e-04 2.12331310e-01 1.37261786e-02\n",
      " 1.39839407e-02 1.06308684e-01 3.27715576e-01 4.41921502e-03\n",
      " 3.43741919e-03 2.97591925e-01 6.86173439e-02 1.99506328e-01\n",
      " 3.07589650e-01 6.31515741e-01 8.50910008e-01 9.63245451e-01\n",
      " 4.86198306e-01 8.64222348e-01 9.77315664e-01 8.24314952e-01\n",
      " 1.45048378e-02 2.11065672e-02 8.62613618e-01 6.60147488e-01\n",
      " 2.87175089e-01 3.14652896e-03 8.11198540e-03 7.74803897e-03\n",
      " 6.76216790e-03 2.64923722e-02 3.66768330e-01 4.59087975e-02\n",
      " 1.51960582e-01 1.71585884e-02 3.23310904e-02 8.89866799e-02\n",
      " 6.56483620e-02 9.05661583e-01 9.47412670e-01 9.83009875e-01\n",
      " 9.99236345e-01 9.27437603e-01 9.99530077e-01 6.29034534e-04\n",
      " 5.93165196e-02 1.92336403e-04 3.84540930e-02 3.21965851e-02\n",
      " 9.33145463e-01 4.70773317e-02 3.24044168e-01 3.32883775e-01\n",
      " 9.93956804e-01 9.81709540e-01 9.97182131e-01 9.97090220e-01\n",
      " 9.69062686e-01 6.66458428e-01 7.20529318e-01 7.99560308e-01\n",
      " 9.99111593e-01 5.20144582e-01 9.61056888e-01 9.28797662e-01\n",
      " 9.91863132e-01 9.70314980e-01 8.61776888e-01 9.99294996e-01\n",
      " 9.99982595e-01 9.99955058e-01 9.26204264e-01 9.99817193e-01\n",
      " 9.99957442e-01 9.99935865e-01 9.99816716e-01 4.10393924e-01\n",
      " 2.61319874e-05 9.47903454e-01 3.01179349e-01 6.16863012e-01\n",
      " 8.36650878e-02 6.19101822e-01 9.30167198e-01 7.35652983e-01\n",
      " 1.97318435e-01 9.99725759e-01 9.77619410e-01 9.86335874e-01\n",
      " 2.39962842e-02 1.16387650e-01 4.57505107e-01 8.90321374e-01\n",
      " 3.80483381e-02 9.98181343e-01 3.29728425e-02 4.23428625e-01\n",
      " 1.60435066e-01 1.32145479e-01 4.03887659e-01 1.00577925e-03\n",
      " 8.88825220e-04 3.00376932e-03 2.64046714e-03 5.74807869e-03\n",
      " 2.64089704e-02 6.75847471e-01 2.91507021e-02 1.03898242e-03\n",
      " 9.44269836e-01 7.85539210e-01]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 135 [0/54 (0%)]\tTrain Loss: 0.075226\n",
      "Train Epoch: 135 [8/54 (15%)]\tTrain Loss: 0.032401\n",
      "Train Epoch: 135 [16/54 (30%)]\tTrain Loss: 0.051145\n",
      "Train Epoch: 135 [24/54 (44%)]\tTrain Loss: 0.028553\n",
      "Train Epoch: 135 [32/54 (59%)]\tTrain Loss: 0.037545\n",
      "Train Epoch: 135 [40/54 (74%)]\tTrain Loss: 0.035204\n",
      "Train Epoch: 135 [48/54 (89%)]\tTrain Loss: 0.065579\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.91658945e-03 2.18040049e-01 7.78316110e-02 2.11066157e-02\n",
      " 1.94139741e-02 2.97336392e-02 6.66427612e-02 7.55408453e-03\n",
      " 2.10769828e-02 4.37979121e-03 3.75078209e-02 8.54325108e-03\n",
      " 3.64303961e-03 5.33836021e-04 1.18092680e-03 7.70277402e-04\n",
      " 4.34554601e-03 3.46328318e-02 6.00213418e-03 6.57244911e-03\n",
      " 2.67338399e-02 3.48108783e-02 5.17350398e-02 6.82928622e-01\n",
      " 1.54335033e-02 2.44690821e-01 6.78695023e-01 1.98728647e-02\n",
      " 2.33487808e-03 2.08931044e-03 7.64638791e-03 6.68274611e-03\n",
      " 3.65019019e-04 7.45454198e-03 4.87062242e-03 2.69290502e-03\n",
      " 2.89323134e-03 9.75081231e-03 3.42987664e-02 1.43760787e-02\n",
      " 1.46574853e-02 1.36574330e-02 3.18543077e-03 4.70224675e-03\n",
      " 3.72131285e-03 3.29548633e-03 4.49040905e-02 3.52561712e-01\n",
      " 5.90269789e-02 7.52103508e-01 8.69181514e-01 6.90574932e-04\n",
      " 4.93708474e-04 5.73723167e-02 9.03178453e-02 1.39622565e-03\n",
      " 2.55011111e-01 1.03031599e-03 1.90114381e-03 3.14911653e-04\n",
      " 9.57113206e-01 9.47493374e-01 9.83833492e-01 9.79206920e-01\n",
      " 9.55493748e-03 1.33770816e-02 7.00075133e-03 4.49421406e-01\n",
      " 2.80685574e-01 2.79962838e-01 9.92926538e-01 9.79299486e-01\n",
      " 5.92929423e-01 6.12364054e-01 1.03088140e-01 9.58373547e-01\n",
      " 9.92895067e-01 9.82501686e-01 6.19652689e-01 8.01942706e-01\n",
      " 9.69152153e-01 9.88035738e-01 9.96519804e-01 2.40393281e-01\n",
      " 7.96245456e-01 7.17520833e-01 3.47350746e-01 5.87082684e-01\n",
      " 6.10378943e-03 6.32987544e-02 6.42119229e-01 2.75230825e-01\n",
      " 4.01510000e-02 9.64388251e-01 6.34183109e-01 4.33668405e-01\n",
      " 8.27354321e-04 1.03683569e-01 1.50938019e-01 4.84976798e-01\n",
      " 8.09779689e-02 9.24555421e-01 1.83997061e-02 4.26217705e-01\n",
      " 6.06355146e-02 4.29854214e-01 8.55126739e-01 3.54993064e-03\n",
      " 2.97875167e-03 7.20923115e-03 5.02805784e-03 3.59204300e-02\n",
      " 9.28695314e-03 2.94103861e-01 9.93896089e-03 6.61411369e-03\n",
      " 8.05598497e-01 5.48738182e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 136 [0/54 (0%)]\tTrain Loss: 0.020743\n",
      "Train Epoch: 136 [8/54 (15%)]\tTrain Loss: 0.041616\n",
      "Train Epoch: 136 [16/54 (30%)]\tTrain Loss: 0.038813\n",
      "Train Epoch: 136 [24/54 (44%)]\tTrain Loss: 0.035006\n",
      "Train Epoch: 136 [32/54 (59%)]\tTrain Loss: 0.058589\n",
      "Train Epoch: 136 [40/54 (74%)]\tTrain Loss: 0.095070\n",
      "Train Epoch: 136 [48/54 (89%)]\tTrain Loss: 0.016431\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [6.91829389e-03 6.52853966e-01 3.26373428e-01 4.63417798e-01\n",
      " 7.36910552e-02 3.59038706e-03 2.35955775e-01 3.21255535e-01\n",
      " 1.39384516e-04 1.38473203e-02 8.95819783e-01 9.08958986e-02\n",
      " 5.19662857e-01 2.05547847e-02 1.76451784e-02 5.71577884e-02\n",
      " 2.33610831e-02 6.99402034e-01 4.79694784e-01 1.28487244e-01\n",
      " 4.20631677e-01 8.51267695e-01 9.76703465e-01 9.95932996e-01\n",
      " 8.41635942e-01 9.94591296e-01 9.98255908e-01 7.29546905e-01\n",
      " 8.31051245e-02 1.83043495e-01 8.79624426e-01 9.49120522e-01\n",
      " 8.43949839e-02 5.71933128e-02 1.48154765e-01 8.61182809e-02\n",
      " 7.74848834e-02 1.71468601e-01 8.92427325e-01 6.13267779e-01\n",
      " 7.74751842e-01 5.18182635e-01 1.78772762e-01 6.68672740e-01\n",
      " 1.69218555e-01 3.19011211e-02 8.69454026e-01 7.22920716e-01\n",
      " 9.83327687e-01 9.96869624e-01 9.70995545e-01 9.81648266e-03\n",
      " 2.94442009e-03 5.48453152e-01 5.51165998e-01 5.92410080e-02\n",
      " 9.95716035e-01 9.71245840e-02 6.21142626e-01 3.20563745e-03\n",
      " 9.97375965e-01 9.93658364e-01 9.99049127e-01 9.98901963e-01\n",
      " 9.01874304e-01 7.31395066e-01 6.64792597e-01 9.89515662e-01\n",
      " 9.98638213e-01 9.96729970e-01 9.91755366e-01 9.72635746e-01\n",
      " 9.99564469e-01 9.98431504e-01 9.94118929e-01 9.98817742e-01\n",
      " 9.95728910e-01 9.94678974e-01 9.14222956e-01 9.99088645e-01\n",
      " 9.99699950e-01 9.99619961e-01 9.99939919e-01 9.50685799e-01\n",
      " 8.18954229e-01 9.95876133e-01 9.66845870e-01 9.81652558e-01\n",
      " 1.82726651e-01 9.83014464e-01 9.99753296e-01 9.91544008e-01\n",
      " 7.72053957e-01 9.99994516e-01 9.96677160e-01 9.98473585e-01\n",
      " 1.97053194e-01 4.18928027e-01 9.68268216e-01 9.97483671e-01\n",
      " 9.71523821e-01 9.99443471e-01 9.04238075e-02 9.75585043e-01\n",
      " 8.32662642e-01 8.82238686e-01 9.87507999e-01 5.76619618e-02\n",
      " 7.40124583e-02 8.03869814e-02 3.49558383e-01 5.90455830e-01\n",
      " 9.86294687e-01 9.97028172e-01 1.63532227e-01 3.65526117e-02\n",
      " 9.81973648e-01 9.54318523e-01]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 137 [0/54 (0%)]\tTrain Loss: 0.021893\n",
      "Train Epoch: 137 [8/54 (15%)]\tTrain Loss: 0.044754\n",
      "Train Epoch: 137 [16/54 (30%)]\tTrain Loss: 0.086268\n",
      "Train Epoch: 137 [24/54 (44%)]\tTrain Loss: 0.046384\n",
      "Train Epoch: 137 [32/54 (59%)]\tTrain Loss: 0.217874\n",
      "Train Epoch: 137 [40/54 (74%)]\tTrain Loss: 0.021240\n",
      "Train Epoch: 137 [48/54 (89%)]\tTrain Loss: 0.043314\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [9.67051625e-01 9.99987364e-01 9.99790609e-01 9.92365062e-01\n",
      " 9.78634477e-01 3.60498726e-01 4.02387083e-01 9.76653397e-01\n",
      " 2.15840135e-02 8.09292669e-06 4.33180839e-01 4.87652188e-03\n",
      " 3.24823055e-03 3.81332964e-01 9.20408428e-01 1.08951349e-02\n",
      " 2.26828828e-02 4.85019445e-01 9.27309915e-02 1.44645035e-01\n",
      " 2.79804498e-01 5.62145650e-01 8.27711105e-01 9.89426970e-01\n",
      " 5.69191337e-01 9.30573046e-01 8.63025188e-01 8.16541553e-01\n",
      " 2.94296741e-01 5.25636613e-01 9.43980575e-01 9.96342123e-01\n",
      " 9.80470777e-01 3.66143836e-03 4.27987650e-02 4.24194857e-02\n",
      " 3.42282206e-02 3.84892255e-01 5.92061162e-01 1.28369376e-01\n",
      " 3.20366800e-01 1.63157314e-01 4.83282834e-01 9.56633568e-01\n",
      " 8.18660915e-01 8.57628882e-01 9.99710739e-01 9.99784529e-01\n",
      " 9.99914169e-01 9.94495332e-01 9.99922395e-01 1.45677537e-01\n",
      " 2.92539954e-01 5.00544794e-02 4.93640825e-02 4.76862311e-01\n",
      " 9.43950295e-01 3.72206330e-01 6.43797576e-01 9.39584136e-01\n",
      " 9.94308531e-01 3.37249786e-01 9.97585654e-01 9.98741329e-01\n",
      " 9.99454677e-01 9.81287837e-01 9.05514240e-01 5.27740836e-01\n",
      " 9.99952435e-01 4.53075022e-01 9.89494503e-01 9.54473495e-01\n",
      " 9.98658895e-01 9.97984648e-01 9.58619475e-01 9.99449909e-01\n",
      " 9.99997020e-01 9.99971509e-01 9.97670591e-01 9.99967933e-01\n",
      " 9.99978781e-01 9.99988914e-01 9.99921203e-01 8.60512078e-01\n",
      " 9.31960568e-02 9.84782875e-01 9.92828786e-01 9.88364458e-01\n",
      " 4.05481339e-01 6.12279117e-01 8.99623513e-01 9.95551646e-01\n",
      " 9.87103403e-01 9.99548018e-01 9.01220858e-01 9.37473774e-01\n",
      " 3.27538669e-01 1.33906305e-01 7.99148560e-01 7.57024765e-01\n",
      " 7.42227882e-02 9.99956608e-01 1.55911282e-01 9.28449392e-01\n",
      " 9.08786476e-01 8.56000066e-01 9.70071077e-01 3.30118905e-03\n",
      " 1.33808830e-03 1.30844777e-02 2.04846077e-02 4.46110964e-03\n",
      " 7.70068243e-02 3.23815405e-01 2.13744760e-01 3.32620391e-03\n",
      " 7.54998505e-01 8.43534648e-01]\n",
      "predict [1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "Train Epoch: 138 [0/54 (0%)]\tTrain Loss: 0.035028\n",
      "Train Epoch: 138 [8/54 (15%)]\tTrain Loss: 0.110481\n",
      "Train Epoch: 138 [16/54 (30%)]\tTrain Loss: 0.026944\n",
      "Train Epoch: 138 [24/54 (44%)]\tTrain Loss: 0.028935\n",
      "Train Epoch: 138 [32/54 (59%)]\tTrain Loss: 0.025992\n",
      "Train Epoch: 138 [40/54 (74%)]\tTrain Loss: 0.032314\n",
      "Train Epoch: 138 [48/54 (89%)]\tTrain Loss: 0.107327\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.40875402 0.9998017  0.99751651 0.72256136 0.65829849 0.44488293\n",
      " 0.98333782 0.26976126 0.29678252 0.02431912 0.05142458 0.05197654\n",
      " 0.04986561 0.02066343 0.03065323 0.02415852 0.01523991 0.29243252\n",
      " 0.05510498 0.02793929 0.33935633 0.31832382 0.70403773 0.99404573\n",
      " 0.08645848 0.95308155 0.99495041 0.11563537 0.04199488 0.0204033\n",
      " 0.07280559 0.88677627 0.12188026 0.02371365 0.02430974 0.0116797\n",
      " 0.01431523 0.04651977 0.28747326 0.18034199 0.09518564 0.14170043\n",
      " 0.06418278 0.05680294 0.07172891 0.23176073 0.99118012 0.99963772\n",
      " 0.99994659 0.99965322 0.99997342 0.0027286  0.00613093 0.06616801\n",
      " 0.79549813 0.01373279 0.74582511 0.00547738 0.01183415 0.1547697\n",
      " 0.99866354 0.9955408  0.998384   0.99895287 0.89914602 0.19436029\n",
      " 0.0623825  0.86680627 0.99378753 0.96639967 0.99977177 0.99886012\n",
      " 0.98218876 0.98722827 0.84911925 0.99187112 0.99998665 0.99998569\n",
      " 0.99947661 0.9993754  0.99993205 0.99991977 0.99999726 0.80426639\n",
      " 0.97069114 0.99804711 0.99654984 0.99866068 0.15792952 0.53494453\n",
      " 0.98446053 0.95454901 0.72522604 0.99937195 0.962466   0.44929305\n",
      " 0.0104042  0.71793067 0.96671748 0.88768214 0.2595135  0.99982917\n",
      " 0.03161193 0.97567874 0.94328892 0.97112167 0.9991197  0.01238001\n",
      " 0.04630863 0.0193805  0.04356641 0.11256292 0.09627577 0.92374676\n",
      " 0.22031771 0.22495319 0.93834972 0.62951231]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 139 [0/54 (0%)]\tTrain Loss: 0.035528\n",
      "Train Epoch: 139 [8/54 (15%)]\tTrain Loss: 0.039232\n",
      "Train Epoch: 139 [16/54 (30%)]\tTrain Loss: 0.031084\n",
      "Train Epoch: 139 [24/54 (44%)]\tTrain Loss: 0.007583\n",
      "Train Epoch: 139 [32/54 (59%)]\tTrain Loss: 0.018674\n",
      "Train Epoch: 139 [40/54 (74%)]\tTrain Loss: 0.007727\n",
      "Train Epoch: 139 [48/54 (89%)]\tTrain Loss: 0.024409\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.66914274e-03 9.99853611e-01 9.99500751e-01 3.66997153e-01\n",
      " 1.93371743e-01 3.43755307e-03 9.82056320e-01 3.33676636e-01\n",
      " 5.20130107e-03 2.68119993e-03 5.76676905e-01 5.07417647e-03\n",
      " 1.27777785e-01 2.95790116e-04 4.59763175e-03 1.03779929e-03\n",
      " 4.86869714e-04 5.04031122e-01 4.22630049e-02 3.55005078e-02\n",
      " 7.52352476e-02 9.09500003e-01 9.06848609e-01 9.60678816e-01\n",
      " 4.53307301e-01 8.46584439e-01 7.57782698e-01 1.37677729e-01\n",
      " 1.54318102e-02 2.07825322e-02 8.99909139e-02 8.58178675e-01\n",
      " 1.80580884e-01 4.85461485e-03 1.13116782e-02 2.96787079e-02\n",
      " 2.80245114e-02 5.25458418e-02 8.29879820e-01 3.97677749e-01\n",
      " 5.34890771e-01 5.34352958e-01 7.47115985e-02 1.04951620e-01\n",
      " 6.85208440e-02 1.52754653e-02 9.93045747e-01 9.99815404e-01\n",
      " 9.95484829e-01 9.54778790e-01 9.97890174e-01 9.59213008e-04\n",
      " 5.33110648e-03 6.10126793e-01 1.96558014e-02 1.49895251e-02\n",
      " 9.69226062e-01 1.65409059e-03 1.39375791e-01 3.98721127e-03\n",
      " 9.98284757e-01 9.97486949e-01 9.99614954e-01 9.99261916e-01\n",
      " 2.36512199e-01 3.11545692e-02 9.04912036e-03 9.91243124e-01\n",
      " 9.97060716e-01 8.24395478e-01 9.84895468e-01 9.04865563e-01\n",
      " 9.69648898e-01 9.26525474e-01 5.50820827e-01 9.99915957e-01\n",
      " 9.99998927e-01 9.99999285e-01 9.99793708e-01 9.99723494e-01\n",
      " 9.99899268e-01 9.99770224e-01 9.99976754e-01 9.71740246e-01\n",
      " 9.87538338e-01 9.90287066e-01 9.93211091e-01 9.98087704e-01\n",
      " 8.66284780e-03 4.55534279e-01 6.98853910e-01 9.87576723e-01\n",
      " 8.70593071e-01 9.98024106e-01 7.71502316e-01 6.02901220e-01\n",
      " 1.13051407e-01 8.99079144e-01 9.34283078e-01 9.86890674e-01\n",
      " 9.39996719e-01 9.99887705e-01 1.87273324e-01 9.76248443e-01\n",
      " 5.00161350e-01 7.56475747e-01 9.91552770e-01 2.13786727e-03\n",
      " 2.58075772e-03 9.74489469e-03 1.07838465e-02 3.25332843e-02\n",
      " 2.65025143e-02 9.18566883e-01 5.02233626e-03 6.98658451e-02\n",
      " 9.65800285e-01 9.34570730e-01]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 140 [0/54 (0%)]\tTrain Loss: 0.028456\n",
      "Train Epoch: 140 [8/54 (15%)]\tTrain Loss: 0.013764\n",
      "Train Epoch: 140 [16/54 (30%)]\tTrain Loss: 0.073856\n",
      "Train Epoch: 140 [24/54 (44%)]\tTrain Loss: 0.019940\n",
      "Train Epoch: 140 [32/54 (59%)]\tTrain Loss: 0.054069\n",
      "Train Epoch: 140 [40/54 (74%)]\tTrain Loss: 0.008908\n",
      "Train Epoch: 140 [48/54 (89%)]\tTrain Loss: 0.023827\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.38194633e-01 9.28834617e-01 7.92341411e-01 2.80707926e-01\n",
      " 1.23428963e-01 8.44029486e-02 7.10175991e-01 6.65743947e-01\n",
      " 3.25103812e-02 7.58064154e-04 2.01905537e-02 1.57416775e-03\n",
      " 9.72167030e-03 7.54611893e-03 1.01249777e-02 1.84759195e-03\n",
      " 7.88194942e-04 2.12591346e-02 3.54761956e-03 8.23986810e-03\n",
      " 8.54083002e-02 7.31522962e-02 8.76275972e-02 9.68558550e-01\n",
      " 1.15257213e-02 6.45665467e-01 9.43459392e-01 1.86077863e-01\n",
      " 3.31435190e-03 4.66983300e-03 3.53085101e-02 2.12186560e-01\n",
      " 2.17576325e-02 3.68237193e-03 6.10712962e-03 1.16921915e-02\n",
      " 1.16364071e-02 4.92765792e-02 1.52744696e-01 3.99810001e-02\n",
      " 8.14648792e-02 8.99898335e-02 1.31916339e-02 1.83345508e-02\n",
      " 1.06597496e-02 1.00675374e-01 6.24001801e-01 9.03276443e-01\n",
      " 9.86324489e-01 9.27819371e-01 9.94994402e-01 4.06612060e-04\n",
      " 3.06324894e-03 1.51589796e-01 2.55960915e-02 5.16944844e-03\n",
      " 2.67033130e-01 2.09779316e-03 6.58421293e-02 1.08885085e-02\n",
      " 9.51117039e-01 9.44327474e-01 9.63900685e-01 9.44344044e-01\n",
      " 7.65334129e-01 1.59631312e-01 2.88049001e-02 9.86423671e-01\n",
      " 9.97548521e-01 9.64470208e-01 8.88207197e-01 7.50640571e-01\n",
      " 8.30971003e-01 9.64101553e-01 7.26558208e-01 9.09807622e-01\n",
      " 9.98840868e-01 9.98848319e-01 6.38316125e-02 9.58975792e-01\n",
      " 9.95615005e-01 9.97447729e-01 9.99336541e-01 8.67158324e-02\n",
      " 3.66368264e-01 3.93794894e-01 1.83902949e-01 5.03896236e-01\n",
      " 3.79707292e-02 1.90625399e-01 9.41445947e-01 8.52368951e-01\n",
      " 2.44655848e-01 9.99909520e-01 8.14865351e-01 8.95397067e-01\n",
      " 9.70875379e-03 9.37600881e-02 6.62704885e-01 9.62671280e-01\n",
      " 1.10106044e-01 9.39608753e-01 1.04321660e-02 3.12433362e-01\n",
      " 3.63927968e-02 2.20600918e-01 9.15885806e-01 6.32434012e-03\n",
      " 7.72044389e-03 2.82003861e-02 4.98288348e-02 1.56934455e-01\n",
      " 1.08884469e-01 9.28461075e-01 4.91510984e-03 1.28350442e-03\n",
      " 4.25783873e-01 2.59956419e-01]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "vote_pred [0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 41 TN= 45 FN= 17 FP= 15\n",
      "TP+FP 56\n",
      "precision 0.7321428571428571\n",
      "recall 0.7068965517241379\n",
      "F1 0.719298245614035\n",
      "acc 0.7288135593220338\n",
      "AUCp 0.728448275862069\n",
      "AUC 0.7614942528735632\n",
      "\n",
      " The epoch is 140, average recall: 0.7069, average precision: 0.7321,average F1: 0.7193, average accuracy: 0.7288, average AUC: 0.7615\n",
      "Train Epoch: 141 [0/54 (0%)]\tTrain Loss: 0.005320\n",
      "Train Epoch: 141 [8/54 (15%)]\tTrain Loss: 0.065242\n",
      "Train Epoch: 141 [16/54 (30%)]\tTrain Loss: 0.012591\n",
      "Train Epoch: 141 [24/54 (44%)]\tTrain Loss: 0.109294\n",
      "Train Epoch: 141 [32/54 (59%)]\tTrain Loss: 0.052677\n",
      "Train Epoch: 141 [40/54 (74%)]\tTrain Loss: 0.017139\n",
      "Train Epoch: 141 [48/54 (89%)]\tTrain Loss: 0.050009\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.09403231 0.97684878 0.90763593 0.59172583 0.24978699 0.06928569\n",
      " 0.89736265 0.85720384 0.01764956 0.00702698 0.07771908 0.016174\n",
      " 0.03670504 0.03922418 0.07687362 0.00412246 0.01154175 0.20774746\n",
      " 0.07712005 0.06489725 0.17051406 0.64879775 0.68708426 0.90848815\n",
      " 0.29585126 0.93518931 0.87677562 0.32639548 0.01238647 0.01591943\n",
      " 0.48125681 0.69504654 0.33240256 0.00741311 0.0083475  0.01887741\n",
      " 0.02066882 0.26733935 0.45125628 0.10806335 0.09377144 0.25621042\n",
      " 0.14672893 0.25899777 0.17926303 0.22716373 0.9002986  0.96207631\n",
      " 0.95416963 0.77589655 0.98322898 0.00613946 0.01816941 0.63443935\n",
      " 0.04100849 0.08034458 0.70119524 0.01972464 0.14968428 0.04260555\n",
      " 0.99203354 0.97982454 0.99650723 0.99354219 0.98121798 0.6627686\n",
      " 0.38876444 0.99893528 0.99380589 0.61338532 0.88774621 0.8349371\n",
      " 0.96510857 0.95977676 0.9071129  0.99780411 0.99962366 0.99982029\n",
      " 0.99841213 0.99363989 0.99662364 0.99812454 0.99829859 0.5991208\n",
      " 0.54381001 0.77644992 0.69126445 0.88677382 0.04854666 0.08400559\n",
      " 0.71663296 0.74696636 0.69613051 0.99681455 0.79734784 0.84329134\n",
      " 0.1794523  0.18464544 0.55027604 0.73788369 0.37837195 0.98711061\n",
      " 0.05656232 0.67104739 0.19735901 0.66199464 0.84202015 0.00589532\n",
      " 0.00659229 0.0260875  0.02342797 0.04942428 0.06255709 0.60969424\n",
      " 0.03042798 0.01266524 0.88808799 0.89489126]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 142 [0/54 (0%)]\tTrain Loss: 0.017931\n",
      "Train Epoch: 142 [8/54 (15%)]\tTrain Loss: 0.076098\n",
      "Train Epoch: 142 [16/54 (30%)]\tTrain Loss: 0.026375\n",
      "Train Epoch: 142 [24/54 (44%)]\tTrain Loss: 0.014567\n",
      "Train Epoch: 142 [32/54 (59%)]\tTrain Loss: 0.016125\n",
      "Train Epoch: 142 [40/54 (74%)]\tTrain Loss: 0.017177\n",
      "Train Epoch: 142 [48/54 (89%)]\tTrain Loss: 0.019087\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [6.06149323e-02 4.80941474e-01 1.28325507e-01 6.79598808e-01\n",
      " 7.57143795e-02 2.06248034e-02 6.56084716e-01 9.49322641e-01\n",
      " 4.32824576e-03 1.31723266e-02 6.03035927e-01 5.41409329e-02\n",
      " 2.50740051e-01 1.98719446e-02 1.25026461e-02 3.33802565e-03\n",
      " 3.32701541e-02 3.72593254e-01 2.76267588e-01 9.37272608e-02\n",
      " 3.13201636e-01 9.89800751e-01 9.50369358e-01 9.97324109e-01\n",
      " 9.15727019e-01 9.97378826e-01 9.96922791e-01 8.87190402e-01\n",
      " 8.38994533e-02 4.62439321e-02 3.44608903e-01 3.06696236e-01\n",
      " 1.46796787e-02 1.09852012e-02 2.64077820e-02 1.13762707e-01\n",
      " 1.78343028e-01 8.41769874e-02 8.96590769e-01 3.88447225e-01\n",
      " 3.80328745e-01 6.77775979e-01 2.40562297e-02 4.43263769e-01\n",
      " 3.79148573e-01 2.25069225e-01 9.92507115e-02 4.22921449e-01\n",
      " 1.45002767e-01 9.30286765e-01 4.38365817e-01 8.96076439e-04\n",
      " 5.86207882e-02 6.10256046e-02 1.77886873e-01 3.43002900e-02\n",
      " 9.68470633e-01 5.07000461e-02 6.73722088e-01 6.46466836e-02\n",
      " 9.98362005e-01 9.97799098e-01 9.99421716e-01 9.98082757e-01\n",
      " 8.85013163e-01 1.72905907e-01 9.49254707e-02 9.99789655e-01\n",
      " 9.97567415e-01 9.74540949e-01 6.71793640e-01 5.28105617e-01\n",
      " 9.37433958e-01 9.65736330e-01 9.13170099e-01 9.95784342e-01\n",
      " 9.81415510e-01 9.65345800e-01 1.37470931e-01 9.92072046e-01\n",
      " 9.94594395e-01 9.93232131e-01 9.96845007e-01 9.79728043e-01\n",
      " 9.38618302e-01 9.67506588e-01 8.76255989e-01 8.96150172e-01\n",
      " 1.62016943e-01 9.58607018e-01 9.97907400e-01 7.54779816e-01\n",
      " 3.07176203e-01 9.99964356e-01 9.73569989e-01 9.86881852e-01\n",
      " 7.38690272e-02 4.69241917e-01 9.20047343e-01 9.80660915e-01\n",
      " 9.40025210e-01 9.91824925e-01 1.19173996e-01 9.27447021e-01\n",
      " 3.55009019e-01 6.16473377e-01 9.69601452e-01 1.23646939e-02\n",
      " 5.86695923e-03 1.50527535e-02 9.62230191e-02 5.53607047e-01\n",
      " 4.44844514e-01 9.98152792e-01 6.66345134e-02 1.30781040e-01\n",
      " 8.42580616e-01 7.73463190e-01]\n",
      "predict [0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 143 [0/54 (0%)]\tTrain Loss: 0.025987\n",
      "Train Epoch: 143 [8/54 (15%)]\tTrain Loss: 0.021152\n",
      "Train Epoch: 143 [16/54 (30%)]\tTrain Loss: 0.032353\n",
      "Train Epoch: 143 [24/54 (44%)]\tTrain Loss: 0.101500\n",
      "Train Epoch: 143 [32/54 (59%)]\tTrain Loss: 0.041398\n",
      "Train Epoch: 143 [40/54 (74%)]\tTrain Loss: 0.033759\n",
      "Train Epoch: 143 [48/54 (89%)]\tTrain Loss: 0.017539\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.20478734 0.79291892 0.5028863  0.73354805 0.19738691 0.20041414\n",
      " 0.41120389 0.7988494  0.02953721 0.01147419 0.04977069 0.0864163\n",
      " 0.23901765 0.13994913 0.02301519 0.00823001 0.6049192  0.24110079\n",
      " 0.32914102 0.12583451 0.38244954 0.79639995 0.69877291 0.96028894\n",
      " 0.47941512 0.96640211 0.93624699 0.228787   0.0586652  0.04176307\n",
      " 0.08799398 0.56963378 0.14630102 0.00865122 0.00600193 0.00795733\n",
      " 0.00892947 0.14769858 0.17621374 0.03282212 0.04208452 0.05317307\n",
      " 0.02836379 0.08079524 0.12252883 0.46371061 0.45265043 0.81561583\n",
      " 0.94920468 0.89561921 0.97639275 0.09242032 0.02539469 0.97340524\n",
      " 0.03271871 0.07104629 0.35875151 0.01978238 0.04249667 0.12801942\n",
      " 0.99118072 0.99018621 0.9955042  0.99040389 0.90423977 0.74210823\n",
      " 0.73072922 0.99372649 0.99681658 0.90160656 0.95116115 0.91103619\n",
      " 0.9597832  0.9916048  0.97349894 0.95739138 0.99640626 0.99227202\n",
      " 0.9533416  0.98965502 0.99833208 0.99864072 0.99761933 0.99285275\n",
      " 0.98243344 0.94948649 0.82194251 0.92103112 0.64559984 0.87584436\n",
      " 0.97952628 0.71349573 0.62907487 0.99955577 0.90235901 0.9579581\n",
      " 0.09172133 0.01261975 0.60222876 0.97580069 0.00483537 0.80781114\n",
      " 0.01182555 0.8178547  0.23782614 0.51364458 0.49688777 0.00326503\n",
      " 0.01711413 0.0279208  0.02185355 0.03002453 0.9679023  0.83170933\n",
      " 0.03229574 0.02930893 0.53689003 0.48144504]\n",
      "predict [0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Train Epoch: 144 [0/54 (0%)]\tTrain Loss: 0.030592\n",
      "Train Epoch: 144 [8/54 (15%)]\tTrain Loss: 0.014184\n",
      "Train Epoch: 144 [16/54 (30%)]\tTrain Loss: 0.023438\n",
      "Train Epoch: 144 [24/54 (44%)]\tTrain Loss: 0.017666\n",
      "Train Epoch: 144 [32/54 (59%)]\tTrain Loss: 0.026627\n",
      "Train Epoch: 144 [40/54 (74%)]\tTrain Loss: 0.086740\n",
      "Train Epoch: 144 [48/54 (89%)]\tTrain Loss: 0.029060\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.03337791 0.5943684  0.20436808 0.41558048 0.12318248 0.03313268\n",
      " 0.20898779 0.79838789 0.04946914 0.01829339 0.11953641 0.07587218\n",
      " 0.25409999 0.00850065 0.01458222 0.01384246 0.08022509 0.28086495\n",
      " 0.2338156  0.04744808 0.14507003 0.83074766 0.85268098 0.95359063\n",
      " 0.5003922  0.95612305 0.97039247 0.37230995 0.13932918 0.03847642\n",
      " 0.08094604 0.57286829 0.01663236 0.01198673 0.0141813  0.01839534\n",
      " 0.03626839 0.31781143 0.72031075 0.09607394 0.05219157 0.1304452\n",
      " 0.11445145 0.05675113 0.49033141 0.4264904  0.33934352 0.75507116\n",
      " 0.96941561 0.97037429 0.97159189 0.00314982 0.0845511  0.11324473\n",
      " 0.26967168 0.12930448 0.65177906 0.0110824  0.31619412 0.08175283\n",
      " 0.92752361 0.91696155 0.87634623 0.86661446 0.91379321 0.47892222\n",
      " 0.35091442 0.9943046  0.90432847 0.89533716 0.93317056 0.94115186\n",
      " 0.86807877 0.83791971 0.90281707 0.96127844 0.98062241 0.95796376\n",
      " 0.74969447 0.96778786 0.98256207 0.98968446 0.99677104 0.88830483\n",
      " 0.68458974 0.91934729 0.876387   0.95059586 0.08261302 0.14658688\n",
      " 0.80815136 0.77223468 0.57017511 0.99789917 0.89267623 0.85733396\n",
      " 0.20111208 0.17010659 0.20443639 0.67588669 0.36704224 0.97649711\n",
      " 0.09131576 0.90029043 0.56076908 0.75023103 0.9730444  0.00348932\n",
      " 0.00913629 0.01366505 0.00818262 0.0650031  0.08533099 0.86003524\n",
      " 0.12745364 0.09787591 0.9377802  0.57886261]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 145 [0/54 (0%)]\tTrain Loss: 0.052856\n",
      "Train Epoch: 145 [8/54 (15%)]\tTrain Loss: 0.026667\n",
      "Train Epoch: 145 [16/54 (30%)]\tTrain Loss: 0.033767\n",
      "Train Epoch: 145 [24/54 (44%)]\tTrain Loss: 0.038568\n",
      "Train Epoch: 145 [32/54 (59%)]\tTrain Loss: 0.095740\n",
      "Train Epoch: 145 [40/54 (74%)]\tTrain Loss: 0.031656\n",
      "Train Epoch: 145 [48/54 (89%)]\tTrain Loss: 0.034893\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.43430817e-03 5.50599635e-01 5.43068424e-02 6.03724271e-02\n",
      " 1.35576418e-02 2.11839052e-03 1.09604664e-01 1.09261848e-01\n",
      " 3.16025410e-03 2.02077022e-03 1.90591440e-01 1.80834811e-02\n",
      " 9.36679915e-03 2.10632849e-03 1.37924007e-03 4.70925029e-03\n",
      " 2.72966572e-03 1.14762373e-01 7.99467638e-02 1.69025846e-02\n",
      " 2.48472288e-01 2.39978030e-01 4.37989056e-01 9.58464861e-01\n",
      " 2.34678611e-01 8.15917552e-01 9.93713439e-01 5.16251959e-02\n",
      " 4.35621059e-03 2.57127569e-03 4.25169952e-02 7.96513781e-02\n",
      " 6.61838939e-03 8.82479828e-03 6.39051758e-03 4.95871156e-03\n",
      " 6.72497600e-03 3.76325324e-02 6.63743541e-02 3.70294526e-02\n",
      " 2.69444454e-02 3.89842801e-02 9.41753294e-03 2.73175985e-02\n",
      " 1.43151246e-02 5.05966507e-02 1.13851674e-01 5.11341453e-01\n",
      " 7.54595697e-01 8.96656394e-01 9.20200348e-01 5.70750155e-04\n",
      " 1.43305038e-03 4.08936851e-03 1.91162407e-01 2.75927619e-03\n",
      " 3.79764825e-01 3.02558090e-03 2.42018048e-02 2.48523080e-03\n",
      " 9.41996217e-01 9.26526487e-01 9.26276386e-01 9.28861141e-01\n",
      " 1.08137839e-01 7.09004030e-02 1.87016260e-02 9.45648193e-01\n",
      " 9.66829717e-01 9.11102891e-01 8.31124425e-01 7.28434145e-01\n",
      " 8.90089810e-01 9.34976041e-01 7.22775102e-01 4.20276791e-01\n",
      " 9.68557000e-01 9.30909276e-01 7.81209841e-02 8.34418595e-01\n",
      " 9.43492830e-01 9.72060382e-01 9.97335613e-01 7.63369322e-01\n",
      " 7.67869711e-01 7.80544817e-01 9.36722979e-02 4.62872833e-01\n",
      " 4.91920896e-02 8.08227360e-01 9.98465776e-01 2.04171389e-01\n",
      " 5.32939509e-02 9.99974370e-01 9.66722429e-01 9.62204456e-01\n",
      " 3.41945281e-03 7.64190033e-02 5.99816442e-01 8.72907698e-01\n",
      " 4.22577202e-01 4.88616407e-01 6.48064306e-03 9.07064021e-01\n",
      " 3.84155393e-01 5.86380064e-01 8.53700042e-01 4.58336988e-04\n",
      " 1.21338223e-03 3.43507691e-03 1.71402213e-03 3.18418676e-03\n",
      " 1.36470690e-01 8.68593276e-01 6.55277306e-03 3.65970843e-03\n",
      " 7.58278012e-01 9.33230042e-01]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 146 [0/54 (0%)]\tTrain Loss: 0.057315\n",
      "Train Epoch: 146 [8/54 (15%)]\tTrain Loss: 0.044158\n",
      "Train Epoch: 146 [16/54 (30%)]\tTrain Loss: 0.046528\n",
      "Train Epoch: 146 [24/54 (44%)]\tTrain Loss: 0.026798\n",
      "Train Epoch: 146 [32/54 (59%)]\tTrain Loss: 0.058036\n",
      "Train Epoch: 146 [40/54 (74%)]\tTrain Loss: 0.041005\n",
      "Train Epoch: 146 [48/54 (89%)]\tTrain Loss: 0.092551\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.73027639e-02 7.23731041e-01 3.63120697e-02 4.06352989e-02\n",
      " 4.63845171e-02 2.14806106e-02 2.58033097e-01 2.56036855e-02\n",
      " 4.69116401e-03 3.22172567e-02 1.43477485e-01 1.06511168e-01\n",
      " 1.75011009e-02 1.85820833e-03 5.20743022e-04 2.70941388e-03\n",
      " 1.06020188e-02 1.70080841e-01 4.17589366e-01 1.76670402e-01\n",
      " 6.70292675e-01 4.63674068e-01 8.88421118e-01 9.99415994e-01\n",
      " 5.26578069e-01 9.79350805e-01 9.99497294e-01 8.41061920e-02\n",
      " 1.27595477e-02 2.88559985e-03 2.11531699e-01 1.21502630e-01\n",
      " 1.72127725e-03 5.56705594e-02 1.69218797e-02 1.50520646e-03\n",
      " 5.31922327e-03 3.81086357e-02 5.01528025e-01 1.00329675e-01\n",
      " 7.25915879e-02 1.87158540e-01 1.50290709e-02 1.03740571e-02\n",
      " 8.41501262e-03 1.55220896e-01 1.69415876e-01 8.47523093e-01\n",
      " 9.07450914e-01 9.96822596e-01 9.92316127e-01 4.08317801e-03\n",
      " 1.38083915e-03 4.73234534e-01 1.97439462e-01 1.20491290e-03\n",
      " 6.96627557e-01 1.29880826e-03 4.97035570e-02 1.20808487e-03\n",
      " 9.85076249e-01 9.53793168e-01 9.90193009e-01 9.92452562e-01\n",
      " 7.85194933e-02 3.85304354e-02 2.78018285e-02 9.81156111e-01\n",
      " 8.84328246e-01 9.98778641e-01 9.83159959e-01 9.83072639e-01\n",
      " 9.47909892e-01 8.98876369e-01 7.85124600e-01 9.82918501e-01\n",
      " 9.95352864e-01 9.87673581e-01 9.37138021e-01 9.41541135e-01\n",
      " 9.92586195e-01 9.94862616e-01 9.99309778e-01 9.18334365e-01\n",
      " 9.70627666e-01 9.69372988e-01 8.26442122e-01 8.87691259e-01\n",
      " 7.01120272e-02 9.26630795e-01 9.99562919e-01 5.95609367e-01\n",
      " 4.57123891e-02 9.99963641e-01 9.66771424e-01 9.87338066e-01\n",
      " 1.79237872e-02 1.10434331e-01 9.25639272e-01 9.87272203e-01\n",
      " 7.37593949e-01 8.92919958e-01 1.06405225e-02 8.44913006e-01\n",
      " 6.92238212e-01 7.51431406e-01 9.95904267e-01 2.09331105e-04\n",
      " 1.65507357e-04 6.15241937e-04 3.60048696e-04 1.06672375e-02\n",
      " 5.86174009e-03 9.81115222e-01 1.54973641e-02 3.01128682e-02\n",
      " 9.55233872e-01 8.69966567e-01]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 147 [0/54 (0%)]\tTrain Loss: 0.036151\n",
      "Train Epoch: 147 [8/54 (15%)]\tTrain Loss: 0.011902\n",
      "Train Epoch: 147 [16/54 (30%)]\tTrain Loss: 0.105149\n",
      "Train Epoch: 147 [24/54 (44%)]\tTrain Loss: 0.088604\n",
      "Train Epoch: 147 [32/54 (59%)]\tTrain Loss: 0.008853\n",
      "Train Epoch: 147 [40/54 (74%)]\tTrain Loss: 0.013890\n",
      "Train Epoch: 147 [48/54 (89%)]\tTrain Loss: 0.027357\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.57406589e-02 9.99505281e-01 9.93629158e-01 7.07270145e-01\n",
      " 5.54162323e-01 2.69284286e-02 9.92387295e-01 9.17430043e-01\n",
      " 3.43565387e-03 6.93431357e-05 2.50992160e-02 1.68797038e-02\n",
      " 1.37173254e-02 5.90546196e-03 1.73402224e-02 1.87937345e-03\n",
      " 6.21643057e-03 3.77431989e-01 1.21190883e-01 1.61719456e-01\n",
      " 3.94883066e-01 7.30930030e-01 8.98383856e-01 9.91524637e-01\n",
      " 6.00547552e-01 9.66546178e-01 9.57461357e-01 1.21671997e-01\n",
      " 7.85218775e-02 2.64656860e-02 6.43412843e-02 4.38789159e-01\n",
      " 5.97338425e-04 2.88596423e-03 2.31038965e-03 3.63195455e-03\n",
      " 8.92485026e-03 4.06258494e-01 8.35577965e-01 1.10921793e-01\n",
      " 2.23056003e-01 1.23476153e-02 2.07350597e-01 2.49614388e-01\n",
      " 1.62836298e-01 1.74251050e-01 6.09871864e-01 9.99172091e-01\n",
      " 9.56482172e-01 9.98905063e-01 9.97535825e-01 1.09635992e-02\n",
      " 8.06173123e-03 9.84542887e-04 3.08385760e-01 3.25641520e-02\n",
      " 5.63829362e-01 1.16281472e-02 1.05871305e-01 1.07087605e-02\n",
      " 9.99610722e-01 9.98181224e-01 9.99702752e-01 9.99878883e-01\n",
      " 8.08619976e-01 1.59269497e-01 5.14873974e-02 9.59641457e-01\n",
      " 9.97099876e-01 9.35875833e-01 9.50496852e-01 9.32374001e-01\n",
      " 9.36785758e-01 9.01746809e-01 9.23665762e-01 9.99754965e-01\n",
      " 9.99998212e-01 9.99997258e-01 9.95392680e-01 9.97015834e-01\n",
      " 9.99537706e-01 9.99748051e-01 9.99617815e-01 9.83555615e-01\n",
      " 3.01659703e-01 9.57374454e-01 9.09012496e-01 9.77536440e-01\n",
      " 1.67269539e-02 4.28614616e-01 9.69278157e-01 6.83754504e-01\n",
      " 4.43228602e-01 9.99879599e-01 9.32406723e-01 8.47165644e-01\n",
      " 1.25769556e-01 4.07456070e-01 7.11449027e-01 8.36182356e-01\n",
      " 3.95184867e-02 9.77695167e-01 9.59843472e-02 9.92130578e-01\n",
      " 9.30969000e-01 9.01602983e-01 9.83428061e-01 2.62013404e-04\n",
      " 2.00142807e-04 8.09631601e-04 8.21292575e-04 8.12774710e-03\n",
      " 3.52480053e-03 5.06204844e-01 2.48878822e-02 8.38123914e-03\n",
      " 9.78337944e-01 9.86002803e-01]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 148 [0/54 (0%)]\tTrain Loss: 0.018135\n",
      "Train Epoch: 148 [8/54 (15%)]\tTrain Loss: 0.096474\n",
      "Train Epoch: 148 [16/54 (30%)]\tTrain Loss: 0.080240\n",
      "Train Epoch: 148 [24/54 (44%)]\tTrain Loss: 0.035881\n",
      "Train Epoch: 148 [32/54 (59%)]\tTrain Loss: 0.021645\n",
      "Train Epoch: 148 [40/54 (74%)]\tTrain Loss: 0.099823\n",
      "Train Epoch: 148 [48/54 (89%)]\tTrain Loss: 0.102432\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.18130133e-01 7.50181258e-01 2.31813893e-01 5.73341548e-01\n",
      " 3.87752026e-01 3.39783043e-01 3.08689773e-01 8.44866097e-01\n",
      " 2.60261387e-01 3.91920619e-02 5.53390324e-01 7.55856857e-02\n",
      " 1.64494276e-01 4.78385529e-03 4.88933269e-03 1.76432375e-02\n",
      " 4.02147174e-02 5.81697404e-01 5.08376598e-01 7.73544833e-02\n",
      " 2.27951601e-01 8.61264467e-01 6.88856423e-01 9.98730242e-01\n",
      " 8.45762491e-01 9.56547976e-01 9.91205990e-01 3.24793845e-01\n",
      " 7.13940039e-02 1.67890508e-02 5.24254553e-02 2.01092973e-01\n",
      " 8.79120664e-04 1.69381782e-01 1.77597985e-01 2.20353261e-01\n",
      " 1.00017503e-01 6.17708981e-01 7.80849993e-01 5.20503163e-01\n",
      " 4.12924886e-01 7.72016823e-01 5.24194539e-01 6.93323851e-01\n",
      " 5.44606149e-02 1.96986616e-01 5.10293663e-01 9.59012747e-01\n",
      " 8.77737761e-01 9.81683910e-01 9.80810344e-01 2.07685642e-02\n",
      " 7.88552687e-03 2.35768095e-01 1.88688993e-01 1.81701444e-02\n",
      " 9.47296798e-01 5.89795923e-03 6.84504688e-01 3.27548641e-03\n",
      " 9.95603442e-01 9.89441395e-01 9.99104083e-01 9.98164237e-01\n",
      " 3.67602795e-01 3.23887944e-01 1.45700350e-01 9.98752952e-01\n",
      " 9.89064813e-01 9.85567689e-01 9.94428873e-01 9.87493038e-01\n",
      " 9.23513770e-01 9.24018860e-01 7.16806531e-01 9.99268353e-01\n",
      " 9.97978747e-01 9.95713115e-01 8.74312580e-01 9.93830264e-01\n",
      " 9.98545289e-01 9.98191655e-01 9.99346673e-01 8.55436862e-01\n",
      " 8.77942204e-01 9.90842700e-01 7.34166741e-01 8.79425168e-01\n",
      " 1.12964399e-01 9.86861944e-01 9.95301604e-01 9.13657486e-01\n",
      " 4.44378942e-01 9.99954581e-01 9.27079439e-01 9.66769099e-01\n",
      " 1.58915490e-01 5.28765619e-01 9.86614645e-01 9.14478660e-01\n",
      " 9.44963574e-01 9.97533083e-01 5.94950497e-01 9.92125332e-01\n",
      " 9.64505374e-01 8.54315341e-01 9.97405589e-01 4.31833975e-03\n",
      " 4.75496007e-03 5.10205049e-03 1.85606144e-02 1.60917133e-01\n",
      " 7.97015578e-02 8.75763535e-01 2.21100479e-01 5.76407671e-01\n",
      " 9.86671746e-01 9.86290038e-01]\n",
      "predict [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 149 [0/54 (0%)]\tTrain Loss: 0.027620\n",
      "Train Epoch: 149 [8/54 (15%)]\tTrain Loss: 0.052594\n",
      "Train Epoch: 149 [16/54 (30%)]\tTrain Loss: 0.024759\n",
      "Train Epoch: 149 [24/54 (44%)]\tTrain Loss: 0.038007\n",
      "Train Epoch: 149 [32/54 (59%)]\tTrain Loss: 0.023116\n",
      "Train Epoch: 149 [40/54 (74%)]\tTrain Loss: 0.019158\n",
      "Train Epoch: 149 [48/54 (89%)]\tTrain Loss: 0.043303\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.31255055e-03 9.96129394e-01 9.84324336e-01 3.64369512e-01\n",
      " 1.55430123e-01 2.04629481e-01 9.65512455e-01 1.59984589e-01\n",
      " 8.18523392e-02 9.13881838e-01 3.90467763e-01 3.46533395e-02\n",
      " 6.26535058e-01 4.45742905e-03 3.62276653e-04 3.83119844e-03\n",
      " 5.53136459e-03 3.10256779e-01 1.06168292e-01 6.34558722e-02\n",
      " 2.10954055e-01 9.44807649e-01 7.25076020e-01 9.96011496e-01\n",
      " 5.98254561e-01 9.26665187e-01 9.55237150e-01 7.72525445e-02\n",
      " 1.17598139e-02 4.97533055e-03 4.26143361e-03 7.47907981e-02\n",
      " 2.32587001e-04 2.93442840e-03 3.79998656e-03 4.00881981e-03\n",
      " 7.70352688e-03 8.57985243e-02 8.00281346e-01 7.93555796e-01\n",
      " 3.35113406e-01 4.61653233e-01 7.23527372e-02 4.98428196e-02\n",
      " 1.19033065e-02 1.91375494e-01 8.92849088e-01 9.98078704e-01\n",
      " 8.89655948e-01 9.97353792e-01 9.89761651e-01 2.81465966e-02\n",
      " 1.20592548e-03 9.99988556e-01 1.88264549e-01 7.35417241e-03\n",
      " 7.38068640e-01 2.40062061e-03 4.59605694e-01 1.22189219e-03\n",
      " 9.98167992e-01 9.99797881e-01 9.99418855e-01 9.98379588e-01\n",
      " 5.92389822e-01 1.33848488e-02 2.38807127e-02 9.99999762e-01\n",
      " 9.99578059e-01 9.15990174e-01 9.99958634e-01 9.99845266e-01\n",
      " 8.89839172e-01 8.80973458e-01 5.66687047e-01 9.97955799e-01\n",
      " 9.99996901e-01 9.99997377e-01 9.99998450e-01 9.94129658e-01\n",
      " 9.98695791e-01 9.97299492e-01 9.99483347e-01 9.48566258e-01\n",
      " 9.98909235e-01 9.68230963e-01 9.52716291e-01 9.85211730e-01\n",
      " 1.21176345e-02 7.46406555e-01 9.73793328e-01 6.67232633e-01\n",
      " 9.30070132e-02 9.99931693e-01 8.78107250e-01 9.44143355e-01\n",
      " 1.69213545e-02 2.44451329e-01 9.98891413e-01 9.01330411e-01\n",
      " 9.82215643e-01 9.95403171e-01 5.54243587e-02 9.47499156e-01\n",
      " 5.97792983e-01 5.62795162e-01 8.40050399e-01 7.76745263e-04\n",
      " 1.03009772e-03 4.03375505e-03 1.84788078e-03 4.03688140e-02\n",
      " 1.46051943e-02 8.05258811e-01 3.48210372e-02 9.90502715e-01\n",
      " 9.89540517e-01 9.03814912e-01]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.]\n",
      "Train Epoch: 150 [0/54 (0%)]\tTrain Loss: 0.035044\n",
      "Train Epoch: 150 [8/54 (15%)]\tTrain Loss: 0.091541\n",
      "Train Epoch: 150 [16/54 (30%)]\tTrain Loss: 0.023128\n",
      "Train Epoch: 150 [24/54 (44%)]\tTrain Loss: 0.019029\n",
      "Train Epoch: 150 [32/54 (59%)]\tTrain Loss: 0.038176\n",
      "Train Epoch: 150 [40/54 (74%)]\tTrain Loss: 0.021037\n",
      "Train Epoch: 150 [48/54 (89%)]\tTrain Loss: 0.026700\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [6.39375970e-02 9.20057669e-03 1.18197377e-05 9.35733140e-01\n",
      " 1.81385219e-01 2.54204441e-02 5.97435283e-04 8.79159451e-01\n",
      " 2.26787571e-02 7.03658952e-06 1.57671079e-01 9.43844989e-02\n",
      " 2.00080149e-06 5.77451865e-05 8.70483089e-03 2.30084197e-06\n",
      " 5.86137129e-03 3.66142780e-01 7.62380511e-02 5.01715951e-02\n",
      " 1.08270727e-01 3.65883298e-03 8.72450054e-01 9.62177753e-01\n",
      " 3.05571944e-01 9.41857338e-01 9.71717060e-01 9.99547690e-02\n",
      " 1.01848975e-01 2.95585822e-02 1.84989676e-01 8.10193539e-01\n",
      " 8.31709988e-03 2.99171102e-03 5.72002586e-03 4.82812477e-03\n",
      " 1.16960593e-02 1.03072740e-01 5.20455837e-01 1.10229226e-02\n",
      " 3.05747688e-02 2.20169611e-02 1.81182489e-01 3.23429331e-03\n",
      " 8.80016759e-03 9.72197831e-01 1.63043081e-03 2.65305847e-01\n",
      " 8.68419334e-02 2.33301497e-03 2.29535121e-02 4.32011393e-06\n",
      " 1.76775213e-02 8.22973113e-12 2.04356357e-01 4.44639055e-03\n",
      " 2.11334467e-01 1.61066893e-02 1.98452190e-01 8.14011728e-04\n",
      " 1.49335384e-01 8.68498057e-04 4.35329862e-02 3.80119443e-01\n",
      " 6.64113653e-09 1.02629751e-01 6.64435551e-02 2.38681559e-06\n",
      " 1.96298305e-03 9.88951981e-01 2.99287137e-07 2.12854820e-06\n",
      " 9.86699581e-01 9.91407156e-01 8.97418141e-01 6.17964387e-01\n",
      " 8.33944380e-01 2.05993578e-02 5.47366496e-03 9.89588499e-01\n",
      " 9.97028291e-01 9.98841822e-01 9.99472439e-01 1.83741391e-01\n",
      " 2.43873805e-01 8.96443665e-01 7.73271143e-01 3.35791856e-01\n",
      " 6.01404756e-02 7.69108653e-01 9.95934010e-01 9.05546427e-01\n",
      " 4.83851165e-01 9.99745429e-01 9.36761141e-01 9.44279730e-01\n",
      " 1.20080234e-02 3.17685932e-01 4.90694046e-01 8.10334563e-01\n",
      " 3.71374044e-05 9.97718811e-01 2.23064259e-01 9.66835618e-01\n",
      " 8.45142901e-01 9.03378189e-01 9.89805102e-01 6.34664157e-03\n",
      " 2.59679668e-02 3.88938934e-02 1.89292803e-01 3.01054902e-02\n",
      " 3.73361975e-01 5.72434604e-01 2.49695368e-02 1.39151496e-06\n",
      " 8.57983530e-01 4.38542336e-01]\n",
      "predict [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "vote_pred [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 42 TN= 45 FN= 16 FP= 15\n",
      "TP+FP 57\n",
      "precision 0.7368421052631579\n",
      "recall 0.7241379310344828\n",
      "F1 0.7304347826086957\n",
      "acc 0.7372881355932204\n",
      "AUCp 0.7370689655172414\n",
      "AUC 0.7770114942528736\n",
      "\n",
      " The epoch is 150, average recall: 0.7241, average precision: 0.7368,average F1: 0.7304, average accuracy: 0.7373, average AUC: 0.7770\n",
      "Train Epoch: 151 [0/54 (0%)]\tTrain Loss: 0.040847\n",
      "Train Epoch: 151 [8/54 (15%)]\tTrain Loss: 0.019895\n",
      "Train Epoch: 151 [16/54 (30%)]\tTrain Loss: 0.075445\n",
      "Train Epoch: 151 [24/54 (44%)]\tTrain Loss: 0.045244\n",
      "Train Epoch: 151 [32/54 (59%)]\tTrain Loss: 0.021860\n",
      "Train Epoch: 151 [40/54 (74%)]\tTrain Loss: 0.038874\n",
      "Train Epoch: 151 [48/54 (89%)]\tTrain Loss: 0.094983\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.08525418e-02 9.64723110e-01 8.75263155e-01 2.13781551e-01\n",
      " 5.20331413e-02 4.50411402e-02 7.08120644e-01 2.54977614e-01\n",
      " 2.76041143e-02 3.34283360e-03 3.72721702e-01 5.29998057e-02\n",
      " 2.03997875e-03 3.21723637e-03 1.49764412e-03 3.26258712e-04\n",
      " 4.61771066e-04 1.47120655e-01 1.95652828e-01 1.17977858e-01\n",
      " 1.57842562e-01 3.51394862e-01 8.53102744e-01 9.74942744e-01\n",
      " 2.97293544e-01 8.89408648e-01 9.60994542e-01 3.94364834e-01\n",
      " 1.68810766e-02 2.53341813e-03 7.54909068e-02 1.90164760e-01\n",
      " 6.12991455e-04 2.40384904e-03 2.98762671e-03 6.66633656e-04\n",
      " 2.28000781e-03 2.55618487e-02 1.30229980e-01 2.93966266e-03\n",
      " 3.98029434e-03 1.15968594e-02 2.14499477e-02 8.48577637e-03\n",
      " 1.76707152e-02 1.12304457e-01 1.66339234e-01 9.96009469e-01\n",
      " 6.08243465e-01 9.50064540e-01 7.39122033e-01 1.89683982e-04\n",
      " 1.07473717e-03 3.58873769e-03 4.01745945e-01 2.27799034e-03\n",
      " 9.66148973e-01 1.29211624e-03 2.12621406e-01 5.21012628e-03\n",
      " 9.95588303e-01 9.98275518e-01 9.98643935e-01 9.98082161e-01\n",
      " 5.21174371e-01 6.80153817e-02 3.02181188e-02 9.98489141e-01\n",
      " 9.79650140e-01 9.01517749e-01 9.97787714e-01 9.86122429e-01\n",
      " 9.25133467e-01 9.21270370e-01 6.73631907e-01 9.86533165e-01\n",
      " 9.98005688e-01 9.98064935e-01 9.71831918e-01 9.89144385e-01\n",
      " 9.94115829e-01 9.97038603e-01 9.98559773e-01 9.69599009e-01\n",
      " 6.25774860e-01 9.85101342e-01 6.69335961e-01 7.91770399e-01\n",
      " 1.31360088e-02 8.22798133e-01 9.30339277e-01 3.74833137e-01\n",
      " 1.18638456e-01 9.99219775e-01 9.05797601e-01 7.63131320e-01\n",
      " 3.99680017e-03 1.20607696e-01 9.97792602e-01 9.86063182e-01\n",
      " 4.67513740e-01 8.94292235e-01 2.32591983e-02 9.42862272e-01\n",
      " 4.06540036e-01 7.67813921e-01 9.47714865e-01 7.63722928e-04\n",
      " 2.25086114e-03 4.58953809e-03 6.54667336e-03 1.28989415e-02\n",
      " 6.27712021e-03 7.62785852e-01 1.93892382e-02 4.40397151e-02\n",
      " 9.92661834e-01 9.73654628e-01]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 152 [0/54 (0%)]\tTrain Loss: 0.047594\n",
      "Train Epoch: 152 [8/54 (15%)]\tTrain Loss: 0.015641\n",
      "Train Epoch: 152 [16/54 (30%)]\tTrain Loss: 0.048979\n",
      "Train Epoch: 152 [24/54 (44%)]\tTrain Loss: 0.020744\n",
      "Train Epoch: 152 [32/54 (59%)]\tTrain Loss: 0.079235\n",
      "Train Epoch: 152 [40/54 (74%)]\tTrain Loss: 0.097605\n",
      "Train Epoch: 152 [48/54 (89%)]\tTrain Loss: 0.053238\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.12679134 0.98899883 0.93117166 0.9907822  0.43212634 0.02014283\n",
      " 0.92648244 0.99565661 0.02531284 0.00132491 0.21835081 0.00551613\n",
      " 0.03459763 0.61501616 0.61604863 0.0029369  0.00841295 0.5959897\n",
      " 0.0721053  0.02686736 0.054269   0.73274869 0.92342907 0.98227304\n",
      " 0.2092495  0.96276808 0.97496593 0.56219548 0.06828184 0.04600654\n",
      " 0.38787895 0.54596305 0.12032579 0.00220124 0.00330308 0.00589949\n",
      " 0.00937483 0.58663106 0.41137698 0.04735307 0.04855055 0.04306122\n",
      " 0.51166409 0.16540194 0.34093881 0.80274731 0.68700576 0.99606162\n",
      " 0.99759382 0.78082216 0.98928261 0.00715471 0.16797093 0.61314517\n",
      " 0.23842169 0.11624558 0.68193132 0.10020834 0.91522467 0.58212328\n",
      " 0.98288006 0.97422194 0.99158305 0.98405415 0.99672687 0.92892498\n",
      " 0.8094008  0.99920708 0.9999522  0.91123569 0.97832549 0.84262556\n",
      " 0.92313641 0.97054303 0.95884448 0.99626821 0.99907613 0.99849653\n",
      " 0.99751532 0.99174845 0.99275124 0.99700803 0.99739021 0.84052831\n",
      " 0.56717992 0.74056298 0.52678406 0.62543684 0.08384988 0.45982784\n",
      " 0.86403471 0.55922896 0.19074638 0.99938178 0.86590445 0.85757583\n",
      " 0.07452231 0.39665052 0.99010372 0.94164556 0.91644192 0.94757217\n",
      " 0.1328682  0.34240577 0.06987099 0.1188736  0.99172896 0.00542738\n",
      " 0.00642497 0.0417096  0.01842373 0.00499552 0.03123693 0.34889686\n",
      " 0.00720921 0.08592625 0.98355377 0.91635174]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "Train Epoch: 153 [0/54 (0%)]\tTrain Loss: 0.066351\n",
      "Train Epoch: 153 [8/54 (15%)]\tTrain Loss: 0.136705\n",
      "Train Epoch: 153 [16/54 (30%)]\tTrain Loss: 0.008596\n",
      "Train Epoch: 153 [24/54 (44%)]\tTrain Loss: 0.089811\n",
      "Train Epoch: 153 [32/54 (59%)]\tTrain Loss: 0.014228\n",
      "Train Epoch: 153 [40/54 (74%)]\tTrain Loss: 0.059070\n",
      "Train Epoch: 153 [48/54 (89%)]\tTrain Loss: 0.033602\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.31780186e-02 9.11155522e-01 8.07738602e-01 6.30852997e-01\n",
      " 1.90666243e-01 6.43984973e-02 6.00918829e-01 9.15261149e-01\n",
      " 2.29374290e-01 5.77225955e-03 6.27419233e-01 3.06029655e-02\n",
      " 5.77737316e-02 4.73714061e-02 4.65176329e-02 5.22322953e-03\n",
      " 4.20879852e-03 6.65046573e-01 5.32764614e-01 1.20687492e-01\n",
      " 3.89466763e-01 9.70254302e-01 9.62183356e-01 9.98567820e-01\n",
      " 9.07837451e-01 9.88759995e-01 9.53348815e-01 5.23917496e-01\n",
      " 9.81349573e-02 1.13351241e-01 2.01854948e-02 2.71704048e-01\n",
      " 7.87722645e-04 2.27613058e-02 1.49934925e-02 5.85134998e-02\n",
      " 2.97497641e-02 4.92115855e-01 1.71776921e-01 1.07552096e-01\n",
      " 1.13639891e-01 2.80796856e-01 4.39567238e-01 5.98823547e-01\n",
      " 1.17785916e-01 3.39177817e-01 1.51313797e-01 9.40049946e-01\n",
      " 9.64883208e-01 9.26061332e-01 9.74850476e-01 4.68313787e-03\n",
      " 4.87317517e-03 3.98611814e-01 1.16316922e-01 1.52043074e-01\n",
      " 8.99076760e-01 4.44173776e-02 9.00544107e-01 4.85597691e-03\n",
      " 9.93996859e-01 9.93571520e-01 9.97074842e-01 9.94739234e-01\n",
      " 9.67976809e-01 4.87865001e-01 2.56513447e-01 9.99492288e-01\n",
      " 9.99219537e-01 9.17462051e-01 9.97183979e-01 9.88692522e-01\n",
      " 7.89399683e-01 8.80156219e-01 8.65068316e-01 9.96928155e-01\n",
      " 9.95937347e-01 9.97034550e-01 9.99070704e-01 9.91160810e-01\n",
      " 9.89334047e-01 9.96293366e-01 9.98123825e-01 9.62573349e-01\n",
      " 9.31756258e-01 9.82541561e-01 8.78017545e-01 9.06244397e-01\n",
      " 1.42145336e-01 9.22225356e-01 9.30723608e-01 5.80834746e-01\n",
      " 1.62853569e-01 9.99879599e-01 9.61945236e-01 9.76647258e-01\n",
      " 4.46754068e-01 6.86059952e-01 9.93133128e-01 9.33360875e-01\n",
      " 6.66955829e-01 9.89080906e-01 3.39613646e-01 9.65917945e-01\n",
      " 7.81843483e-01 5.84547222e-01 9.96855140e-01 2.67805671e-03\n",
      " 1.09641226e-02 1.71268228e-02 8.58721137e-03 1.64637659e-02\n",
      " 2.90753152e-02 8.04610670e-01 1.39359655e-02 8.30729544e-01\n",
      " 9.97223854e-01 9.93760169e-01]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.]\n",
      "Train Epoch: 154 [0/54 (0%)]\tTrain Loss: 0.055530\n",
      "Train Epoch: 154 [8/54 (15%)]\tTrain Loss: 0.065746\n",
      "Train Epoch: 154 [16/54 (30%)]\tTrain Loss: 0.050259\n",
      "Train Epoch: 154 [24/54 (44%)]\tTrain Loss: 0.070109\n",
      "Train Epoch: 154 [32/54 (59%)]\tTrain Loss: 0.022932\n",
      "Train Epoch: 154 [40/54 (74%)]\tTrain Loss: 0.021973\n",
      "Train Epoch: 154 [48/54 (89%)]\tTrain Loss: 0.064148\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01181456 0.97184813 0.95898074 0.91928136 0.56912953 0.05043126\n",
      " 0.70625162 0.97029042 0.0541302  0.00386711 0.61132842 0.01764556\n",
      " 0.26504171 0.27226743 0.17585118 0.06250598 0.0225495  0.60725474\n",
      " 0.28159115 0.03633829 0.05731645 0.90593165 0.82725859 0.98934281\n",
      " 0.83763081 0.97490251 0.97735298 0.78410918 0.1442713  0.07566794\n",
      " 0.30796254 0.7900148  0.02702854 0.06369315 0.05427898 0.04311233\n",
      " 0.17746118 0.53339869 0.85975832 0.54947883 0.56867296 0.69656605\n",
      " 0.31265464 0.64420807 0.20644875 0.58254415 0.53507775 0.99668956\n",
      " 0.99685365 0.9616915  0.97626966 0.00318686 0.00634711 0.43740985\n",
      " 0.02227571 0.06058899 0.8678214  0.0595408  0.21875714 0.0337295\n",
      " 0.99250507 0.99018925 0.99570483 0.98985833 0.94011736 0.93285114\n",
      " 0.70094711 0.99984026 0.99986446 0.99612552 0.99096745 0.97649479\n",
      " 0.96581382 0.99539703 0.98732579 0.9980641  0.99872416 0.99870849\n",
      " 0.99149722 0.99793661 0.99908459 0.99941945 0.9996419  0.97025651\n",
      " 0.94993442 0.97055882 0.47710568 0.87607378 0.10073812 0.97522688\n",
      " 0.9923833  0.9636789  0.76072788 0.99991703 0.98675144 0.98710865\n",
      " 0.03451705 0.69464737 0.92081004 0.99769717 0.75947613 0.99549991\n",
      " 0.17153685 0.97060746 0.5267269  0.50842446 0.98700422 0.0370191\n",
      " 0.02788517 0.02272653 0.27859175 0.45686656 0.98206276 0.99252677\n",
      " 0.02541564 0.07561624 0.98028183 0.96172357]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 155 [0/54 (0%)]\tTrain Loss: 0.070035\n",
      "Train Epoch: 155 [8/54 (15%)]\tTrain Loss: 0.041364\n",
      "Train Epoch: 155 [16/54 (30%)]\tTrain Loss: 0.044806\n",
      "Train Epoch: 155 [24/54 (44%)]\tTrain Loss: 0.060726\n",
      "Train Epoch: 155 [32/54 (59%)]\tTrain Loss: 0.051264\n",
      "Train Epoch: 155 [40/54 (74%)]\tTrain Loss: 0.094344\n",
      "Train Epoch: 155 [48/54 (89%)]\tTrain Loss: 0.015059\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.98428035 0.99997187 0.99944085 0.99944216 0.64439732 0.28035712\n",
      " 0.99983048 0.99979609 0.18456633 0.04168807 0.15425147 0.02941601\n",
      " 0.51280338 0.94550902 0.98424953 0.21602832 0.4676739  0.77857715\n",
      " 0.23744269 0.11672889 0.21244867 0.95924443 0.98777628 0.9907499\n",
      " 0.73678207 0.99220651 0.9909566  0.98663241 0.64739275 0.41000557\n",
      " 0.98772728 0.99769479 0.99951053 0.00824975 0.01961199 0.03888471\n",
      " 0.11057602 0.74713838 0.9380641  0.51245946 0.63183528 0.55969578\n",
      " 0.31217584 0.89145666 0.93128085 0.99786252 0.99758017 0.99989772\n",
      " 0.99969923 0.80748391 0.99908769 0.06275234 0.25536317 0.98597312\n",
      " 0.05369449 0.15404253 0.90431756 0.74024636 0.2968249  0.98705208\n",
      " 0.99937755 0.99758542 0.99971145 0.99940789 0.99930227 0.99319118\n",
      " 0.98913866 0.99998355 0.99993122 0.8938145  0.99339527 0.98603237\n",
      " 0.99947208 0.99975795 0.99922943 0.99988019 1.         1.\n",
      " 0.99993408 0.99998188 0.99998546 0.99999249 0.99996459 0.92627573\n",
      " 0.90941811 0.97338831 0.98863608 0.99796033 0.48239484 0.51214349\n",
      " 0.96258026 0.93855327 0.96259063 0.9998523  0.95903248 0.9931885\n",
      " 0.30872273 0.68700445 0.85408479 0.97668266 0.93114829 0.99963963\n",
      " 0.16364643 0.94950384 0.61318952 0.88516569 0.94873238 0.16786569\n",
      " 0.01756296 0.05804551 0.05793502 0.18811874 0.72019786 0.76460713\n",
      " 0.04876152 0.51311171 0.99756098 0.99233139]\n",
      "predict [1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 156 [0/54 (0%)]\tTrain Loss: 0.080864\n",
      "Train Epoch: 156 [8/54 (15%)]\tTrain Loss: 0.055990\n",
      "Train Epoch: 156 [16/54 (30%)]\tTrain Loss: 0.096632\n",
      "Train Epoch: 156 [24/54 (44%)]\tTrain Loss: 0.027781\n",
      "Train Epoch: 156 [32/54 (59%)]\tTrain Loss: 0.038598\n",
      "Train Epoch: 156 [40/54 (74%)]\tTrain Loss: 0.050655\n",
      "Train Epoch: 156 [48/54 (89%)]\tTrain Loss: 0.014226\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.67626426e-01 3.58105302e-01 5.81475757e-02 1.13088250e-01\n",
      " 4.97116446e-02 7.06525892e-02 9.90530923e-02 4.48374778e-01\n",
      " 8.98765028e-02 1.38748884e-01 5.35636663e-01 1.75738901e-01\n",
      " 1.39220404e-02 2.67545789e-01 7.89178088e-02 4.25062403e-02\n",
      " 5.68683678e-03 5.98126531e-01 1.03472076e-01 2.04024408e-02\n",
      " 8.46487954e-02 6.11398399e-01 3.22750330e-01 9.57348824e-01\n",
      " 3.26896936e-01 8.84318292e-01 9.43652987e-01 3.97943854e-01\n",
      " 6.00385554e-02 9.65588540e-03 3.43220860e-01 7.72303760e-01\n",
      " 6.27475465e-03 3.12634893e-02 6.63007377e-03 5.40237362e-03\n",
      " 1.56089440e-02 1.52878463e-01 1.63808793e-01 7.71386400e-02\n",
      " 8.07769448e-02 3.09830159e-01 7.55472183e-02 1.62401110e-01\n",
      " 1.13275252e-01 6.88154101e-01 4.17349488e-01 8.29598665e-01\n",
      " 9.74106729e-01 9.81087387e-01 9.45249140e-01 6.19659759e-03\n",
      " 1.05647491e-02 6.50022402e-02 1.42107829e-01 2.33990606e-02\n",
      " 4.26040798e-01 1.58843882e-02 6.81360960e-02 8.90551955e-02\n",
      " 9.76312518e-01 9.80236828e-01 9.91441488e-01 9.78511870e-01\n",
      " 7.31977344e-01 8.15396309e-01 6.31930351e-01 9.80096519e-01\n",
      " 9.78399873e-01 9.70204115e-01 9.80772138e-01 9.48895276e-01\n",
      " 9.65541720e-01 9.95154023e-01 9.37796772e-01 9.50985312e-01\n",
      " 9.40828025e-01 7.88644195e-01 4.39054668e-01 9.78334188e-01\n",
      " 9.93988633e-01 9.92588103e-01 9.95613933e-01 8.84864807e-01\n",
      " 8.71709704e-01 9.63045955e-01 2.22657233e-01 3.96910369e-01\n",
      " 1.33509904e-01 7.82259345e-01 9.76155162e-01 6.56741798e-01\n",
      " 3.46206665e-01 9.98995006e-01 8.66145611e-01 9.07232463e-01\n",
      " 2.54187975e-02 2.25989863e-01 9.25275087e-01 9.59917068e-01\n",
      " 5.86098790e-01 9.74780738e-01 1.73347890e-02 9.83961344e-01\n",
      " 9.36967909e-01 9.40066814e-01 9.63712871e-01 9.09002498e-04\n",
      " 1.04946615e-02 3.57426493e-03 2.27963366e-02 9.47411507e-02\n",
      " 4.13221985e-01 8.43405485e-01 4.35358398e-02 7.02245310e-02\n",
      " 9.75541592e-01 9.19239283e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 157 [0/54 (0%)]\tTrain Loss: 0.079311\n",
      "Train Epoch: 157 [8/54 (15%)]\tTrain Loss: 0.005992\n",
      "Train Epoch: 157 [16/54 (30%)]\tTrain Loss: 0.012167\n",
      "Train Epoch: 157 [24/54 (44%)]\tTrain Loss: 0.044282\n",
      "Train Epoch: 157 [32/54 (59%)]\tTrain Loss: 0.020664\n",
      "Train Epoch: 157 [40/54 (74%)]\tTrain Loss: 0.037023\n",
      "Train Epoch: 157 [48/54 (89%)]\tTrain Loss: 0.049397\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.82093652e-03 9.97069538e-01 9.16036785e-01 5.16014218e-01\n",
      " 1.66971296e-01 3.62828411e-02 9.46856081e-01 3.15945983e-01\n",
      " 8.41778424e-03 2.12268472e-01 6.11733794e-01 3.11621338e-01\n",
      " 6.85433997e-03 2.39830092e-02 3.49901454e-03 1.46521360e-01\n",
      " 1.52562531e-02 7.41686106e-01 4.27751392e-01 5.09877875e-02\n",
      " 4.46426719e-01 9.12052870e-01 8.83960068e-01 9.93406773e-01\n",
      " 7.73448706e-01 9.71174955e-01 9.66252029e-01 9.17002618e-01\n",
      " 9.78195146e-02 3.11165638e-02 1.99022129e-01 9.20626342e-01\n",
      " 8.21789354e-03 3.69502306e-02 2.12631654e-02 1.84168499e-02\n",
      " 4.68862727e-02 4.32247192e-01 9.47850943e-01 9.02552307e-01\n",
      " 6.29249394e-01 9.66663480e-01 1.96111828e-01 6.71446085e-01\n",
      " 3.29921633e-01 1.45769149e-01 3.76392752e-01 9.89367902e-01\n",
      " 3.86647195e-01 9.99369204e-01 9.55099761e-01 7.62505922e-03\n",
      " 2.63247434e-02 7.53090143e-01 1.85167104e-01 1.28793448e-01\n",
      " 8.72766733e-01 4.87193987e-02 2.10182011e-01 2.89765060e-01\n",
      " 9.92222726e-01 9.84526396e-01 9.98275042e-01 9.97652113e-01\n",
      " 9.16157544e-01 7.34569609e-01 3.80287498e-01 9.99954939e-01\n",
      " 9.99581397e-01 9.51808035e-01 9.98223007e-01 9.92805660e-01\n",
      " 9.95476186e-01 9.98178124e-01 9.69338179e-01 9.99008238e-01\n",
      " 9.99990940e-01 9.99983788e-01 4.53092992e-01 9.98051167e-01\n",
      " 9.99843955e-01 9.99921441e-01 9.99925137e-01 9.84870255e-01\n",
      " 9.89574850e-01 9.83480632e-01 8.67630184e-01 8.30989838e-01\n",
      " 1.90929294e-01 9.23268914e-01 9.98989284e-01 9.94339645e-01\n",
      " 7.67688394e-01 9.99995232e-01 9.98173237e-01 9.98070419e-01\n",
      " 9.80021879e-02 5.49030602e-01 9.91685152e-01 9.76766229e-01\n",
      " 9.78639364e-01 9.96249139e-01 4.00993004e-02 9.98797774e-01\n",
      " 9.87652898e-01 9.88896728e-01 9.83364224e-01 1.61544871e-04\n",
      " 1.01020886e-03 6.17206504e-04 1.38177862e-03 1.00808702e-01\n",
      " 7.99276650e-01 9.77942288e-01 3.46188582e-02 8.59343946e-01\n",
      " 9.91700709e-01 9.58584905e-01]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
      "Train Epoch: 158 [0/54 (0%)]\tTrain Loss: 0.065772\n",
      "Train Epoch: 158 [8/54 (15%)]\tTrain Loss: 0.039745\n",
      "Train Epoch: 158 [16/54 (30%)]\tTrain Loss: 0.125075\n",
      "Train Epoch: 158 [24/54 (44%)]\tTrain Loss: 0.022093\n",
      "Train Epoch: 158 [32/54 (59%)]\tTrain Loss: 0.087871\n",
      "Train Epoch: 158 [40/54 (74%)]\tTrain Loss: 0.053436\n",
      "Train Epoch: 158 [48/54 (89%)]\tTrain Loss: 0.021814\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.21134405 0.96996683 0.70221895 0.55406576 0.07458887 0.08645475\n",
      " 0.82758433 0.82470191 0.04463649 0.0135145  0.05091047 0.07453607\n",
      " 0.01344008 0.27780628 0.11541324 0.00577841 0.01182587 0.61532819\n",
      " 0.07073694 0.00804363 0.06875984 0.85625249 0.67665744 0.99106979\n",
      " 0.54427433 0.94305676 0.95344949 0.58298218 0.05933363 0.01460324\n",
      " 0.72112608 0.94705039 0.43332973 0.01834108 0.00539663 0.01996255\n",
      " 0.03305443 0.36696669 0.74985772 0.26319104 0.29576671 0.32496178\n",
      " 0.17280455 0.04446477 0.02656499 0.76703143 0.79373503 0.9809832\n",
      " 0.99289304 0.94786638 0.99006128 0.00163951 0.00762689 0.11234426\n",
      " 0.10664858 0.0681958  0.59193015 0.01483613 0.11339701 0.09631319\n",
      " 0.99008465 0.94455278 0.99421281 0.99464524 0.88987863 0.89492834\n",
      " 0.87087333 0.99461728 0.99807066 0.95097697 0.97981262 0.96249336\n",
      " 0.9960658  0.99795568 0.99386483 0.99655747 0.9998197  0.99941456\n",
      " 0.80845588 0.99818963 0.99972051 0.99972349 0.99951243 0.77984124\n",
      " 0.89382428 0.97923547 0.59981012 0.78888851 0.20467304 0.82959956\n",
      " 0.99889946 0.93987954 0.57210016 0.99997687 0.99435556 0.99303353\n",
      " 0.0213072  0.36848703 0.38274935 0.96695054 0.58607489 0.99188626\n",
      " 0.0313789  0.97784913 0.83686757 0.84375757 0.94170856 0.00863053\n",
      " 0.04300472 0.0141836  0.02047932 0.2313244  0.87300247 0.93598998\n",
      " 0.02652464 0.11023049 0.9772709  0.97248894]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 159 [0/54 (0%)]\tTrain Loss: 0.042409\n",
      "Train Epoch: 159 [8/54 (15%)]\tTrain Loss: 0.050683\n",
      "Train Epoch: 159 [16/54 (30%)]\tTrain Loss: 0.010913\n",
      "Train Epoch: 159 [24/54 (44%)]\tTrain Loss: 0.041145\n",
      "Train Epoch: 159 [32/54 (59%)]\tTrain Loss: 0.099360\n",
      "Train Epoch: 159 [40/54 (74%)]\tTrain Loss: 0.017897\n",
      "Train Epoch: 159 [48/54 (89%)]\tTrain Loss: 0.059954\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.04023200e-02 7.80768991e-02 6.29267022e-02 1.01683855e-01\n",
      " 1.70553066e-02 2.98167001e-02 7.52070034e-03 1.68290854e-01\n",
      " 3.13855000e-02 6.25756336e-03 2.35922653e-02 1.30087152e-01\n",
      " 9.97616560e-04 3.26422662e-01 1.11785745e-02 1.17783470e-03\n",
      " 7.80352973e-04 6.22545660e-01 1.14387542e-01 8.12932011e-03\n",
      " 4.00509685e-02 4.05947059e-01 4.53452736e-01 9.84069824e-01\n",
      " 7.93934390e-02 9.45618153e-01 9.54296708e-01 5.06345987e-01\n",
      " 4.12286036e-02 4.26016096e-03 4.44580317e-02 5.07139981e-01\n",
      " 1.46362828e-02 1.56606548e-03 1.38670218e-03 3.98465199e-04\n",
      " 4.64710500e-03 3.65903050e-01 5.52119315e-01 3.20973955e-02\n",
      " 3.42663862e-02 2.35665794e-02 4.18717489e-02 1.72431245e-01\n",
      " 5.87205403e-02 3.38405728e-01 6.09142110e-02 4.34109151e-01\n",
      " 8.00874531e-01 9.59126294e-01 7.25440681e-01 4.90149017e-04\n",
      " 2.06310907e-03 1.87444743e-02 1.81499310e-02 1.48293050e-02\n",
      " 2.40931660e-01 1.30391866e-03 6.51799142e-03 2.19265036e-02\n",
      " 9.64332819e-01 9.34845209e-01 9.65964079e-01 9.35804427e-01\n",
      " 3.13258678e-01 7.49491513e-01 6.37581170e-01 9.76068139e-01\n",
      " 9.99394059e-01 9.91083145e-01 6.23981893e-01 4.96069729e-01\n",
      " 9.02942717e-01 9.75096762e-01 9.49997067e-01 8.87924314e-01\n",
      " 4.55868453e-01 2.43774101e-01 2.89579213e-01 9.70013082e-01\n",
      " 9.91958141e-01 9.75590825e-01 9.99147415e-01 5.16872644e-01\n",
      " 7.55099714e-01 9.12652671e-01 9.84600633e-02 1.94307238e-01\n",
      " 1.33478314e-01 9.12810981e-01 9.86143053e-01 8.63142669e-01\n",
      " 5.82352936e-01 9.99968767e-01 8.89494479e-01 9.46539938e-01\n",
      " 2.16345093e-03 3.35825607e-02 9.72237289e-01 9.79530275e-01\n",
      " 3.61976549e-02 9.66092587e-01 9.46732587e-04 8.03185225e-01\n",
      " 1.97904721e-01 4.13226336e-01 8.96890044e-01 1.20724435e-03\n",
      " 1.36982333e-02 5.89633081e-03 2.25983411e-02 3.72378491e-02\n",
      " 7.29483306e-01 8.67181838e-01 1.14667194e-03 9.97727457e-03\n",
      " 1.67392001e-01 5.64188063e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      "Train Epoch: 160 [0/54 (0%)]\tTrain Loss: 0.054557\n",
      "Train Epoch: 160 [8/54 (15%)]\tTrain Loss: 0.011592\n",
      "Train Epoch: 160 [16/54 (30%)]\tTrain Loss: 0.068686\n",
      "Train Epoch: 160 [24/54 (44%)]\tTrain Loss: 0.046775\n",
      "Train Epoch: 160 [32/54 (59%)]\tTrain Loss: 0.069140\n",
      "Train Epoch: 160 [40/54 (74%)]\tTrain Loss: 0.031591\n",
      "Train Epoch: 160 [48/54 (89%)]\tTrain Loss: 0.100436\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.15663873e-02 1.54190347e-01 5.53542469e-03 8.33038986e-02\n",
      " 2.35671252e-02 1.73621252e-01 2.50618130e-01 6.18757844e-01\n",
      " 9.77634117e-02 2.43746191e-02 8.44074264e-02 7.34836906e-02\n",
      " 1.41761219e-03 6.91058114e-02 4.96094637e-02 6.55360799e-03\n",
      " 3.32466356e-04 3.63670439e-01 8.08780566e-02 3.59298452e-03\n",
      " 8.48219171e-03 4.25960600e-01 5.33783078e-01 9.76486981e-01\n",
      " 1.47058532e-01 8.20101500e-01 8.23677599e-01 1.98035777e-01\n",
      " 1.40492693e-01 3.90270129e-02 1.20901056e-02 2.34556198e-01\n",
      " 4.35449561e-04 7.27659976e-03 1.38547726e-03 3.68110882e-03\n",
      " 2.32376568e-02 2.16309279e-01 5.09602904e-01 1.44636869e-01\n",
      " 2.53589720e-01 1.51109636e-01 2.88573325e-01 1.44479483e-01\n",
      " 1.14513533e-02 4.83818740e-01 1.30232293e-02 1.58650756e-01\n",
      " 3.34078133e-01 7.05040097e-01 3.43052804e-01 3.59093159e-04\n",
      " 2.62024789e-03 1.63066685e-02 1.18938657e-02 6.06548041e-04\n",
      " 8.39301944e-01 8.23511917e-04 3.96804176e-02 1.79583440e-04\n",
      " 9.84012604e-01 9.42915678e-01 9.89495933e-01 9.80602503e-01\n",
      " 7.93541200e-04 3.82398754e-01 8.62991512e-02 5.58023274e-01\n",
      " 2.02706218e-01 9.34555471e-01 9.53166008e-01 5.81318378e-01\n",
      " 7.63448834e-01 3.47469717e-01 4.05050963e-01 9.70924616e-01\n",
      " 9.81876493e-01 7.48170137e-01 2.10076109e-01 9.75503564e-01\n",
      " 9.93144989e-01 9.96127069e-01 9.99292254e-01 8.08691978e-01\n",
      " 6.69695199e-01 9.65898156e-01 7.25739002e-01 8.20825636e-01\n",
      " 4.82138433e-02 9.11003828e-01 9.75170970e-01 7.84456372e-01\n",
      " 2.65350044e-01 9.94957507e-01 7.10369110e-01 3.49062830e-01\n",
      " 1.27788552e-03 2.12422311e-01 8.44588876e-01 8.95593345e-01\n",
      " 9.37845230e-01 9.89809632e-01 4.37955558e-02 9.43746328e-01\n",
      " 7.65764654e-01 8.96899402e-01 9.88670945e-01 3.45264608e-03\n",
      " 2.50783600e-02 1.89036247e-03 8.94222688e-03 1.51559725e-01\n",
      " 5.47343455e-02 7.50258565e-01 1.68082062e-02 5.08301668e-02\n",
      " 7.86190748e-01 6.14858270e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 45 TN= 41 FN= 13 FP= 19\n",
      "TP+FP 64\n",
      "precision 0.703125\n",
      "recall 0.7758620689655172\n",
      "F1 0.7377049180327868\n",
      "acc 0.7288135593220338\n",
      "AUCp 0.7295977011494252\n",
      "AUC 0.7841954022988505\n",
      "\n",
      " The epoch is 160, average recall: 0.7759, average precision: 0.7031,average F1: 0.7377, average accuracy: 0.7288, average AUC: 0.7842\n",
      "Train Epoch: 161 [0/54 (0%)]\tTrain Loss: 0.013805\n",
      "Train Epoch: 161 [8/54 (15%)]\tTrain Loss: 0.043045\n",
      "Train Epoch: 161 [16/54 (30%)]\tTrain Loss: 0.031983\n",
      "Train Epoch: 161 [24/54 (44%)]\tTrain Loss: 0.021907\n",
      "Train Epoch: 161 [32/54 (59%)]\tTrain Loss: 0.065643\n",
      "Train Epoch: 161 [40/54 (74%)]\tTrain Loss: 0.038830\n",
      "Train Epoch: 161 [48/54 (89%)]\tTrain Loss: 0.051971\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.40391504e-02 5.76144271e-03 1.03464571e-03 2.92309940e-01\n",
      " 7.78986737e-02 4.68854941e-02 4.24035974e-02 6.11935794e-01\n",
      " 2.77937553e-03 3.12256981e-02 7.24090338e-01 2.72174120e-01\n",
      " 5.39753586e-03 1.36960531e-02 6.47880556e-03 2.00320734e-03\n",
      " 1.04364427e-03 4.10574049e-01 4.04837504e-02 3.73927993e-03\n",
      " 5.88757172e-02 7.07912564e-01 3.80096525e-01 9.86630201e-01\n",
      " 5.38967371e-01 8.73068511e-01 9.69856739e-01 2.60313377e-02\n",
      " 6.99878624e-03 2.88638775e-03 2.24791700e-03 1.54498801e-01\n",
      " 4.85867821e-03 1.44911315e-02 8.32207687e-03 4.73765358e-02\n",
      " 2.08827369e-02 2.18057364e-01 7.19213605e-01 4.46706444e-01\n",
      " 4.19538289e-01 6.59826696e-01 2.03183159e-01 5.64797856e-02\n",
      " 4.40239254e-03 2.78384257e-02 2.23785080e-03 8.28933865e-02\n",
      " 5.63367367e-01 9.09219801e-01 4.36927736e-01 2.15456006e-04\n",
      " 1.47180050e-04 6.37104502e-03 6.19861111e-03 1.48226554e-03\n",
      " 1.68872923e-01 1.39977329e-03 3.90151203e-01 7.70377483e-06\n",
      " 9.55089450e-01 8.45706820e-01 9.88457859e-01 9.81137276e-01\n",
      " 3.05803405e-04 4.47685242e-01 1.17379770e-01 9.91087735e-01\n",
      " 9.33604062e-01 9.09631073e-01 9.24972177e-01 8.21873009e-01\n",
      " 7.76966870e-01 9.36501503e-01 7.09099054e-01 7.87058115e-01\n",
      " 8.69805455e-01 5.75066328e-01 1.75685738e-03 9.87548470e-01\n",
      " 9.95649993e-01 9.97991085e-01 9.99449909e-01 3.38571787e-01\n",
      " 7.35231638e-01 9.49944913e-01 1.78002834e-01 3.79448444e-01\n",
      " 6.42597526e-02 9.67126846e-01 9.96165156e-01 9.44095016e-01\n",
      " 7.31544852e-01 9.99957919e-01 9.65833426e-01 9.87881064e-01\n",
      " 8.10524449e-03 3.34814519e-01 8.92252386e-01 9.97790575e-01\n",
      " 8.98072064e-01 9.95373905e-01 2.96304636e-02 9.53679621e-01\n",
      " 6.51235223e-01 4.61665630e-01 9.82690871e-01 4.65922663e-03\n",
      " 1.81964785e-02 6.93732500e-03 2.67898347e-02 6.70501769e-01\n",
      " 7.13721693e-01 9.48617578e-01 3.19733890e-03 3.72285815e-03\n",
      " 9.96904790e-01 9.96357143e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 162 [0/54 (0%)]\tTrain Loss: 0.010153\n",
      "Train Epoch: 162 [8/54 (15%)]\tTrain Loss: 0.013468\n",
      "Train Epoch: 162 [16/54 (30%)]\tTrain Loss: 0.061368\n",
      "Train Epoch: 162 [24/54 (44%)]\tTrain Loss: 0.029491\n",
      "Train Epoch: 162 [32/54 (59%)]\tTrain Loss: 0.038399\n",
      "Train Epoch: 162 [40/54 (74%)]\tTrain Loss: 0.046188\n",
      "Train Epoch: 162 [48/54 (89%)]\tTrain Loss: 0.053264\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.53813986e-03 4.46736306e-01 3.68433222e-02 4.33512852e-02\n",
      " 8.85331538e-03 2.98984093e-03 1.41201600e-01 6.52802706e-01\n",
      " 6.88190456e-04 4.62397409e-04 7.84075558e-01 1.86894387e-02\n",
      " 9.71935038e-03 1.67699922e-02 3.78871448e-02 2.08113878e-03\n",
      " 5.93824196e-04 4.31704640e-01 9.87792481e-03 8.78595654e-03\n",
      " 3.76721621e-02 6.60006523e-01 5.00558376e-01 9.42129076e-01\n",
      " 4.60913181e-01 7.66428947e-01 9.86687958e-01 2.42535427e-01\n",
      " 4.93020145e-03 2.90014804e-03 2.43601784e-01 5.58434188e-01\n",
      " 1.77121125e-02 3.08766658e-03 7.75329117e-03 6.50669411e-02\n",
      " 4.87327576e-02 2.12116987e-01 1.09551132e-01 1.39862269e-01\n",
      " 1.31867796e-01 1.19946308e-01 4.23395708e-02 8.75381082e-02\n",
      " 1.05117522e-02 5.09865940e-01 6.36203051e-01 9.61945117e-01\n",
      " 9.71691310e-01 9.48961616e-01 9.92416739e-01 2.13557010e-04\n",
      " 4.55242523e-04 1.19927414e-02 1.47417774e-02 1.48011930e-03\n",
      " 9.78724778e-01 4.55734262e-04 1.62475422e-01 6.21929648e-05\n",
      " 9.41291392e-01 8.07066739e-01 9.86784637e-01 9.69012916e-01\n",
      " 2.40116902e-02 5.75317085e-01 8.02361146e-02 9.83189821e-01\n",
      " 9.63536024e-01 9.44320977e-01 6.42094910e-01 2.40000457e-01\n",
      " 9.30951536e-01 9.65555906e-01 8.83320332e-01 9.31778193e-01\n",
      " 9.91940439e-01 9.66654122e-01 2.41296068e-01 9.84132528e-01\n",
      " 9.95158613e-01 9.97535706e-01 9.98846412e-01 7.19387710e-01\n",
      " 5.60890496e-01 8.78111422e-01 1.51007339e-01 5.27348280e-01\n",
      " 8.66048224e-03 8.72231245e-01 8.99156928e-01 5.38599014e-01\n",
      " 1.18018083e-01 9.99751031e-01 8.11818779e-01 4.46186900e-01\n",
      " 8.47603660e-04 2.20891520e-01 7.15990603e-01 9.33215439e-01\n",
      " 9.58916485e-01 9.69067216e-01 1.83278210e-02 8.92038345e-01\n",
      " 1.83225736e-01 3.89546961e-01 9.28012669e-01 4.10179142e-04\n",
      " 1.19856314e-03 7.63687363e-04 2.64924159e-03 2.48627812e-02\n",
      " 3.33311111e-02 8.71067822e-01 9.99108655e-04 1.30381680e-03\n",
      " 7.34478235e-01 7.41477013e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 163 [0/54 (0%)]\tTrain Loss: 0.103912\n",
      "Train Epoch: 163 [8/54 (15%)]\tTrain Loss: 0.015758\n",
      "Train Epoch: 163 [16/54 (30%)]\tTrain Loss: 0.006610\n",
      "Train Epoch: 163 [24/54 (44%)]\tTrain Loss: 0.035725\n",
      "Train Epoch: 163 [32/54 (59%)]\tTrain Loss: 0.012334\n",
      "Train Epoch: 163 [40/54 (74%)]\tTrain Loss: 0.058627\n",
      "Train Epoch: 163 [48/54 (89%)]\tTrain Loss: 0.063166\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.56060097e-03 7.62748897e-01 2.20201448e-01 1.83097944e-01\n",
      " 1.35096073e-01 5.65507077e-02 1.87446579e-01 4.85163778e-01\n",
      " 2.83173043e-02 4.92662657e-03 4.53446716e-01 3.11963484e-02\n",
      " 7.28788879e-03 9.86423530e-03 3.84400077e-02 1.03548430e-02\n",
      " 3.97882657e-03 7.63747156e-01 1.66379735e-01 1.92913264e-02\n",
      " 7.10788667e-02 9.49735582e-01 8.09720874e-01 9.61022496e-01\n",
      " 6.37405574e-01 9.56710160e-01 9.23090398e-01 6.33056462e-01\n",
      " 5.31125814e-02 4.70793098e-02 3.31721157e-02 6.70865834e-01\n",
      " 3.99932563e-02 2.47363430e-02 4.01482172e-02 1.41591914e-02\n",
      " 3.88432816e-02 2.77786434e-01 7.65837669e-01 1.73533335e-01\n",
      " 3.33752513e-01 3.76773268e-01 5.02476394e-01 1.39659360e-01\n",
      " 7.80649995e-03 1.74930960e-01 3.17212157e-02 9.86466944e-01\n",
      " 3.09576333e-01 8.67800474e-01 8.58539283e-01 4.20031283e-04\n",
      " 1.46828312e-03 7.56817535e-02 2.75766067e-02 8.22989084e-03\n",
      " 9.18212950e-01 1.18253464e-02 6.96569443e-01 1.61351752e-03\n",
      " 9.92670894e-01 9.48910296e-01 9.98161972e-01 9.98264492e-01\n",
      " 4.59728390e-01 7.43059814e-01 4.48858172e-01 9.99247074e-01\n",
      " 9.99722540e-01 9.97016311e-01 9.23488855e-01 8.11149716e-01\n",
      " 9.72918689e-01 9.99324083e-01 9.63094711e-01 9.75668132e-01\n",
      " 9.99000609e-01 9.96324718e-01 6.83401942e-01 9.89998162e-01\n",
      " 9.93650854e-01 9.99202073e-01 9.99052823e-01 8.35323095e-01\n",
      " 8.24235022e-01 9.68361080e-01 6.87648892e-01 8.12102377e-01\n",
      " 8.85279551e-02 9.93511140e-01 9.90400136e-01 9.38723087e-01\n",
      " 5.58485329e-01 9.99972343e-01 9.44374621e-01 9.57454562e-01\n",
      " 1.76403597e-02 2.73073643e-01 9.95154142e-01 9.99503136e-01\n",
      " 6.92932367e-01 9.82616961e-01 6.92917183e-02 8.73761714e-01\n",
      " 9.44823772e-02 5.26019707e-02 9.08619046e-01 3.58150639e-02\n",
      " 6.32722452e-02 8.82762671e-03 3.05853754e-01 1.36196852e-01\n",
      " 9.87980902e-01 9.86954153e-01 5.37280366e-03 4.55238782e-02\n",
      " 9.47903872e-01 9.94047284e-01]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 164 [0/54 (0%)]\tTrain Loss: 0.015357\n",
      "Train Epoch: 164 [8/54 (15%)]\tTrain Loss: 0.065706\n",
      "Train Epoch: 164 [16/54 (30%)]\tTrain Loss: 0.073870\n",
      "Train Epoch: 164 [24/54 (44%)]\tTrain Loss: 0.031159\n",
      "Train Epoch: 164 [32/54 (59%)]\tTrain Loss: 0.013762\n",
      "Train Epoch: 164 [40/54 (74%)]\tTrain Loss: 0.060931\n",
      "Train Epoch: 164 [48/54 (89%)]\tTrain Loss: 0.065415\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.02019233e-04 2.00547904e-01 3.93150263e-02 1.11353412e-01\n",
      " 5.16820550e-02 4.52368381e-03 4.12932560e-02 4.76112008e-01\n",
      " 4.41055046e-03 4.85629728e-03 3.23681422e-02 2.74229348e-02\n",
      " 1.50272250e-03 5.85589884e-03 2.75782626e-02 1.86995568e-03\n",
      " 5.05542557e-04 4.78442430e-01 2.12695319e-02 3.69583024e-03\n",
      " 7.46762892e-03 5.73802292e-01 1.21829398e-01 7.43884385e-01\n",
      " 4.27024886e-02 3.62433255e-01 4.55609947e-01 1.72105040e-02\n",
      " 2.56338110e-03 5.62600978e-03 2.90305266e-04 2.17434037e-02\n",
      " 7.25282414e-04 2.20541889e-03 1.50154543e-03 1.55911432e-03\n",
      " 2.26487238e-02 1.11619115e-01 3.57959330e-01 5.23068942e-02\n",
      " 6.66369349e-02 4.61470298e-02 7.37180412e-02 9.20757372e-03\n",
      " 3.27870902e-03 1.93970595e-02 1.53370682e-04 8.42858404e-02\n",
      " 4.08913521e-03 9.06900167e-01 3.79638433e-01 1.77360154e-04\n",
      " 2.09729042e-04 3.71818664e-03 8.80744588e-03 1.23198924e-03\n",
      " 2.12388158e-01 1.22274423e-03 4.74052364e-03 1.33952080e-05\n",
      " 8.84741008e-01 8.07358265e-01 9.25724387e-01 8.87321532e-01\n",
      " 2.67461920e-03 2.06148148e-01 2.47967727e-02 9.40963447e-01\n",
      " 9.92777109e-01 7.95026302e-01 8.73000979e-01 7.11263001e-01\n",
      " 2.80938029e-01 4.80590373e-01 4.27313775e-01 7.81569779e-01\n",
      " 9.93937016e-01 9.75481927e-01 2.47828546e-03 7.04757512e-01\n",
      " 8.64515901e-01 9.72637594e-01 9.94263232e-01 5.35968959e-01\n",
      " 8.87854218e-01 5.73914826e-01 8.63299742e-02 2.40897194e-01\n",
      " 6.34463504e-03 4.18078214e-01 8.13009679e-01 5.36579609e-01\n",
      " 3.05026352e-01 9.99714315e-01 7.74849892e-01 7.36620605e-01\n",
      " 2.36578798e-03 1.56432107e-01 5.40590048e-01 9.55736697e-01\n",
      " 1.20746329e-01 7.77496934e-01 1.15816174e-02 9.63753283e-01\n",
      " 6.59168005e-01 2.95130700e-01 8.88858199e-01 2.19403859e-03\n",
      " 8.80111882e-04 3.68469139e-03 2.97694211e-03 1.04895830e-02\n",
      " 6.72537461e-02 7.07960427e-01 3.36794113e-03 2.60081538e-03\n",
      " 7.60236025e-01 9.43614066e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 165 [0/54 (0%)]\tTrain Loss: 0.052026\n",
      "Train Epoch: 165 [8/54 (15%)]\tTrain Loss: 0.033166\n",
      "Train Epoch: 165 [16/54 (30%)]\tTrain Loss: 0.013703\n",
      "Train Epoch: 165 [24/54 (44%)]\tTrain Loss: 0.039654\n",
      "Train Epoch: 165 [32/54 (59%)]\tTrain Loss: 0.010184\n",
      "Train Epoch: 165 [40/54 (74%)]\tTrain Loss: 0.011710\n",
      "Train Epoch: 165 [48/54 (89%)]\tTrain Loss: 0.054604\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.14681646e-02 6.80930436e-01 3.43661569e-02 1.55186042e-01\n",
      " 4.44210321e-03 8.82200990e-03 8.25818181e-02 6.87818885e-01\n",
      " 8.34994391e-03 1.99499913e-03 7.30247051e-02 5.21402806e-03\n",
      " 2.03008484e-03 1.93641558e-02 4.37190123e-02 2.81879143e-03\n",
      " 4.75048233e-04 2.01836795e-01 3.63565562e-03 1.37799478e-03\n",
      " 7.17654685e-03 5.31311214e-01 2.05113679e-01 9.87139285e-01\n",
      " 5.24169430e-02 7.79267609e-01 9.76719797e-01 2.29357015e-02\n",
      " 2.26943288e-03 2.15621735e-03 1.91087648e-02 3.05531144e-01\n",
      " 5.97181916e-02 1.48790877e-03 1.39896024e-03 8.52552895e-03\n",
      " 2.65462901e-02 1.66652024e-01 1.67512074e-01 3.09398491e-02\n",
      " 7.71500915e-02 9.27442163e-02 4.64291200e-02 5.28997071e-02\n",
      " 2.34315358e-03 2.17193976e-01 4.43276554e-01 8.21533442e-01\n",
      " 9.59332287e-01 9.08457577e-01 9.92708802e-01 4.51893953e-04\n",
      " 2.88346258e-04 5.40447235e-03 9.46379732e-03 3.45542794e-04\n",
      " 7.27410793e-01 5.95999474e-04 6.77293301e-01 2.74629536e-04\n",
      " 9.94269252e-01 9.69691336e-01 9.98718143e-01 9.96610105e-01\n",
      " 3.75057012e-02 6.32496119e-01 2.25922242e-01 9.95130777e-01\n",
      " 9.98330057e-01 9.32808638e-01 9.36911285e-01 7.55259633e-01\n",
      " 8.77528369e-01 9.51158881e-01 8.62653136e-01 9.34383035e-01\n",
      " 9.99536872e-01 9.97058392e-01 5.20160735e-01 9.93382931e-01\n",
      " 9.97698486e-01 9.96830165e-01 9.99830961e-01 6.87417924e-01\n",
      " 8.10408175e-01 9.68028307e-01 4.26092654e-01 7.49359667e-01\n",
      " 1.13444859e-02 8.03986907e-01 9.85654175e-01 7.14456379e-01\n",
      " 1.53183520e-01 9.99925613e-01 9.09762621e-01 4.30479169e-01\n",
      " 1.59862661e-03 1.18005484e-01 2.77311742e-01 9.83056068e-01\n",
      " 7.51459777e-01 9.83963788e-01 2.52522286e-02 7.57925272e-01\n",
      " 2.33581051e-01 1.36936575e-01 9.89203393e-01 1.81732024e-03\n",
      " 4.07042913e-04 1.76550832e-03 7.64549477e-03 5.90048358e-03\n",
      " 2.83800334e-01 1.70156285e-01 1.78594724e-03 7.59431347e-03\n",
      " 9.14040744e-01 9.44263458e-01]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "Train Epoch: 166 [0/54 (0%)]\tTrain Loss: 0.023515\n",
      "Train Epoch: 166 [8/54 (15%)]\tTrain Loss: 0.013079\n",
      "Train Epoch: 166 [16/54 (30%)]\tTrain Loss: 0.026014\n",
      "Train Epoch: 166 [24/54 (44%)]\tTrain Loss: 0.027552\n",
      "Train Epoch: 166 [32/54 (59%)]\tTrain Loss: 0.034223\n",
      "Train Epoch: 166 [40/54 (74%)]\tTrain Loss: 0.009711\n",
      "Train Epoch: 166 [48/54 (89%)]\tTrain Loss: 0.039878\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.06938444e-02 4.65157807e-01 2.08179746e-02 1.40194178e-01\n",
      " 1.66732445e-01 2.40704030e-01 4.62351777e-02 6.00087345e-01\n",
      " 1.15344100e-01 1.47178983e-02 6.99956045e-02 6.72680065e-02\n",
      " 1.36608100e-02 9.43552032e-02 2.42439862e-02 4.70439978e-02\n",
      " 5.43321855e-03 3.57996315e-01 5.01433849e-01 3.23550636e-03\n",
      " 6.64570853e-02 7.98130929e-01 9.29169595e-01 9.98108745e-01\n",
      " 5.31081200e-01 9.65639055e-01 9.80031490e-01 1.84305206e-01\n",
      " 4.81964946e-02 4.54362594e-02 6.13682624e-03 3.56258690e-01\n",
      " 1.00341924e-02 1.42893093e-02 3.21915839e-03 4.88067977e-03\n",
      " 1.05977366e-02 9.43188816e-02 4.32852983e-01 2.24708691e-02\n",
      " 1.54029066e-02 5.65909520e-02 4.74937968e-02 1.41082034e-01\n",
      " 1.20281801e-02 8.94465595e-02 7.74841160e-02 9.44100320e-01\n",
      " 5.81104636e-01 9.81537461e-01 9.77382481e-01 1.18264963e-03\n",
      " 2.33943574e-03 1.70046657e-01 1.46204457e-02 1.03720138e-02\n",
      " 6.15907073e-01 5.19471103e-03 3.99560422e-01 5.62960980e-04\n",
      " 9.98885810e-01 9.93415356e-01 9.99533057e-01 9.98758078e-01\n",
      " 1.58891380e-01 8.26654434e-01 3.16021025e-01 9.97827709e-01\n",
      " 9.94563520e-01 9.69669163e-01 9.94524956e-01 9.92727876e-01\n",
      " 9.47066486e-01 9.92055595e-01 9.55848455e-01 9.99044478e-01\n",
      " 9.99909282e-01 9.97172534e-01 1.83120087e-01 9.76777911e-01\n",
      " 9.93752539e-01 9.98670816e-01 9.99867320e-01 8.65856588e-01\n",
      " 9.70738232e-01 9.96551752e-01 9.46340561e-01 9.93176818e-01\n",
      " 9.57445428e-02 9.98442113e-01 9.99455035e-01 9.08021212e-01\n",
      " 4.88459080e-01 9.99780118e-01 9.44409013e-01 8.80992770e-01\n",
      " 2.10368764e-02 2.46440366e-01 6.18485391e-01 9.85395491e-01\n",
      " 8.78538489e-02 9.96220648e-01 4.41634059e-02 9.53684390e-01\n",
      " 5.98288238e-01 9.13449645e-01 9.83658612e-01 1.47754827e-03\n",
      " 1.51635543e-01 2.87995907e-03 6.25734925e-02 2.76003517e-02\n",
      " 7.11856306e-01 9.33238328e-01 3.85942385e-02 6.84852451e-02\n",
      " 7.03179240e-01 9.45859790e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 167 [0/54 (0%)]\tTrain Loss: 0.018429\n",
      "Train Epoch: 167 [8/54 (15%)]\tTrain Loss: 0.020282\n",
      "Train Epoch: 167 [16/54 (30%)]\tTrain Loss: 0.063989\n",
      "Train Epoch: 167 [24/54 (44%)]\tTrain Loss: 0.032682\n",
      "Train Epoch: 167 [32/54 (59%)]\tTrain Loss: 0.038795\n",
      "Train Epoch: 167 [40/54 (74%)]\tTrain Loss: 0.056827\n",
      "Train Epoch: 167 [48/54 (89%)]\tTrain Loss: 0.026495\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.51269899e-04 3.14451084e-02 2.52904762e-02 2.90688816e-02\n",
      " 7.92239443e-04 2.14891462e-03 5.59720304e-03 4.84776162e-02\n",
      " 7.62415933e-04 2.41893195e-02 2.30096936e-01 3.81440222e-02\n",
      " 2.76178471e-03 1.37530249e-02 9.19375219e-04 6.41539926e-04\n",
      " 3.05198773e-04 8.65527466e-02 3.58017697e-03 3.62388761e-04\n",
      " 3.24769551e-03 1.51712894e-01 2.82344520e-01 9.94111955e-01\n",
      " 1.84094533e-01 8.88048172e-01 9.92721736e-01 1.39949452e-02\n",
      " 1.72918383e-03 5.91048331e-04 3.91158229e-03 7.31104091e-02\n",
      " 1.46277642e-04 2.24691024e-03 1.63586531e-03 1.69267145e-03\n",
      " 8.09536688e-03 5.02954889e-03 1.96968064e-01 1.95337888e-02\n",
      " 3.22059840e-02 3.42939906e-02 1.59649272e-03 7.80481263e-04\n",
      " 5.55394392e-04 3.51380967e-02 7.12458119e-02 4.53598410e-01\n",
      " 3.94067973e-01 8.84476542e-01 8.00977528e-01 3.48918038e-05\n",
      " 2.78466359e-05 1.02976523e-03 1.06232185e-02 4.61164855e-05\n",
      " 1.92849845e-01 5.36737381e-04 3.13864811e-03 2.99322506e-04\n",
      " 4.71400440e-01 2.31523916e-01 8.83263588e-01 8.36323977e-01\n",
      " 3.03676375e-03 3.89510900e-01 2.30876207e-01 9.52633321e-01\n",
      " 9.92532790e-01 9.97004926e-01 9.63380218e-01 9.04377222e-01\n",
      " 5.74099839e-01 9.74232435e-01 6.44534469e-01 5.70171177e-02\n",
      " 8.13743532e-01 7.45598495e-01 2.02977331e-03 8.66930783e-01\n",
      " 9.51083422e-01 9.57857788e-01 9.95814025e-01 3.82889301e-01\n",
      " 7.78951645e-01 9.77657795e-01 1.18884079e-01 1.64298624e-01\n",
      " 1.00464441e-01 9.49638486e-01 9.99336779e-01 4.37761337e-01\n",
      " 1.00032933e-01 9.99957204e-01 9.32744741e-01 7.17495084e-01\n",
      " 1.05124280e-04 4.34555948e-01 3.67722899e-01 9.97661948e-01\n",
      " 4.35330838e-01 4.86146480e-01 1.13487465e-03 4.96636242e-01\n",
      " 4.85232204e-01 1.34654671e-01 9.58525181e-01 9.52628499e-04\n",
      " 1.52806137e-02 4.76824353e-04 7.15162791e-03 1.51403576e-01\n",
      " 9.65826869e-01 9.97352600e-01 1.78827380e-03 2.30186080e-04\n",
      " 3.89238268e-01 3.72115783e-02]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 168 [0/54 (0%)]\tTrain Loss: 0.063100\n",
      "Train Epoch: 168 [8/54 (15%)]\tTrain Loss: 0.117254\n",
      "Train Epoch: 168 [16/54 (30%)]\tTrain Loss: 0.030409\n",
      "Train Epoch: 168 [24/54 (44%)]\tTrain Loss: 0.063119\n",
      "Train Epoch: 168 [32/54 (59%)]\tTrain Loss: 0.016744\n",
      "Train Epoch: 168 [40/54 (74%)]\tTrain Loss: 0.059868\n",
      "Train Epoch: 168 [48/54 (89%)]\tTrain Loss: 0.046962\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.06790960e-03 9.42730784e-01 7.96255529e-01 8.53495717e-01\n",
      " 3.27758014e-01 7.87567813e-03 6.84807301e-01 8.44617128e-01\n",
      " 1.43848872e-03 3.58284172e-03 4.48786840e-02 6.30487781e-03\n",
      " 8.30348767e-03 3.15274820e-02 4.76199016e-02 7.84727093e-03\n",
      " 2.56639905e-03 2.25256562e-01 6.00434244e-02 4.25875336e-02\n",
      " 5.67973495e-01 5.10958850e-01 6.67560160e-01 9.81323361e-01\n",
      " 3.94477814e-01 8.54290843e-01 9.64554191e-01 1.25964820e-01\n",
      " 4.58455570e-02 2.75091659e-02 6.97540864e-03 1.54299825e-01\n",
      " 1.44917483e-03 1.41646271e-03 3.24642938e-03 2.05477979e-02\n",
      " 9.73058026e-03 2.70678639e-01 9.41762269e-01 6.05194509e-01\n",
      " 6.25465631e-01 4.88892883e-01 3.97923619e-01 1.57441422e-02\n",
      " 2.83064060e-02 6.63510635e-02 3.86897683e-01 9.45394456e-01\n",
      " 7.35829949e-01 9.57028687e-01 9.54868853e-01 4.35421738e-04\n",
      " 2.71898159e-03 5.43070883e-02 4.02932912e-01 8.70430563e-03\n",
      " 7.13702023e-01 1.14377262e-02 4.42742735e-01 9.41892713e-03\n",
      " 9.15119350e-01 7.13054121e-01 9.28551316e-01 9.76608932e-01\n",
      " 1.46376982e-01 6.20455921e-01 3.06422800e-01 9.95396197e-01\n",
      " 9.99038100e-01 8.72119784e-01 9.85436857e-01 9.58415270e-01\n",
      " 9.14187849e-01 9.78693843e-01 8.81640375e-01 7.52647042e-01\n",
      " 9.98140931e-01 9.92388666e-01 7.04544365e-01 9.95842755e-01\n",
      " 9.96668994e-01 9.98092949e-01 9.99995828e-01 7.21748710e-01\n",
      " 8.46805036e-01 9.70144749e-01 7.26035774e-01 7.66260624e-01\n",
      " 2.03578651e-01 7.33193040e-01 9.37155902e-01 8.12342107e-01\n",
      " 4.63351279e-01 9.99747097e-01 9.86118317e-01 9.39894378e-01\n",
      " 3.13949888e-03 6.47954047e-01 7.54939854e-01 9.97752845e-01\n",
      " 6.90092742e-01 9.69636440e-01 2.88054466e-01 6.29511118e-01\n",
      " 4.10063386e-01 7.39905417e-01 9.98200059e-01 3.04768719e-02\n",
      " 1.38095869e-02 1.64973252e-02 4.26219106e-02 1.63238883e-01\n",
      " 9.01990592e-01 9.12385106e-01 1.02088992e-02 1.47397872e-02\n",
      " 9.47366238e-01 8.00620735e-01]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 169 [0/54 (0%)]\tTrain Loss: 0.088201\n",
      "Train Epoch: 169 [8/54 (15%)]\tTrain Loss: 0.037598\n",
      "Train Epoch: 169 [16/54 (30%)]\tTrain Loss: 0.045271\n",
      "Train Epoch: 169 [24/54 (44%)]\tTrain Loss: 0.082143\n",
      "Train Epoch: 169 [32/54 (59%)]\tTrain Loss: 0.017748\n",
      "Train Epoch: 169 [40/54 (74%)]\tTrain Loss: 0.025063\n",
      "Train Epoch: 169 [48/54 (89%)]\tTrain Loss: 0.064145\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.32081652e-04 1.92350224e-02 2.15579659e-01 4.39169019e-01\n",
      " 4.86538596e-02 2.55897152e-03 1.91324651e-02 3.36990833e-01\n",
      " 6.66357984e-04 1.57937221e-03 3.29492211e-01 4.44782199e-03\n",
      " 1.91659760e-02 4.36697807e-03 7.15047645e-04 1.56871905e-03\n",
      " 6.21001120e-04 1.04510993e-01 4.08067182e-03 1.64946646e-03\n",
      " 3.38692740e-02 3.95925105e-01 2.10750669e-01 9.13943470e-01\n",
      " 1.30812541e-01 8.93880188e-01 9.72833812e-01 8.43398646e-02\n",
      " 4.45622066e-03 6.00857753e-03 4.50579636e-03 5.49097322e-02\n",
      " 3.06120119e-03 1.36467896e-03 2.20478349e-03 1.80018544e-02\n",
      " 8.27653985e-03 2.36729071e-01 6.26180887e-01 1.33062944e-01\n",
      " 6.86254799e-02 1.55153498e-01 7.83848315e-02 6.14122599e-02\n",
      " 2.13334151e-03 1.23021137e-02 1.86524808e-01 4.13023770e-01\n",
      " 2.69695848e-01 3.33100587e-01 5.94660223e-01 6.17750920e-04\n",
      " 3.48499598e-04 6.53504068e-03 2.77464837e-03 6.22947002e-04\n",
      " 3.89950186e-01 2.15939083e-03 1.01303488e-01 6.40856335e-04\n",
      " 7.68895745e-01 7.48873591e-01 9.15475786e-01 8.16216886e-01\n",
      " 2.80454196e-02 5.32111049e-01 2.56092548e-01 9.97809470e-01\n",
      " 9.99555171e-01 5.24697304e-01 7.77160823e-01 7.08789170e-01\n",
      " 6.49118364e-01 9.74594653e-01 9.14985180e-01 3.77158195e-01\n",
      " 9.51587558e-01 9.55369413e-01 1.23576280e-02 9.44340825e-01\n",
      " 9.85057414e-01 9.98645246e-01 9.99345958e-01 6.99523926e-01\n",
      " 7.77333677e-01 5.26826560e-01 4.33321409e-02 6.59266636e-02\n",
      " 1.40940603e-02 9.51067865e-01 9.72750664e-01 5.45307696e-01\n",
      " 2.52447277e-01 9.99938607e-01 8.99734616e-01 8.90358031e-01\n",
      " 7.93658197e-03 1.31142646e-01 8.27894032e-01 9.80001032e-01\n",
      " 3.06978136e-01 8.71939838e-01 2.69498937e-02 7.24903762e-01\n",
      " 4.58868682e-01 1.93034828e-01 5.87569416e-01 1.07669495e-02\n",
      " 5.22936974e-03 1.43262802e-03 3.21724266e-02 2.79453434e-02\n",
      " 6.58134162e-01 9.55584407e-01 1.70426723e-03 1.84443430e-03\n",
      " 7.74897456e-01 9.25548792e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 170 [0/54 (0%)]\tTrain Loss: 0.058388\n",
      "Train Epoch: 170 [8/54 (15%)]\tTrain Loss: 0.022372\n",
      "Train Epoch: 170 [16/54 (30%)]\tTrain Loss: 0.044756\n",
      "Train Epoch: 170 [24/54 (44%)]\tTrain Loss: 0.021641\n",
      "Train Epoch: 170 [32/54 (59%)]\tTrain Loss: 0.054061\n",
      "Train Epoch: 170 [40/54 (74%)]\tTrain Loss: 0.027715\n",
      "Train Epoch: 170 [48/54 (89%)]\tTrain Loss: 0.004489\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.00818634 0.96037549 0.57173896 0.37804276 0.03179939 0.01161678\n",
      " 0.23857687 0.33289108 0.00214719 0.01937487 0.18547136 0.15823436\n",
      " 0.01822634 0.00412213 0.02696453 0.00148876 0.00641202 0.15441646\n",
      " 0.05310442 0.0114954  0.03463935 0.87684798 0.91265023 0.94602042\n",
      " 0.84901392 0.99648857 0.99991417 0.12819648 0.01871587 0.01516147\n",
      " 0.02115726 0.21440591 0.01216717 0.00388319 0.00298323 0.03206259\n",
      " 0.01561079 0.27008861 0.77488679 0.35228801 0.49093404 0.72471523\n",
      " 0.28734511 0.02307617 0.03299697 0.03436476 0.8631404  0.97972667\n",
      " 0.22352326 0.14219548 0.94886845 0.00724839 0.00290648 0.45690197\n",
      " 0.02580708 0.0112398  0.78114563 0.00794818 0.68980908 0.00344671\n",
      " 0.98176885 0.97834659 0.99487692 0.99529922 0.03288392 0.16567208\n",
      " 0.27240261 0.99727684 0.99377245 0.56831795 0.93113208 0.86399359\n",
      " 0.33914578 0.89363301 0.86634213 0.97001809 0.99961013 0.99943751\n",
      " 0.81723028 0.99105364 0.9658671  0.99228656 0.97984707 0.91270584\n",
      " 0.93558753 0.8450411  0.7127825  0.51356912 0.01959906 0.92964131\n",
      " 0.99825257 0.13843738 0.22568868 0.99999797 0.93654883 0.99512202\n",
      " 0.28424764 0.75293612 0.96182591 0.99274158 0.99538827 0.99343383\n",
      " 0.67280489 0.87819421 0.70110899 0.6085723  0.96115243 0.03447944\n",
      " 0.00165447 0.0036663  0.01146926 0.04005101 0.01644149 0.85032833\n",
      " 0.04065359 0.286823   0.99825937 0.99899441]\n",
      "predict [0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 40 TN= 52 FN= 18 FP= 8\n",
      "TP+FP 48\n",
      "precision 0.8333333333333334\n",
      "recall 0.6896551724137931\n",
      "F1 0.7547169811320755\n",
      "acc 0.7796610169491526\n",
      "AUCp 0.7781609195402299\n",
      "AUC 0.8204022988505747\n",
      "\n",
      " The epoch is 170, average recall: 0.6897, average precision: 0.8333,average F1: 0.7547, average accuracy: 0.7797, average AUC: 0.8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 171 [0/54 (0%)]\tTrain Loss: 0.088551\n",
      "Train Epoch: 171 [8/54 (15%)]\tTrain Loss: 0.066846\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# train\n",
    "bs =batchsize\n",
    "votenum = 10\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "r_list = []\n",
    "p_list = []\n",
    "acc_list = []\n",
    "AUC_list = []\n",
    "# TP = 0\n",
    "# TN = 0\n",
    "# FN = 0\n",
    "# FP = 0\n",
    "vote_pred = np.zeros(valset.__len__())\n",
    "vote_score = np.zeros(valset.__len__())\n",
    "\n",
    "lr = 0.001\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr) \n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
    "                                             \n",
    "scheduler = StepLR(optimizer, step_size=1)\n",
    "\n",
    "total_epoch = 500\n",
    "for epoch in range(1, total_epoch+1):\n",
    "    train(optimizer, epoch)\n",
    "    \n",
    "    targetlist, scorelist, predlist = val(epoch)\n",
    "    print('target',targetlist)\n",
    "    print('score',scorelist)\n",
    "    print('predict',predlist)\n",
    "    vote_pred = vote_pred + predlist \n",
    "    vote_score = vote_score + scorelist \n",
    "\n",
    "    if epoch % votenum == 0:\n",
    "        \n",
    "        # major vote\n",
    "        vote_pred[vote_pred <= (votenum/2)] = 0\n",
    "        vote_pred[vote_pred > (votenum/2)] = 1\n",
    "        vote_score = vote_score/votenum\n",
    "        \n",
    "        print('vote_pred', vote_pred)\n",
    "        print('targetlist', targetlist)\n",
    "        TP = ((vote_pred == 1) & (targetlist == 1)).sum()\n",
    "        TN = ((vote_pred == 0) & (targetlist == 0)).sum()\n",
    "        FN = ((vote_pred == 0) & (targetlist == 1)).sum()\n",
    "        FP = ((vote_pred == 1) & (targetlist == 0)).sum()\n",
    "        \n",
    "        \n",
    "        print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n",
    "        print('TP+FP',TP+FP)\n",
    "        p = TP / (TP + FP)\n",
    "        print('precision',p)\n",
    "        p = TP / (TP + FP)\n",
    "        r = TP / (TP + FN)\n",
    "        print('recall',r)\n",
    "        F1 = 2 * r * p / (r + p)\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "        print('F1',F1)\n",
    "        print('acc',acc)\n",
    "        AUC = roc_auc_score(targetlist, vote_score)\n",
    "        print('AUCp', roc_auc_score(targetlist, vote_pred))\n",
    "        print('AUC', AUC)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         if epoch == total_epoch:\n",
    "        torch.save(model.state_dict(), \"/data/cv_final/CT-Predict/2D-Pretrain/result/UCSD_{}_{}_{}_{}.pt\".format(modelname,alpha_name,epoch, datetime.datetime.now()))  \n",
    "\n",
    "        vote_pred = np.zeros(valset.__len__())\n",
    "        vote_score = np.zeros(valset.__len__())\n",
    "        print('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n",
    "average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
    "        epoch, r, p, F1, acc, AUC))\n",
    "\n",
    "        f = open('/data/cv_final/CT-Predict/2D-Pretrain/result/UCSD_{}_{}_{}_{}.txt'.format(modelname,alpha_name, epoch, lr), 'a+')\n",
    "        f.write('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n",
    "average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
    "        epoch, r, p, F1, acc, AUC))\\\n",
    "        \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "import datetime\n",
    "bs = 1\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "epoch = 1\n",
    "r_list = []\n",
    "p_list = []\n",
    "acc_list = []\n",
    "AUC_list = []\n",
    "# TP = 0\n",
    "# TN = 0\n",
    "# FN = 0\n",
    "# FP = 0\n",
    "vote_pred = np.zeros(testset.__len__())\n",
    "vote_score = np.zeros(testset.__len__())\n",
    "\n",
    "\n",
    "targetlist, scorelist, predlist = test(epoch)\n",
    "print('target',targetlist)\n",
    "print('score',scorelist)\n",
    "print('predict',predlist)\n",
    "vote_pred = vote_pred + predlist \n",
    "vote_score = vote_score + scorelist \n",
    "\n",
    "TP = ((predlist == 1) & (targetlist == 1)).sum()\n",
    "\n",
    "TN = ((predlist == 0) & (targetlist == 0)).sum()\n",
    "FN = ((predlist == 0) & (targetlist == 1)).sum()\n",
    "FP = ((predlist == 1) & (targetlist == 0)).sum()\n",
    "\n",
    "print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n",
    "print('TP+FP',TP+FP)\n",
    "p = TP / (TP + FP)\n",
    "print('precision',p)\n",
    "p = TP / (TP + FP)\n",
    "r = TP / (TP + FN)\n",
    "print('recall',r)\n",
    "F1 = 2 * r * p / (r + p)\n",
    "acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "print('F1',F1)\n",
    "print('acc',acc)\n",
    "AUC = roc_auc_score(targetlist, vote_score)\n",
    "print('AUC', AUC)\n",
    "\n",
    "\n",
    "save_p = os.path.join(PATH_to_log_dir, f'test_UCSD_{modelname}_{alpha}_{epoch}_{datetime.datetime.now()}.txt')\n",
    "f = open(save_p, 'a+')\n",
    "f.write('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n",
    "average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
    "epoch, r, p, F1, acc, AUC))\n",
    "f.close()\n",
    "# torch.save(model.state_dict(), \"model_backup/medical_transfer/{}_{}_wuhan.pt\".format(modelname,alpha_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#x_axix，train_pn_dis这些都是长度相同的list()\n",
    "# print(his['train_loss'])\n",
    "x_axix = range(len(his['train_loss']))\n",
    "#开始画图\n",
    "sub_axix = filter(lambda x:x%200 == 0, x_axix)\n",
    "plt.title('Result Analysis')\n",
    "plt.plot(x_axix, his['train_acc'],  label='train accuracy')\n",
    "plt.plot(x_axix, his['val_acc'], label='val accuracy')\n",
    "\n",
    "# plt.plot(x_axix,his['train_loss'],  color='skyblue', label='train loss')\n",
    "# plt.plot(x_axix, his['val_loss'], color='blue', label='val loss')\n",
    "plt.legend() # 显示图例\n",
    "\n",
    "plt.xlabel('iteration times')\n",
    "plt.ylabel('value')\n",
    "plt.show()\n",
    "#python 一个折线图绘制多个曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(his)\n",
    "df.to_csv('/data/cv_final/CT-Predict/2D-Pretrain/result/his.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = '/data/Data/prediction_img'\n",
    "import glob\n",
    "file_list = sorted(glob.glob(pred_path+\"/*.npy\"))\n",
    "file_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "target_list = file_list[10]\n",
    "input = np.empty([1, 128, 128 ,3])\n",
    "# for idx, p in enumerate(target_list):\n",
    "for c in range(3):\n",
    "    print(c)\n",
    "    input[0, :, :, c] = np.resize(np.load(p, allow_pickle=True), (128, 128))\n",
    "\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# input = input.resize([128, 128])\n",
    "print(input.shape)\n",
    "i = torch.tensor(input, dtype=torch.float).to(device)\n",
    "i = i.permute(3,0,1,2) # to (C,D,H,W)\n",
    "i = i.reshape([1, 3, 128, 128]).to(device)\n",
    "output = model(i)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = F.softmax(output, dim=1)\n",
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " output.argmax(dim=1, keepdim=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}